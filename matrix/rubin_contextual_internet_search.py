#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Smart Rubin AI - –ö–æ–Ω—Ç–µ–∫—Å—Ç—É–∞–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ
–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç, –∫–æ–≥–¥–∞ –Ω—É–∂–µ–Ω –ø–æ–∏—Å–∫ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ, –∏ –∏—â–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
"""

import requests
import sqlite3
import json
import os
import datetime
import time
import re
from urllib.parse import quote_plus
from bs4 import BeautifulSoup
from typing import Dict, List, Any, Optional, Tuple
import hashlib

class RubinContextualInternetSearch:
    """–ö–ª–∞—Å—Å –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç—É–∞–ª—å–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ"""
    
    def __init__(self, db_path: str = "rubin_knowledge_base.db"):
        self.db_path = db_path
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
        })
        
        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞, —É–∫–∞–∑—ã–≤–∞—é—â–∏–µ –Ω–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å –ø–æ–∏—Å–∫–∞ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ
        self.internet_search_triggers = {
            '–Ω–æ–≤—ã–µ_—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏': ['–Ω–æ–≤—ã–π', '—Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π', '–ø–æ—Å–ª–µ–¥–Ω–∏–π', '–∞–∫—Ç—É–∞–ª—å–Ω—ã–π', '—Ç—Ä–µ–Ω–¥', '–∏–Ω–Ω–æ–≤–∞—Ü–∏—è'],
            '–∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ_–≤–æ–ø—Ä–æ—Å—ã': ['–∫–∞–∫ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å', '–∫–∞–∫ –ø–æ–¥–∫–ª—é—á–∏—Ç—å', '–∫–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å', '–∫–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç'],
            '—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ_—Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏–∏': ['—Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏', '–ø–∞—Ä–∞–º–µ—Ç—Ä—ã', '—Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏—è', '—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ'],
            '–æ–±–Ω–æ–≤–ª–µ–Ω–∏—è': ['–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ', '–≤–µ—Ä—Å–∏—è', '–ø–∞—Ç—á', '–∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ', '–Ω–æ–≤—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏'],
            '—Å—Ä–∞–≤–Ω–µ–Ω–∏—è': ['—Å—Ä–∞–≤–Ω–∏—Ç—å', '—Ä–∞–∑–Ω–∏—Ü–∞', '–æ—Ç–ª–∏—á–∏—è', '–ª—É—á—à–µ', '—Ö—É–∂–µ', '–ø—Ä–æ—Ç–∏–≤'],
            '–ø—Ä–∏–º–µ—Ä—ã_–∫–æ–¥–∞': ['–ø—Ä–∏–º–µ—Ä –∫–æ–¥–∞', '–∫–æ–¥', '–ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ', '—Å–∏–Ω—Ç–∞–∫—Å–∏—Å', '—Ñ—É–Ω–∫—Ü–∏—è'],
            '–¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è': ['–¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è', '—Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ', '–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è', '–º–∞–Ω—É–∞–ª', '—Å–ø—Ä–∞–≤–∫–∞']
        }
        
        # –ö—ç—à –¥–ª—è –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
        self.search_cache = {}
        self.cache_file = "contextual_search_cache.json"
        self.load_cache()
    
    def load_cache(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∫—ç—à –ø–æ–∏—Å–∫–∞"""
        try:
            if os.path.exists(self.cache_file):
                with open(self.cache_file, 'r', encoding='utf-8') as f:
                    self.search_cache = json.load(f)
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫—ç—à–∞: {e}")
            self.search_cache = {}
    
    def save_cache(self):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∫—ç—à –ø–æ–∏—Å–∫–∞"""
        try:
            with open(self.cache_file, 'w', encoding='utf-8') as f:
                json.dump(self.search_cache, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∫—ç—à–∞: {e}")
    
    def needs_internet_search(self, message: str) -> Tuple[bool, str, str]:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, –Ω—É–∂–µ–Ω –ª–∏ –ø–æ–∏—Å–∫ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ"""
        message_lower = message.lower()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç—Ä–∏–≥–≥–µ—Ä—ã
        for category, triggers in self.internet_search_triggers.items():
            for trigger in triggers:
                if trigger in message_lower:
                    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –ø–æ–∏—Å–∫–∞
                    search_type = self.determine_search_type(message, category)
                    return True, category, search_type
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –æ—Ç–≤–µ—Ç –≤ –ª–æ–∫–∞–ª—å–Ω–æ–π –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π
        if self.has_local_answer(message):
            return False, "local", "local"
        
        # –ï—Å–ª–∏ –Ω–µ—Ç –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞, –∏—â–µ–º –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ
        return True, "general", "general"
    
    def determine_search_type(self, message: str, category: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–∏–ø –ø–æ–∏—Å–∫–∞"""
        message_lower = message.lower()
        
        if category == '–Ω–æ–≤—ã–µ_—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏':
            return "technology"
        elif category == '–∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ_–≤–æ–ø—Ä–æ—Å—ã':
            return "howto"
        elif category == '—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ_—Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏–∏':
            return "specifications"
        elif category == '–æ–±–Ω–æ–≤–ª–µ–Ω–∏—è':
            return "updates"
        elif category == '—Å—Ä–∞–≤–Ω–µ–Ω–∏—è':
            return "comparison"
        elif category == '–ø—Ä–∏–º–µ—Ä—ã_–∫–æ–¥–∞':
            return "code_examples"
        elif category == '–¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è':
            return "documentation"
        else:
            return "general"
    
    def has_local_answer(self, message: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –µ—Å—Ç—å –ª–∏ –æ—Ç–≤–µ—Ç –≤ –ª–æ–∫–∞–ª—å–Ω–æ–π –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # –ò—â–µ–º –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
            words = message.lower().split()
            search_terms = [word for word in words if len(word) > 3]
            
            if not search_terms:
                return False
            
            # –°–æ–∑–¥–∞–µ–º –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            search_query = " OR ".join([f"title LIKE '%{term}%' OR content LIKE '%{term}%' OR keywords LIKE '%{term}%'" for term in search_terms])
            
            cursor.execute(f"""
                SELECT COUNT(*) FROM knowledge_base 
                WHERE {search_query}
            """)
            
            count = cursor.fetchone()[0]
            conn.close()
            
            return count > 0
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –ª–æ–∫–∞–ª—å–Ω–æ–π –±–∞–∑—ã: {e}")
            return False
    
    def generate_search_query(self, message: str, search_type: str) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å"""
        message_lower = message.lower()
        
        # –ë–∞–∑–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
        base_query = message
        
        # –û–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞
        if search_type == "technology":
            base_query += " –Ω–æ–≤–µ–π—à–∏–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ 2024"
        elif search_type == "howto":
            base_query += " –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ"
        elif search_type == "specifications":
            base_query += " —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã"
        elif search_type == "updates":
            base_query += " –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –Ω–æ–≤–æ—Å—Ç–∏"
        elif search_type == "comparison":
            base_query += " —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –æ—Ç–ª–∏—á–∏—è"
        elif search_type == "code_examples":
            base_query += " –ø—Ä–∏–º–µ—Ä—ã –∫–æ–¥–∞ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ"
        elif search_type == "documentation":
            base_query += " –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–∞—è"
        
        return base_query
    
    def search_internet(self, query: str, max_results: int = 3) -> List[Dict[str, Any]]:
        """–ò—â–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ"""
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
        cache_key = hashlib.md5(query.encode()).hexdigest()
        if cache_key in self.search_cache:
            print(f"üìã –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã")
            return self.search_cache[cache_key]
        
        print(f"üîç –ü–æ–∏—Å–∫ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ: {query}")
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º DuckDuckGo –¥–ª—è –ø–æ–∏—Å–∫–∞
        try:
            encoded_query = quote_plus(query)
            search_url = f"https://duckduckgo.com/html/?q={encoded_query}"
            
            response = self.session.get(search_url, timeout=10)
            if response.status_code == 200:
                results = self.parse_duckduckgo_results(response.text)
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à
                self.search_cache[cache_key] = results
                self.save_cache()
                
                return results[:max_results]
                
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {e}")
        
        return []
    
    def parse_duckduckgo_results(self, html: str) -> List[Dict[str, Any]]:
        """–ü–∞—Ä—Å–∏—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã DuckDuckGo"""
        results = []
        try:
            soup = BeautifulSoup(html, 'html.parser')
            search_results = soup.find_all('div', class_='result')
            
            for result in search_results:
                try:
                    # –ó–∞–≥–æ–ª–æ–≤–æ–∫ –∏ —Å—Å—ã–ª–∫–∞
                    title_elem = result.find('a', class_='result__a')
                    if not title_elem:
                        continue
                    
                    title = title_elem.get_text().strip()
                    url = title_elem.get('href', '')
                    
                    # –û–ø–∏—Å–∞–Ω–∏–µ
                    desc_elem = result.find('a', class_='result__snippet')
                    snippet = desc_elem.get_text().strip() if desc_elem else ""
                    
                    if url and title:
                        results.append({
                            'title': title,
                            'url': url,
                            'snippet': snippet,
                            'source': 'duckduckgo',
                            'timestamp': datetime.datetime.now().isoformat()
                        })
                        
                except Exception as e:
                    continue
                    
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {e}")
        
        return results
    
    def download_and_analyze_content(self, url: str, title: str) -> Optional[Dict[str, Any]]:
        """–°–∫–∞—á–∏–≤–∞–µ—Ç –∏ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        try:
            print(f"üì• –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º: {title}")
            
            response = self.session.get(url, timeout=15)
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # –£–¥–∞–ª—è–µ–º —Å–∫—Ä–∏–ø—Ç—ã –∏ —Å—Ç–∏–ª–∏
                for script in soup(["script", "style", "nav", "footer", "header"]):
                    script.decompose()
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç
                content_selectors = [
                    'article', 'main', '.content', '.post-content', 
                    '.entry-content', '.article-content', 'div[role="main"]'
                ]
                
                content = ""
                for selector in content_selectors:
                    content_elem = soup.select_one(selector)
                    if content_elem:
                        content = content_elem.get_text()
                        break
                
                if not content:
                    content = soup.get_text()
                
                # –û—á–∏—â–∞–µ–º —Ç–µ–∫—Å—Ç
                lines = (line.strip() for line in content.splitlines())
                chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
                content = ' '.join(chunk for chunk in chunks if chunk)
                
                # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä
                content = content[:3000]
                
                if len(content) > 200:
                    return {
                        'title': title,
                        'content': content,
                        'url': url,
                        'timestamp': datetime.datetime.now().isoformat()
                    }
                
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ {url}: {e}")
        
        return None
    
    def process_user_message(self, message: str) -> Dict[str, Any]:
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç—É–∞–ª—å–Ω—ã–º –ø–æ–∏—Å–∫–æ–º"""
        print(f"üí¨ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ: {message}")
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º, –Ω—É–∂–µ–Ω –ª–∏ –ø–æ–∏—Å–∫ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ
        needs_search, category, search_type = self.needs_internet_search(message)
        
        if not needs_search:
            return {
                'needs_internet_search': False,
                'search_category': category,
                'search_type': search_type,
                'local_answer_available': True,
                'internet_results': [],
                'analyzed_content': None
            }
        
        print(f"üåê –ù—É–∂–µ–Ω –ø–æ–∏—Å–∫ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ: {category} ({search_type})")
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
        search_query = self.generate_search_query(message, search_type)
        
        # –ò—â–µ–º –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ
        search_results = self.search_internet(search_query, max_results=2)
        
        if not search_results:
            return {
                'needs_internet_search': True,
                'search_category': category,
                'search_type': search_type,
                'local_answer_available': False,
                'internet_results': [],
                'analyzed_content': None
            }
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –Ω–∞–∏–±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        analyzed_content = None
        if search_results:
            best_result = search_results[0]
            analyzed_content = self.download_and_analyze_content(
                best_result['url'], best_result['title']
            )
        
        return {
            'needs_internet_search': True,
            'search_category': category,
            'search_type': search_type,
            'local_answer_available': False,
            'internet_results': search_results,
            'analyzed_content': analyzed_content
        }
    
    def save_knowledge_from_internet(self, analyzed_content: Dict[str, Any], original_message: str) -> bool:
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∑–Ω–∞–Ω–∏—è, –ø–æ–ª—É—á–µ–Ω–Ω—ã–µ –∏–∑ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞"""
        if not analyzed_content:
            return False
        
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é
            category = self.detect_category(analyzed_content['content'], analyzed_content['title'])
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
            keywords = self.extract_keywords(analyzed_content['content'], analyzed_content['title'])
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ —É–∂–µ —Ç–∞–∫–∞—è –∑–∞–ø–∏—Å—å
            cursor.execute("""
                SELECT COUNT(*) FROM knowledge_base 
                WHERE title = ? AND source_url = ?
            """, (analyzed_content['title'], analyzed_content['url']))
            
            if cursor.fetchone()[0] == 0:
                # –í—Å—Ç–∞–≤–ª—è–µ–º –Ω–æ–≤—É—é –∑–∞–ø–∏—Å—å
                cursor.execute("""
                    INSERT INTO knowledge_base 
                    (title, content, category, tags, keywords, created_at, usage_count, relevance_score)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                """, (
                    analyzed_content['title'],
                    analyzed_content['content'],
                    category,
                    ', '.join(keywords[:5]),
                    ', '.join(keywords),
                    analyzed_content['timestamp'],
                    0,
                    0.9
                ))
                
                conn.commit()
                conn.close()
                
                print(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω—ã –∑–Ω–∞–Ω–∏—è: {analyzed_content['title']}")
                return True
            else:
                print(f"‚ö†Ô∏è –ó–Ω–∞–Ω–∏—è —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É—é—Ç: {analyzed_content['title']}")
                return False
                
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∑–Ω–∞–Ω–∏–π: {e}")
            return False
    
    def detect_category(self, content: str, title: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏—é –ø–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É"""
        text = f"{title} {content}".lower()
        
        if any(word in text for word in ['–º–∞—Ç–µ–º–∞—Ç–∏–∫–∞', '—Ñ–æ—Ä–º—É–ª–∞', '—É—Ä–∞–≤–Ω–µ–Ω–∏–µ', '–∏–Ω—Ç–µ–≥—Ä–∞–ª', '–ø—Ä–æ–∏–∑–≤–æ–¥–Ω–∞—è']):
            return 'mathematics'
        elif any(word in text for word in ['—Ñ–∏–∑–∏–∫–∞', '–º–µ—Ö–∞–Ω–∏–∫–∞', '—ç–ª–µ–∫—Ç—Ä–∏—á–µ—Å—Ç–≤–æ', '–º–∞–≥–Ω–µ—Ç–∏–∑–º']):
            return 'physics'
        elif any(word in text for word in ['–ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ', '–∫–æ–¥', '–∞–ª–≥–æ—Ä–∏—Ç–º', 'python', 'c++']):
            return 'programming'
        elif any(word in text for word in ['–∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è', 'plc', 'pmac', '–∫–æ–Ω—Ç—Ä–æ–ª–ª–µ—Ä']):
            return 'automation'
        elif any(word in text for word in ['—ç–ª–µ–∫—Ç—Ä–æ–Ω–∏–∫–∞', '—Å—Ö–µ–º–∞', '—Ç—Ä–∞–Ω–∑–∏—Å—Ç–æ—Ä', '—Ä–µ–∑–∏—Å—Ç–æ—Ä']):
            return 'electronics'
        else:
            return 'general'
    
    def extract_keywords(self, content: str, title: str) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞"""
        text = f"{title} {content}".lower()
        text = re.sub(r'[^\w\s]', ' ', text)
        words = text.split()
        
        stop_words = {'–∏', '–≤', '–Ω–∞', '—Å', '–ø–æ', '–¥–ª—è', '–æ—Ç', '–¥–æ', '–∏–∑', '–∫', '–æ', '—É', '–∑–∞', '–ø–æ–¥', '–Ω–∞–¥', '–ø—Ä–∏', '—á–µ—Ä–µ–∑', '–±–µ–∑', '–º–µ–∂–¥—É', '—Å—Ä–µ–¥–∏', '–æ–∫–æ–ª–æ', '–≤–æ–∫—Ä—É–≥', '–≤–Ω—É—Ç—Ä–∏', '–≤–Ω–µ', '–ø–µ—Ä–µ–¥', '–ø–æ—Å–ª–µ', '–≤–æ', '—Å–æ', '–æ–±', '–ø—Ä–æ', '—á—Ç–æ', '–∫–∞–∫', '–≥–¥–µ', '–∫–æ–≥–¥–∞', '–ø–æ—á–µ–º—É', '–∑–∞—á–µ–º', '–∫–æ—Ç–æ—Ä—ã–π', '–∫–æ—Ç–æ—Ä–∞—è', '–∫–æ—Ç–æ—Ä–æ–µ', '–∫–æ—Ç–æ—Ä—ã–µ', '—ç—Ç–æ', '—ç—Ç–æ—Ç', '—ç—Ç–∞', '—ç—Ç–æ', '—ç—Ç–∏', '—Ç–æ—Ç', '—Ç–∞', '—Ç–æ', '—Ç–µ', '–æ–Ω', '–æ–Ω–∞', '–æ–Ω–æ', '–æ–Ω–∏', '–º—ã', '–≤—ã', '—è', '—Ç—ã', '–æ–Ω', '–æ–Ω–∞', '–æ–Ω–æ', '–æ–Ω–∏', '–º–æ–π', '–º–æ—è', '–º–æ–µ', '–º–æ–∏', '—Ç–≤–æ–π', '—Ç–≤–æ—è', '—Ç–≤–æ–µ', '—Ç–≤–æ–∏', '–µ–≥–æ', '–µ–µ', '–∏—Ö', '–Ω–∞—à', '–Ω–∞—à–∞', '–Ω–∞—à–µ', '–Ω–∞—à–∏', '–≤–∞—à', '–≤–∞—à–∞', '–≤–∞—à–µ', '–≤–∞—à–∏'}
        
        keywords = [word for word in words if len(word) > 3 and word not in stop_words]
        
        word_count = {}
        for word in keywords:
            word_count[word] = word_count.get(word, 0) + 1
        
        sorted_words = sorted(word_count.items(), key=lambda x: x[1], reverse=True)
        return [word for word, count in sorted_words[:10]]

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
    print("üåê SMART RUBIN AI - –ö–û–ù–¢–ï–ö–°–¢–£–ê–õ–¨–ù–´–ô –ü–û–ò–°–ö –í –ò–ù–¢–ï–†–ù–ï–¢–ï")
    print("=" * 60)
    
    # –°–æ–∑–¥–∞–µ–º —Å–∏—Å—Ç–µ–º—É –∫–æ–Ω—Ç–µ–∫—Å—Ç—É–∞–ª—å–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
    searcher = RubinContextualInternetSearch()
    
    # –¢–µ—Å—Ç–æ–≤—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è
    test_messages = [
        "–ö–∞–∫ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å Python –¥–ª—è –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è?",
        "–ù–æ–≤—ã–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ –≤ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ 2024",
        "–°—Ä–∞–≤–Ω–µ–Ω–∏–µ PLC –∏ PMAC –∫–æ–Ω—Ç—Ä–æ–ª–ª–µ—Ä–æ–≤",
        "–ß—Ç–æ —Ç–∞–∫–æ–µ –ø—Ä–æ–∏–∑–≤–æ–¥–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏–∏?",
        "–ö–∞–∫ –ø–æ–¥–∫–ª—é—á–∏—Ç—å –¥–∞—Ç—á–∏–∫ —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—ã –∫ Arduino?"
    ]
    
    for message in test_messages:
        print(f"\nüí¨ –¢–µ—Å—Ç: {message}")
        print("-" * 50)
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ
        result = searcher.process_user_message(message)
        
        print(f"üîç –ù—É–∂–µ–Ω –ø–æ–∏—Å–∫ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ: {result['needs_internet_search']}")
        print(f"üìÇ –ö–∞—Ç–µ–≥–æ—Ä–∏—è: {result['search_category']}")
        print(f"üéØ –¢–∏–ø –ø–æ–∏—Å–∫–∞: {result['search_type']}")
        print(f"üìö –õ–æ–∫–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç –¥–æ—Å—Ç—É–ø–µ–Ω: {result['local_answer_available']}")
        
        if result['internet_results']:
            print(f"üåê –ù–∞–π–¥–µ–Ω–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {len(result['internet_results'])}")
            for i, res in enumerate(result['internet_results'], 1):
                print(f"   {i}. {res['title']}")
        
        if result['analyzed_content']:
            print(f"üìÑ –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –∫–æ–Ω—Ç–µ–Ω—Ç: {result['analyzed_content']['title']}")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∑–Ω–∞–Ω–∏—è
            if searcher.save_knowledge_from_internet(result['analyzed_content'], message):
                print("üíæ –ó–Ω–∞–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –±–∞–∑—É!")
        
        time.sleep(2)  # –ü–∞—É–∑–∞ –º–µ–∂–¥—É —Ç–µ—Å—Ç–∞–º–∏

if __name__ == "__main__":
    main()
