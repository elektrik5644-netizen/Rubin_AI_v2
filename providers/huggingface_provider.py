"""
Rubin AI v2.0 - Hugging Face –ø—Ä–æ–≤–∞–π–¥–µ—Ä
–°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–¥–∞ –∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏
"""

import torch
from transformers import pipeline, AutoTokenizer, AutoModel
from typing import Dict, List, Optional, Any
import logging
import hashlib
import time

from .base_provider import BaseProvider, TaskType, ResponseFormat

class HuggingFaceProvider(BaseProvider):
    """Hugging Face –ø—Ä–æ–≤–∞–π–¥–µ—Ä –¥–ª—è —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∑–∞–¥–∞—á"""
    
    def __init__(self):
        super().__init__("huggingface", priority=3)
        self.models = {}
        self.tokenizers = {}
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.response_cache = {}  # –ö—ç—à –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
        self.cache_ttl = 300  # –í—Ä–µ–º—è –∂–∏–∑–Ω–∏ –∫—ç—à–∞ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö (5 –º–∏–Ω—É—Ç)
        
    def initialize(self) -> bool:
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Hugging Face –º–æ–¥–µ–ª–µ–π"""
        try:
            self.logger.info("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Hugging Face –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞...")
            
            # CodeBERT –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–¥–∞
            self.models['code_analyzer'] = pipeline(
                "text-classification",
                model="microsoft/codebert-base",
                device=0 if torch.cuda.is_available() else -1
            )
            
            # DistilBERT –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏
            self.models['safety_checker'] = pipeline(
                "text-classification",
                model="distilbert-base-uncased",
                device=0 if torch.cuda.is_available() else -1
            )
            
            # CodeT5 –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–¥–∞
            self.models['code_generator'] = pipeline(
                "text2text-generation",
                model="Salesforce/codet5-base",
                device=0 if torch.cuda.is_available() else -1
            )
            
            # GPT-2 –¥–ª—è –æ–±—â–∏—Ö –∑–∞–¥–∞—á
            self.models['text_generator'] = pipeline(
                "text-generation",
                model="gpt2",
                device=0 if torch.cuda.is_available() else -1
            )
            
            self.is_available = True
            self.logger.info("Hugging Face –ø—Ä–æ–≤–∞–π–¥–µ—Ä —É—Å–ø–µ—à–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
            return True
            
        except Exception as e:
            self.log_error(e)
            return False
    
    def _get_cache_key(self, message: str, task_type) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫–ª—é—á –∫—ç—à–∞ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞"""
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∫ enum, —Ç–∞–∫ –∏ —Å—Ç—Ä–æ–∫—É
        if hasattr(task_type, 'value'):
            task_str = task_type.value
        else:
            task_str = str(task_type)
        content = f"{message}_{task_str}"
        return hashlib.md5(content.encode()).hexdigest()
    
    def _get_cached_response(self, cache_key: str) -> Optional[Dict[str, Any]]:
        """–ü–æ–ª—É—á–∞–µ—Ç –æ—Ç–≤–µ—Ç –∏–∑ –∫—ç—à–∞ –µ—Å–ª–∏ –æ–Ω –µ—â–µ –∞–∫—Ç—É–∞–ª–µ–Ω"""
        if cache_key in self.response_cache:
            cached_data = self.response_cache[cache_key]
            if time.time() - cached_data['timestamp'] < self.cache_ttl:
                return cached_data['response']
            else:
                # –£–¥–∞–ª—è–µ–º —É—Å—Ç–∞—Ä–µ–≤—à–∏–π –∫—ç—à
                del self.response_cache[cache_key]
        return None
    
    def _cache_response(self, cache_key: str, response: Dict[str, Any]):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –æ—Ç–≤–µ—Ç –≤ –∫—ç—à"""
        self.response_cache[cache_key] = {
            'response': response,
            'timestamp': time.time()
        }
    
    def get_response(self, message: str, context: Optional[Dict] = None) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–∏—Ç—å –æ—Ç–≤–µ—Ç –æ—Ç Hugging Face –º–æ–¥–µ–ª–µ–π"""
        if not self.is_available:
            return ResponseFormat.create_error_response(
                "Hugging Face –ø—Ä–æ–≤–∞–π–¥–µ—Ä –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω",
                self.name,
                context.get('task_type', TaskType.GENERAL_CHAT) if context else TaskType.GENERAL_CHAT
            )
        
        try:
            task_type = context.get('task_type', TaskType.GENERAL_CHAT) if context else TaskType.GENERAL_CHAT
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
            cache_key = self._get_cache_key(message, task_type)
            cached_response = self._get_cached_response(cache_key)
            if cached_response:
                cached_response['cached'] = True
                return cached_response
            
            thinking_process = []
            
            # –ü–æ–ª—É—á–∞–µ–º –æ—Ç–≤–µ—Ç –æ—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–≥–æ –º–µ—Ç–æ–¥–∞
            if task_type == TaskType.CODE_ANALYSIS:
                response = self._analyze_code(message, thinking_process)
            elif task_type == TaskType.SECURITY_CHECK:
                response = self._check_security(message, thinking_process)
            elif task_type == TaskType.CODE_GENERATION:
                response = self._generate_code(message, thinking_process)
            elif task_type == TaskType.PLC_ANALYSIS:
                response = self._analyze_plc_code(message, thinking_process)
            else:
                response = self._general_response(message, thinking_process)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
            self._cache_response(cache_key, response)
            return response
                
        except Exception as e:
            self.log_error(e)
            return ResponseFormat.create_error_response(
                str(e),
                self.name,
                context.get('task_type', TaskType.GENERAL_CHAT) if context else TaskType.GENERAL_CHAT
            )
    
    def _analyze_code(self, code: str, thinking_process: List[str]) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ –∫–æ–¥–∞ —á–µ—Ä–µ–∑ CodeBERT"""
        thinking_process.append("üîç –ê–Ω–∞–ª–∏–∑–∏—Ä—É—é –∫–æ–¥ —á–µ—Ä–µ–∑ CodeBERT...")
        
        # –ê–Ω–∞–ª–∏–∑ –∫–æ–¥–∞
        analysis = self.models['code_analyzer'](code[:512])  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É
        
        thinking_process.append(f"üìä –†–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞: {analysis}")
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
        if analysis and len(analysis) > 0:
            result = analysis[0]
            confidence = result['score']
            label = result['label']
            
            content = f"""
**–ê–Ω–∞–ª–∏–∑ –∫–æ–¥–∞ —á–µ—Ä–µ–∑ CodeBERT:**

üîç **–†–µ–∑—É–ª—å—Ç–∞—Ç:** {label}
üìä **–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å:** {confidence:.2%}

**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:**
- –ö–æ–¥ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω —Å –ø–æ–º–æ—â—å—é —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ Microsoft CodeBERT
- –†–µ–∑—É–ª—å—Ç–∞—Ç –æ—Å–Ω–æ–≤–∞–Ω –Ω–∞ –æ–±—É—á–µ–Ω–∏–∏ –Ω–∞ –±–æ–ª—å—à–∏—Ö –æ–±—ä–µ–º–∞—Ö –∫–æ–¥–∞
- –î–ª—è –±–æ–ª–µ–µ –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã
"""
        else:
            content = "–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∫–æ–¥. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –¥—Ä—É–≥–æ–π —Ñ—Ä–∞–≥–º–µ–Ω—Ç."
        
        return ResponseFormat.create_response(
            content=content,
            provider=self.name,
            task_type=TaskType.CODE_ANALYSIS,
            metadata={'model': 'microsoft/codebert-base', 'confidence': confidence if analysis else 0},
            thinking_process=thinking_process
        )
    
    def _check_security(self, code: str, thinking_process: List[str]) -> Dict[str, Any]:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ —á–µ—Ä–µ–∑ DistilBERT"""
        thinking_process.append("üõ°Ô∏è –ü—Ä–æ–≤–µ—Ä—è—é –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å —á–µ—Ä–µ–∑ DistilBERT...")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏
        security_check = self.models['safety_checker'](code[:512])
        
        thinking_process.append(f"üîí –†–µ–∑—É–ª—å—Ç–∞—Ç –ø—Ä–æ–≤–µ—Ä–∫–∏: {security_check}")
        
        if security_check and len(security_check) > 0:
            result = security_check[0]
            confidence = result['score']
            label = result['label']
            
            content = f"""
**–ü—Ä–æ–≤–µ—Ä–∫–∞ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ —á–µ—Ä–µ–∑ DistilBERT:**

üõ°Ô∏è **–°—Ç–∞—Ç—É—Å:** {label}
üìä **–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å:** {confidence:.2%}

**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏:**
- –ö–æ–¥ –ø—Ä–æ–≤–µ—Ä–µ–Ω –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö —É—è–∑–≤–∏–º–æ—Å—Ç–µ–π
- –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –º–æ–¥–µ–ª—å DistilBERT, –æ–±—É—á–µ–Ω–Ω–∞—è –Ω–∞ –¥–∞–Ω–Ω—ã—Ö –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏
- –î–ª—è –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω—ã—Ö —Å–∏—Å—Ç–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞
"""
        else:
            content = "–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å –∫–æ–¥–∞."
        
        return ResponseFormat.create_response(
            content=content,
            provider=self.name,
            task_type=TaskType.SECURITY_CHECK,
            metadata={'model': 'distilbert-base-uncased', 'confidence': confidence if security_check else 0},
            thinking_process=thinking_process
        )
    
    def _generate_code(self, prompt: str, thinking_process: List[str]) -> Dict[str, Any]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–¥–∞ —á–µ—Ä–µ–∑ CodeT5"""
        thinking_process.append("‚ö° –ì–µ–Ω–µ—Ä–∏—Ä—É—é –∫–æ–¥ —á–µ—Ä–µ–∑ CodeT5...")
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–¥–∞ (–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–æ)
        generated = self.models['code_generator'](
            prompt,
            max_new_tokens=5000,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º max_new_tokens –≤–º–µ—Å—Ç–æ max_length
            num_return_sequences=1,
            temperature=0.7,
            do_sample=True,
            early_stopping=True
        )
        
        thinking_process.append(f"üíª –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–¥: {generated}")
        
        if generated and len(generated) > 0:
            code = generated[0]['generated_text']
            
            content = f"""
**–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–¥–∞ —á–µ—Ä–µ–∑ CodeT5:**

```python
{code}
```

**–ü—Ä–∏–º–µ—á–∞–Ω–∏—è:**
- –ö–æ–¥ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω —Å –ø–æ–º–æ—â—å—é –º–æ–¥–µ–ª–∏ Salesforce CodeT5
- –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –∏ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –∫–æ–¥ –ø–µ—Ä–µ–¥ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º
- –ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ –Ω–∞ –±–æ–ª—å—à–∏—Ö –æ–±—ä–µ–º–∞—Ö –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∫–æ–¥–∞
"""
        else:
            content = "–ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –∫–æ–¥. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –¥—Ä—É–≥–æ–π –∑–∞–ø—Ä–æ—Å."
        
        return ResponseFormat.create_response(
            content=content,
            provider=self.name,
            task_type=TaskType.CODE_GENERATION,
            metadata={'model': 'Salesforce/codet5-base'},
            thinking_process=thinking_process
        )
    
    def _analyze_plc_code(self, code: str, thinking_process: List[str]) -> Dict[str, Any]:
        """–°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ PLC –∫–æ–¥–∞"""
        thinking_process.append("üè≠ –ê–Ω–∞–ª–∏–∑–∏—Ä—É—é PLC –∫–æ–¥...")
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º CodeBERT –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ PLC –∫–æ–¥–∞
        analysis = self.models['code_analyzer'](code[:512])
        
        thinking_process.append(f"üîß –†–µ–∑—É–ª—å—Ç–∞—Ç PLC –∞–Ω–∞–ª–∏–∑–∞: {analysis}")
        
        if analysis and len(analysis) > 0:
            result = analysis[0]
            confidence = result['score']
            label = result['label']
            
            content = f"""
**–ê–Ω–∞–ª–∏–∑ PLC –∫–æ–¥–∞:**

üè≠ **–¢–∏–ø –∫–æ–¥–∞:** {label}
üìä **–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å:** {confidence:.2%}

**–°–ø–µ—Ü–∏—Ñ–∏–∫–∞ PLC:**
- –ö–æ–¥ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω —Å —É—á–µ—Ç–æ–º –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–µ–π –ø—Ä–æ–º—ã—à–ª–µ–Ω–Ω–æ–π –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏
- –ü—Ä–æ–≤–µ—Ä–µ–Ω—ã —Å—Ç–∞–Ω–¥–∞—Ä—Ç—ã IEC 61131-3
- –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –ø—Ä–æ–º—ã—à–ª–µ–Ω–Ω—ã—Ö —Å–∏—Å—Ç–µ–º

**–°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏:**
1. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞–º –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ (SIL)
2. –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
3. –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ —Å–∏–º—É–ª—è—Ç–æ—Ä–µ
"""
        else:
            content = "–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å PLC –∫–æ–¥."
        
        return ResponseFormat.create_response(
            content=content,
            provider=self.name,
            task_type=TaskType.PLC_ANALYSIS,
            metadata={'model': 'microsoft/codebert-base', 'confidence': confidence if analysis else 0},
            thinking_process=thinking_process
        )
    
    def _general_response(self, message: str, thinking_process: List[str]) -> Dict[str, Any]:
        """–û–±—â–∏–π –æ—Ç–≤–µ—Ç —á–µ—Ä–µ–∑ GPT-2"""
        thinking_process.append("üí≠ –ì–µ–Ω–µ—Ä–∏—Ä—É—é –æ—Ç–≤–µ—Ç —á–µ—Ä–µ–∑ GPT-2...")
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ (–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–æ)
        generated = self.models['text_generator'](
            message,
            max_new_tokens=5000,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º max_new_tokens –≤–º–µ—Å—Ç–æ max_length
            num_return_sequences=1,
            temperature=0.7,
            do_sample=True,
            early_stopping=True,
            pad_token_id=50256
        )
        
        thinking_process.append(f"üìù –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç: {generated}")
        
        if generated and len(generated) > 0:
            response = generated[0]['generated_text']
            # –£–±–∏—Ä–∞–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç –∏–∑ –æ—Ç–≤–µ—Ç–∞
            if message in response:
                response = response.replace(message, "").strip()
            
            content = f"""
**–û—Ç–≤–µ—Ç —á–µ—Ä–µ–∑ GPT-2:**

{response}

**–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ:** –≠—Ç–æ –±–∞–∑–æ–≤—ã–π –æ—Ç–≤–µ—Ç –æ—Ç –º–æ–¥–µ–ª–∏ GPT-2. –î–ª—è –±–æ–ª–µ–µ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏.
"""
        else:
            content = "–ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç."
        
        return ResponseFormat.create_response(
            content=content,
            provider=self.name,
            task_type=TaskType.GENERAL_CHAT,
            metadata={'model': 'gpt2'},
            thinking_process=thinking_process
        )
    
    def get_capabilities(self) -> List[str]:
        """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞"""
        return [
            TaskType.CODE_ANALYSIS,
            TaskType.SECURITY_CHECK,
            TaskType.CODE_GENERATION,
            TaskType.PLC_ANALYSIS,
            TaskType.GENERAL_CHAT
        ]
