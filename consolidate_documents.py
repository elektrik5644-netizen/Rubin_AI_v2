#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–ö–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏—è –±–∞–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ Rubin AI v2
–û–±—ä–µ–¥–∏–Ω—è–µ—Ç –¥—É–±–ª–∏—Ä—É—é—â–∏–µ –±–∞–∑—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –º–µ—Å—Ç–∞ –∏ —É—Å–∫–æ—Ä–µ–Ω–∏—è –ø–æ–∏—Å–∫–∞
"""

import sqlite3
import os
import shutil
from datetime import datetime
import hashlib

def backup_database(db_path):
    """–°–æ–∑–¥–∞–µ—Ç —Ä–µ–∑–µ—Ä–≤–Ω—É—é –∫–æ–ø–∏—é –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö"""
    if os.path.exists(db_path):
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_path = f"{db_path}.backup_{timestamp}"
        shutil.copy2(db_path, backup_path)
        print(f"‚úÖ –°–æ–∑–¥–∞–Ω–∞ —Ä–µ–∑–µ—Ä–≤–Ω–∞—è –∫–æ–ø–∏—è: {backup_path}")
        return backup_path
    return None

def get_table_schema(cursor, table_name):
    """–ü–æ–ª—É—á–∞–µ—Ç —Å—Ö–µ–º—É —Ç–∞–±–ª–∏—Ü—ã"""
    cursor.execute(f"PRAGMA table_info({table_name});")
    return [col[1] for col in cursor.fetchall()]

def calculate_content_hash(content):
    """–í—ã—á–∏—Å–ª—è–µ—Ç —Ö—ç—à —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤"""
    if content is None:
        content = ""
    return hashlib.md5(str(content).encode('utf-8')).hexdigest()

def consolidate_documents():
    """–ö–æ–Ω—Å–æ–ª–∏–¥–∏—Ä—É–µ—Ç –±–∞–∑—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
    print("üîÑ –ö–û–ù–°–û–õ–ò–î–ê–¶–ò–Ø –ë–ê–ó –î–û–ö–£–ú–ï–ù–¢–û–í RUBIN AI V2")
    print("=" * 60)
    
    # –û—Å–Ω–æ–≤–Ω—ã–µ –±–∞–∑—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –∫–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏–∏
    source_databases = [
        'rubin_ai_documents.db',
        'rubin_ai_v2.db'
    ]
    
    # –¶–µ–ª–µ–≤–∞—è –∫–æ–Ω—Å–æ–ª–∏–¥–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –±–∞–∑–∞
    target_db = 'rubin_documents_consolidated.db'
    
    # –°–æ–∑–¥–∞–µ–º —Ä–µ–∑–µ—Ä–≤–Ω—ã–µ –∫–æ–ø–∏–∏
    print("üì¶ –°–æ–∑–¥–∞–Ω–∏–µ —Ä–µ–∑–µ—Ä–≤–Ω—ã—Ö –∫–æ–ø–∏–π...")
    backups = []
    for db in source_databases:
        if os.path.exists(db):
            backup = backup_database(db)
            if backup:
                backups.append(backup)
    
    # –°–æ–∑–¥–∞–µ–º —Ü–µ–ª–µ–≤—É—é –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö
    print(f"\nüèóÔ∏è –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–Ω—Å–æ–ª–∏–¥–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –±–∞–∑—ã: {target_db}")
    conn_target = sqlite3.connect(target_db)
    cursor_target = conn_target.cursor()
    
    # –°–æ–∑–¥–∞–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ —Ç–∞–±–ª–∏—Ü—ã
    cursor_target.execute("""
        CREATE TABLE IF NOT EXISTS documents (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            file_name TEXT NOT NULL,
            content TEXT NOT NULL,
            file_path TEXT,
            file_size INTEGER,
            file_type TEXT,
            category TEXT,
            tags TEXT,
            difficulty_level TEXT DEFAULT 'medium',
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            content_hash TEXT,
            source_db TEXT
        );
    """)
    
    cursor_target.execute("""
        CREATE TABLE IF NOT EXISTS document_index (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            document_id INTEGER,
            term TEXT NOT NULL,
            frequency INTEGER DEFAULT 1,
            position INTEGER,
            FOREIGN KEY (document_id) REFERENCES documents (id)
        );
    """)
    
    cursor_target.execute("""
        CREATE TABLE IF NOT EXISTS document_vectors (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            document_id INTEGER,
            vector_data TEXT,
            vector_dimension INTEGER,
            model_name TEXT,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            FOREIGN KEY (document_id) REFERENCES documents (id)
        );
    """)
    
    cursor_target.execute("""
        CREATE TABLE IF NOT EXISTS categories (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            name TEXT UNIQUE NOT NULL,
            description TEXT,
            parent_id INTEGER,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        );
    """)
    
    # –°–æ–∑–¥–∞–µ–º –∏–Ω–¥–µ–∫—Å—ã –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –ø–æ–∏—Å–∫–∞
    cursor_target.execute("CREATE INDEX IF NOT EXISTS idx_documents_content_hash ON documents (content_hash);")
    cursor_target.execute("CREATE INDEX IF NOT EXISTS idx_documents_category ON documents (category);")
    cursor_target.execute("CREATE INDEX IF NOT EXISTS idx_documents_file_name ON documents (file_name);")
    cursor_target.execute("CREATE INDEX IF NOT EXISTS idx_document_index_term ON document_index (term);")
    cursor_target.execute("CREATE INDEX IF NOT EXISTS idx_document_index_doc_id ON document_index (document_id);")
    
    conn_target.commit()
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏–∏
    total_documents = 0
    duplicates_removed = 0
    content_hashes = set()
    
    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—É—é –∏—Å—Ö–æ–¥–Ω—É—é –±–∞–∑—É
    for db_name in source_databases:
        if not os.path.exists(db_name):
            print(f"‚ö†Ô∏è –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {db_name}")
            continue
            
        print(f"\nüìä –û–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞–∑—ã: {db_name}")
        conn_source = sqlite3.connect(db_name)
        cursor_source = conn_source.cursor()
        
        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
        cursor_source.execute("SELECT * FROM documents;")
        documents = cursor_source.fetchall()
        
        print(f"   –ù–∞–π–¥–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(documents)}")
        
        # –ü–æ–ª—É—á–∞–µ–º —Å—Ö–µ–º—É —Ç–∞–±–ª–∏—Ü—ã documents
        doc_columns = get_table_schema(cursor_source, 'documents')
        print(f"   –ö–æ–ª–æ–Ω–∫–∏: {doc_columns}")
        
        for doc in documents:
            # –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å –∏–∑ –∫–æ—Ä—Ç–µ–∂–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            doc_dict = dict(zip(doc_columns, doc))
            
            # –í—ã—á–∏—Å–ª—è–µ–º —Ö—ç—à —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ
            content_hash = calculate_content_hash(doc_dict.get('content', ''))
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç
            if content_hash in content_hashes:
                duplicates_removed += 1
                print(f"   üîÑ –ü—Ä–æ–ø—É—â–µ–Ω –¥—É–±–ª–∏–∫–∞—Ç: {doc_dict.get('file_name', 'Unknown')}")
                continue
            
            content_hashes.add(content_hash)
            
            # –í—Å—Ç–∞–≤–ª—è–µ–º –¥–æ–∫—É–º–µ–Ω—Ç –≤ —Ü–µ–ª–µ–≤—É—é –±–∞–∑—É
            cursor_target.execute("""
                INSERT INTO documents (
                    file_name, content, file_path, file_size, file_type,
                    category, tags, difficulty_level, content_hash, source_db
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                doc_dict.get('file_name', ''),
                doc_dict.get('content', ''),
                doc_dict.get('file_path', ''),
                doc_dict.get('file_size', 0),
                doc_dict.get('file_type', ''),
                doc_dict.get('category', ''),
                doc_dict.get('tags', ''),
                doc_dict.get('difficulty_level', 'medium'),
                content_hash,
                db_name
            ))
            
            total_documents += 1
        
        # –ú–∏–≥—Ä–∏—Ä—É–µ–º –∏–Ω–¥–µ–∫—Å—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        print(f"   üìá –ú–∏–≥—Ä–∞—Ü–∏—è –∏–Ω–¥–µ–∫—Å–æ–≤...")
        cursor_source.execute("SELECT * FROM document_index;")
        indexes = cursor_source.fetchall()
        
        index_columns = get_table_schema(cursor_source, 'document_index')
        print(f"   –ù–∞–π–¥–µ–Ω–æ –∏–Ω–¥–µ–∫—Å–æ–≤: {len(indexes)}")
        
        for idx in indexes:
            idx_dict = dict(zip(index_columns, idx))
            cursor_target.execute("""
                INSERT INTO document_index (document_id, term, frequency, position)
                VALUES (?, ?, ?, ?)
            """, (
                idx_dict.get('document_id', 0),
                idx_dict.get('term', ''),
                idx_dict.get('frequency', 1),
                idx_dict.get('position', 0)
            ))
        
        # –ú–∏–≥—Ä–∏—Ä—É–µ–º –≤–µ–∫—Ç–æ—Ä—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        print(f"   üßÆ –ú–∏–≥—Ä–∞—Ü–∏—è –≤–µ–∫—Ç–æ—Ä–æ–≤...")
        cursor_source.execute("SELECT * FROM document_vectors;")
        vectors = cursor_source.fetchall()
        
        vector_columns = get_table_schema(cursor_source, 'document_vectors')
        print(f"   –ù–∞–π–¥–µ–Ω–æ –≤–µ–∫—Ç–æ—Ä–æ–≤: {len(vectors)}")
        
        for vec in vectors:
            vec_dict = dict(zip(vector_columns, vec))
            cursor_target.execute("""
                INSERT INTO document_vectors (document_id, vector_data, vector_dimension, model_name)
                VALUES (?, ?, ?, ?)
            """, (
                vec_dict.get('document_id', 0),
                vec_dict.get('vector_data', ''),
                vec_dict.get('vector_dimension', 0),
                vec_dict.get('model_name', '')
            ))
        
        conn_source.close()
    
    # –û–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ–º —Ü–µ–ª–µ–≤—É—é –±–∞–∑—É
    print(f"\n‚ö° –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∫–æ–Ω—Å–æ–ª–∏–¥–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –±–∞–∑—ã...")
    cursor_target.execute("VACUUM;")
    cursor_target.execute("ANALYZE;")
    
    conn_target.commit()
    conn_target.close()
    
    # –ü–æ–ª—É—á–∞–µ–º —Ä–∞–∑–º–µ—Ä—ã
    original_size = sum(os.path.getsize(db) for db in source_databases if os.path.exists(db))
    new_size = os.path.getsize(target_db)
    
    print("\n" + "=" * 60)
    print("üìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ö–û–ù–°–û–õ–ò–î–ê–¶–ò–ò")
    print("=" * 60)
    print(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –±–∞–∑ –¥–∞–Ω–Ω—ã—Ö: {len(source_databases)}")
    print(f"‚úÖ –í—Å–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {total_documents}")
    print(f"‚úÖ –£–¥–∞–ª–µ–Ω–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {duplicates_removed}")
    print(f"‚úÖ –ò—Å—Ö–æ–¥–Ω—ã–π —Ä–∞–∑–º–µ—Ä: {original_size / 1024 / 1024:.1f} MB")
    print(f"‚úÖ –ù–æ–≤—ã–π —Ä–∞–∑–º–µ—Ä: {new_size / 1024 / 1024:.1f} MB")
    print(f"‚úÖ –≠–∫–æ–Ω–æ–º–∏—è –º–µ—Å—Ç–∞: {(original_size - new_size) / 1024 / 1024:.1f} MB")
    print(f"‚úÖ –ü—Ä–æ—Ü–µ–Ω—Ç —ç–∫–æ–Ω–æ–º–∏–∏: {((original_size - new_size) / original_size * 100):.1f}%")
    
    print(f"\nüéØ –ö–æ–Ω—Å–æ–ª–∏–¥–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –±–∞–∑–∞ —Å–æ–∑–¥–∞–Ω–∞: {target_db}")
    print("üì¶ –†–µ–∑–µ—Ä–≤–Ω—ã–µ –∫–æ–ø–∏–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ —Ç–µ–∫—É—â–µ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏")
    
    return target_db

def cleanup_old_databases():
    """–£–¥–∞–ª—è–µ—Ç —Å—Ç–∞—Ä—ã–µ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ —É—Å–ø–µ—à–Ω–æ–π –∫–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏–∏"""
    print("\nüóëÔ∏è –û–ß–ò–°–¢–ö–ê –°–¢–ê–†–´–• –ë–ê–ó –î–ê–ù–ù–´–•")
    print("=" * 40)
    
    old_databases = [
        'rubin_ai_documents.db',
        'rubin_ai_v2.db'
    ]
    
    for db in old_databases:
        if os.path.exists(db):
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Ä–µ–∑–µ—Ä–≤–Ω–∞—è –∫–æ–ø–∏—è —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
            backup_exists = any(f.startswith(db) and 'backup' in f for f in os.listdir('.'))
            if backup_exists:
                os.remove(db)
                print(f"‚úÖ –£–¥–∞–ª–µ–Ω–∞ —Å—Ç–∞—Ä–∞—è –±–∞–∑–∞: {db}")
            else:
                print(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω–∞ –±–∞–∑–∞ –±–µ–∑ —Ä–µ–∑–µ—Ä–≤–Ω–æ–π –∫–æ–ø–∏–∏: {db}")

if __name__ == "__main__":
    try:
        # –ö–æ–Ω—Å–æ–ª–∏–¥–∏—Ä—É–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã
        consolidated_db = consolidate_documents()
        
        # –°–ø—Ä–∞—à–∏–≤–∞–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –æ —É–¥–∞–ª–µ–Ω–∏–∏ —Å—Ç–∞—Ä—ã—Ö –±–∞–∑
        print("\n‚ùì –£–¥–∞–ª–∏—Ç—å —Å—Ç–∞—Ä—ã–µ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö? (y/n): ", end="")
        response = input().lower().strip()
        
        if response in ['y', 'yes', '–¥–∞', '–¥']:
            cleanup_old_databases()
        else:
            print("‚ÑπÔ∏è –°—Ç–∞—Ä—ã–µ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã")
        
        print("\nüéâ –ö–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")
        
    except Exception as e:
        print(f"\n‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∫–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏–∏: {e}")
        print("üì¶ –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Ä–µ–∑–µ—Ä–≤–Ω—ã–µ –∫–æ–ø–∏–∏ –¥–ª—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è")





