#!/usr/bin/env python3
"""
–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –∫–æ–º–ø–µ—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–Ω–æ–π –º–æ–¥–µ–ª–∏ –ò–ò –æ—Ç –ò–¢–ú–û –≤ Rubin AI
"""

import os
import json
import re
from pathlib import Path

class AICompetencyIntegrator:
    def __init__(self, competency_model_path):
        self.competency_model_path = Path(competency_model_path)
        self.ai_roles = {}
        self.ai_competencies = {}
        self.ai_learning_paths = {}
        
    def extract_ai_roles(self):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã—Ö —Ä–æ–ª–µ–π –≤ –æ–±–ª–∞—Å—Ç–∏ –ò–ò"""
        print("üîç –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã—Ö —Ä–æ–ª–µ–π –ò–ò...")
        
        jobs_dir = self.competency_model_path / "Jobs"
        if not jobs_dir.exists():
            print("‚ùå –ü–∞–ø–∫–∞ Jobs –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
            return
            
        for job_file in jobs_dir.glob("*.md"):
            role_name = job_file.stem.replace("job", "")
            print(f"üìã –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–æ–ª–∏: {role_name}")
            
            with open(job_file, 'r', encoding='utf-8') as f:
                content = f.read()
                
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏
            competencies = self._extract_competencies_from_content(content)
            
            self.ai_roles[role_name] = {
                "name": role_name,
                "file": str(job_file),
                "competencies": competencies,
                "description": self._extract_description(content)
            }
            
        print(f"‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–æ {len(self.ai_roles)} —Ä–æ–ª–µ–π")
        
    def _extract_competencies_from_content(self, content):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–π –∏–∑ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ —Ñ–∞–π–ª–∞"""
        competencies = []
        
        # –ò—â–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–π
        competency_pattern = r'### (\d+)\. (.+?)(?=###|\Z)'
        matches = re.findall(competency_pattern, content, re.DOTALL)
        
        for i, (num, competency_text) in enumerate(matches):
            competency = {
                "number": int(num),
                "title": self._extract_competency_title(competency_text),
                "description": self._extract_competency_description(competency_text),
                "indicators": self._extract_indicators(competency_text)
            }
            competencies.append(competency)
            
        return competencies
        
    def _extract_competency_title(self, text):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–∞ –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏"""
        lines = text.strip().split('\n')
        if lines:
            return lines[0].strip()
        return ""
        
    def _extract_competency_description(self, text):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ–ø–∏—Å–∞–Ω–∏—è –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏"""
        # –ò—â–µ–º —Å–µ–∫—Ü–∏—é "–¢—Ä—É–¥–æ–≤–∞—è —Ñ—É–Ω–∫—Ü–∏—è"
        function_match = re.search(r'#### "–¢—Ä—É–¥–æ–≤–∞—è —Ñ—É–Ω–∫—Ü–∏—è"\s*\n(.+?)(?=####|\Z)', text, re.DOTALL)
        if function_match:
            return function_match.group(1).strip()
        return ""
        
    def _extract_indicators(self, text):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤ –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏"""
        indicators = []
        
        # –ò—â–µ–º —Å–µ–∫—Ü–∏—é "–ò–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã"
        indicators_match = re.search(r'#### –ò–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã\s*\n(.+?)(?=####|\Z)', text, re.DOTALL)
        if indicators_match:
            indicators_text = indicators_match.group(1)
            # –†–∞–∑–¥–µ–ª—è–µ–º –ø–æ –º–∞—Ä–∫–µ—Ä–∞–º —Å–ø–∏—Å–∫–∞
            indicator_lines = re.findall(r'\* (.+?)(?=\n\*|\Z)', indicators_text, re.DOTALL)
            indicators = [line.strip() for line in indicator_lines if line.strip()]
            
        return indicators
        
    def _extract_description(self, content):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ–±—â–µ–≥–æ –æ–ø–∏—Å–∞–Ω–∏—è —Ä–æ–ª–∏"""
        lines = content.split('\n')
        for line in lines:
            if line.strip() and not line.startswith('#'):
                return line.strip()
        return ""
        
    def extract_competencies_from_roles(self):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–π –∏–∑ —Ñ–∞–π–ª–∞ —Ä–æ–ª–µ–π"""
        print("üîç –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–π –∏–∑ —Ä–æ–ª–µ–π...")
        
        roles_file = self.competency_model_path / "RolesCompetencies.md"
        if not roles_file.exists():
            print("‚ùå –§–∞–π–ª RolesCompetencies.md –Ω–µ –Ω–∞–π–¥–µ–Ω")
            return
            
        with open(roles_file, 'r', encoding='utf-8') as f:
            content = f.read()
            
        # –ü–∞—Ä—Å–∏–º —Ç–∞–±–ª–∏—Ü—É —Ä–æ–ª–µ–π –∏ –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–π
        self._parse_roles_table(content)
        
    def _parse_roles_table(self, content):
        """–ü–∞—Ä—Å–∏–Ω–≥ —Ç–∞–±–ª–∏—Ü—ã —Ä–æ–ª–µ–π –∏ –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–π"""
        lines = content.split('\n')
        
        for line in lines:
            if '|' in line and not line.startswith('|:') and not line.startswith('| **'):
                parts = [part.strip() for part in line.split('|')]
                if len(parts) >= 4:
                    role = parts[1]
                    function = parts[2]
                    competency = parts[3]
                    indicator = parts[4] if len(parts) > 4 else ""
                    
                    if role and competency:
                        if role not in self.ai_competencies:
                            self.ai_competencies[role] = []
                            
                        self.ai_competencies[role].append({
                            "function": function,
                            "competency": competency,
                            "indicator": indicator
                        })
                        
    def create_ai_knowledge_base(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π –ò–ò –¥–ª—è Rubin AI"""
        print("üß† –°–æ–∑–¥–∞–Ω–∏–µ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π –ò–ò...")
        
        knowledge_base = {
            "ai_roles": self.ai_roles,
            "ai_competencies": self.ai_competencies,
            "learning_paths": self._create_learning_paths(),
            "competency_categories": self._categorize_competencies()
        }
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ JSON
        output_file = Path("ai_competency_knowledge_base.json")
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(knowledge_base, f, ensure_ascii=False, indent=2)
            
        print(f"‚úÖ –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {output_file}")
        return knowledge_base
        
    def _create_learning_paths(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π"""
        learning_paths = {
            "data_engineer": {
                "name": "Data Engineer",
                "description": "–°–ø–µ—Ü–∏–∞–ª–∏—Å—Ç –ø–æ —Ä–∞–±–æ—Ç–µ —Å –¥–∞–Ω–Ω—ã–º–∏",
                "prerequisites": ["Python", "SQL", "–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞"],
                "core_skills": ["ETL", "Data Pipeline", "Data Quality"],
                "advanced_skills": ["Big Data", "DataOps", "ML Pipeline"]
            },
            "ai_architect": {
                "name": "AI Architect", 
                "description": "–ê—Ä—Ö–∏—Ç–µ–∫—Ç–æ—Ä —Å–∏—Å—Ç–µ–º –ò–ò",
                "prerequisites": ["ML", "Software Architecture", "Cloud"],
                "core_skills": ["System Design", "MLOps", "AI Strategy"],
                "advanced_skills": ["AI Governance", "Ethics", "Risk Management"]
            },
            "ml_engineer": {
                "name": "ML Engineer",
                "description": "–ò–Ω–∂–µ–Ω–µ—Ä –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è",
                "prerequisites": ["Python", "ML", "DevOps"],
                "core_skills": ["Model Deployment", "MLOps", "Model Monitoring"],
                "advanced_skills": ["Distributed ML", "Model Optimization", "A/B Testing"]
            }
        }
        
        return learning_paths
        
    def _categorize_competencies(self):
        """–ö–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–π"""
        categories = {
            "technical": ["Python", "ML", "Deep Learning", "Data Engineering"],
            "business": ["AI Strategy", "Business Analysis", "Project Management"],
            "mathematical": ["Statistics", "Linear Algebra", "Optimization"],
            "soft_skills": ["Communication", "Leadership", "Problem Solving"]
        }
        
        return categories
        
    def generate_rubin_ai_responses(self):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–æ–≤ Rubin AI –¥–ª—è –ò–ò –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–π"""
        print("ü§ñ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–æ–≤ Rubin AI...")
        
        responses = {}
        
        # –û—Ç–≤–µ—Ç—ã –ø–æ —Ä–æ–ª—è–º
        for role_name, role_data in self.ai_roles.items():
            responses[f"—á—Ç–æ —Ç–∞–∫–æ–µ {role_name.lower()}"] = self._generate_role_response(role_name, role_data)
            responses[f"—Ä–æ–ª—å {role_name.lower()}"] = self._generate_role_response(role_name, role_data)
            
        # –û—Ç–≤–µ—Ç—ã –ø–æ –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏—è–º
        for role, competencies in self.ai_competencies.items():
            for comp in competencies:
                key = f"–∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏—è {comp['competency'][:50].lower()}"
                responses[key] = self._generate_competency_response(comp)
                
        # –û–±—â–∏–µ –æ—Ç–≤–µ—Ç—ã
        responses["–∏–∏ –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏"] = self._generate_general_ai_response()
        responses["–ø—Ä–æ—Ñ–µ—Å—Å–∏–∏ –≤ –∏–∏"] = self._generate_professions_response()
        
        return responses
        
    def _generate_role_response(self, role_name, role_data):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ –æ —Ä–æ–ª–∏"""
        response = f"**{role_name.upper()}** - –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–∞—è —Ä–æ–ª—å –≤ –æ–±–ª–∞—Å—Ç–∏ –ò–ò.\n\n"
        
        if role_data.get("description"):
            response += f"**–û–ø–∏—Å–∞–Ω–∏–µ:** {role_data['description']}\n\n"
            
        if role_data.get("competencies"):
            response += f"**–û—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏:**\n"
            for comp in role_data["competencies"][:3]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 3
                response += f"‚Ä¢ {comp['title']}\n"
                
        response += f"\n**–í—Å–µ–≥–æ –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–π:** {len(role_data.get('competencies', []))}"
        
        return response
        
    def _generate_competency_response(self, competency):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ –æ –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏"""
        response = f"**–ö–æ–º–ø–µ—Ç–µ–Ω—Ü–∏—è:** {competency['competency']}\n\n"
        
        if competency.get("function"):
            response += f"**–¢—Ä—É–¥–æ–≤–∞—è —Ñ—É–Ω–∫—Ü–∏—è:** {competency['function']}\n\n"
            
        if competency.get("indicator"):
            response += f"**–ò–Ω–¥–∏–∫–∞—Ç–æ—Ä:** {competency['indicator']}"
            
        return response
        
    def _generate_general_ai_response(self):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ–±—â–µ–≥–æ –æ—Ç–≤–µ—Ç–∞ –æ–± –ò–ò –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏—è—Ö"""
        response = "**–ö–û–ú–ü–ï–¢–ï–ù–¢–ù–û–°–¢–ù–ê–Ø –ú–û–î–ï–õ–¨ –ò–ò** (–ò–¢–ú–û)\n\n"
        response += "–°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã—Ö —Ä–æ–ª–µ–π –∏ –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–π –≤ –æ–±–ª–∞—Å—Ç–∏ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞.\n\n"
        
        response += "**–û—Å–Ω–æ–≤–Ω—ã–µ —Ä–æ–ª–∏:**\n"
        for role_name in self.ai_roles.keys():
            response += f"‚Ä¢ {role_name}\n"
            
        response += f"\n**–í—Å–µ–≥–æ —Ä–æ–ª–µ–π:** {len(self.ai_roles)}"
        response += f"\n**–í—Å–µ–≥–æ –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–π:** {sum(len(comps) for comps in self.ai_competencies.values())}"
        
        return response
        
    def _generate_professions_response(self):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ –æ –ø—Ä–æ—Ñ–µ—Å—Å–∏—è—Ö –≤ –ò–ò"""
        response = "**–ü–†–û–§–ï–°–°–ò–ò –í –û–ë–õ–ê–°–¢–ò –ò–ò**\n\n"
        
        for role_name, role_data in self.ai_roles.items():
            response += f"**{role_name}**\n"
            if role_data.get("description"):
                response += f"{role_data['description']}\n"
            response += f"–ö–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–π: {len(role_data.get('competencies', []))}\n\n"
            
        return response
        
    def run_integration(self):
        """–ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–π –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏"""
        print("üöÄ –ù–∞—á–∏–Ω–∞–µ–º –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—é –∫–æ–º–ø–µ—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–Ω–æ–π –º–æ–¥–µ–ª–∏ –ò–ò...")
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        self.extract_ai_roles()
        self.extract_competencies_from_roles()
        
        # –°–æ–∑–¥–∞–µ–º –±–∞–∑—É –∑–Ω–∞–Ω–∏–π
        knowledge_base = self.create_ai_knowledge_base()
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç—ã
        responses = self.generate_rubin_ai_responses()
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç–≤–µ—Ç—ã
        responses_file = Path("ai_competency_responses.json")
        with open(responses_file, 'w', encoding='utf-8') as f:
            json.dump(responses, f, ensure_ascii=False, indent=2)
            
        print(f"‚úÖ –û—Ç–≤–µ—Ç—ã Rubin AI —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {responses_file}")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        print(f"\nüìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ò–ù–¢–ï–ì–†–ê–¶–ò–ò:")
        print(f"‚Ä¢ –†–æ–ª–µ–π –ò–ò: {len(self.ai_roles)}")
        print(f"‚Ä¢ –ö–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–π: {sum(len(comps) for comps in self.ai_competencies.values())}")
        print(f"‚Ä¢ –û—Ç–≤–µ—Ç–æ–≤ Rubin AI: {len(responses)}")
        
        return knowledge_base, responses

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    competency_model_path = r"C:\Users\elekt\OneDrive\Desktop\ai-competency-model-main"
    
    if not os.path.exists(competency_model_path):
        print(f"‚ùå –ü—É—Ç—å –Ω–µ –Ω–∞–π–¥–µ–Ω: {competency_model_path}")
        return
        
    integrator = AICompetencyIntegrator(competency_model_path)
    knowledge_base, responses = integrator.run_integration()
    
    print("\nüéâ –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")

if __name__ == "__main__":
    main()
