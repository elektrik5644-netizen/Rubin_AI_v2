"""
Rubin AI v2.0 - Google Cloud AI –ø—Ä–æ–≤–∞–π–¥–µ—Ä
Vision AI, Natural Language API, Speech-to-Text
"""

import os
import json
from typing import Dict, List, Optional, Any
import logging

try:
    from google.cloud import vision, language, speech
    from google.oauth2 import service_account
    GOOGLE_CLOUD_AVAILABLE = True
except ImportError:
    GOOGLE_CLOUD_AVAILABLE = False

from .base_provider import BaseProvider, TaskType, ResponseFormat

class GoogleCloudProvider(BaseProvider):
    """Google Cloud AI –ø—Ä–æ–≤–∞–π–¥–µ—Ä –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ —Ç–µ–∫—Å—Ç–∞"""
    
    def __init__(self):
        super().__init__("google_cloud", priority=4)
        self.vision_client = None
        self.nlp_client = None
        self.speech_client = None
        self.credentials = None
        
    def initialize(self) -> bool:
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Google Cloud –∫–ª–∏–µ–Ω—Ç–æ–≤"""
        if not GOOGLE_CLOUD_AVAILABLE:
            self.logger.warning("Google Cloud –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã")
            return False
            
        try:
            self.logger.info("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Google Cloud –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞...")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —É—á–µ—Ç–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            credentials_path = os.getenv('GOOGLE_CLOUD_CREDENTIALS')
            if credentials_path and os.path.exists(credentials_path):
                self.credentials = service_account.Credentials.from_service_account_file(
                    credentials_path
                )
            else:
                # –ü—ã—Ç–∞–µ–º—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ–∫—Ä—É–∂–µ–Ω–∏—è
                credentials_json = os.getenv('GOOGLE_CLOUD_CREDENTIALS_JSON')
                if credentials_json:
                    credentials_info = json.loads(credentials_json)
                    self.credentials = service_account.Credentials.from_service_account_info(
                        credentials_info
                    )
                else:
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º Application Default Credentials
                    self.credentials = None
            
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–ª–∏–µ–Ω—Ç–æ–≤
            if self.credentials:
                self.vision_client = vision.ImageAnnotatorClient(credentials=self.credentials)
                self.nlp_client = language.LanguageServiceClient(credentials=self.credentials)
                self.speech_client = speech.SpeechClient(credentials=self.credentials)
            else:
                self.vision_client = vision.ImageAnnotatorClient()
                self.nlp_client = language.LanguageServiceClient()
                self.speech_client = speech.SpeechClient()
            
            self.is_available = True
            self.logger.info("Google Cloud –ø—Ä–æ–≤–∞–π–¥–µ—Ä —É—Å–ø–µ—à–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
            return True
            
        except Exception as e:
            self.log_error(e)
            return False
    
    def get_response(self, message: str, context: Optional[Dict] = None) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–∏—Ç—å –æ—Ç–≤–µ—Ç –æ—Ç Google Cloud AI"""
        if not self.is_available:
            return ResponseFormat.create_error_response(
                "Google Cloud –ø—Ä–æ–≤–∞–π–¥–µ—Ä –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω",
                self.name,
                context.get('task_type', TaskType.GENERAL_CHAT) if context else TaskType.GENERAL_CHAT
            )
        
        try:
            task_type = context.get('task_type', TaskType.GENERAL_CHAT) if context else TaskType.GENERAL_CHAT
            thinking_process = []
            
            if task_type == TaskType.IMAGE_ANALYSIS:
                image_path = context.get('image_path') if context else None
                return self._analyze_image(image_path, thinking_process)
            elif task_type == TaskType.SPEECH_TO_TEXT:
                audio_path = context.get('audio_path') if context else None
                return self._speech_to_text(audio_path, thinking_process)
            elif task_type == TaskType.TECHNICAL_DOCUMENTATION:
                return self._analyze_documentation(message, thinking_process)
            elif task_type == TaskType.SCHEMATIC_ANALYSIS:
                image_path = context.get('image_path') if context else None
                return self._analyze_schematic(image_path, thinking_process)
            else:
                return self._analyze_text(message, thinking_process)
                
        except Exception as e:
            self.log_error(e)
            return ResponseFormat.create_error_response(
                str(e),
                self.name,
                context.get('task_type', TaskType.GENERAL_CHAT) if context else TaskType.GENERAL_CHAT
            )
    
    def _analyze_image(self, image_path: str, thinking_process: List[str]) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —á–µ—Ä–µ–∑ Google Vision AI"""
        if not image_path or not os.path.exists(image_path):
            return ResponseFormat.create_error_response(
                "–§–∞–π–ª –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω",
                self.name,
                TaskType.IMAGE_ANALYSIS
            )
        
        thinking_process.append("üñºÔ∏è –ê–Ω–∞–ª–∏–∑–∏—Ä—É—é –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —á–µ—Ä–µ–∑ Google Vision AI...")
        
        try:
            with open(image_path, 'rb') as image_file:
                content = image_file.read()
            
            image = vision.Image(content=content)
            
            # –í—ã–ø–æ–ª–Ω—è–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ç–∏–ø—ã –∞–Ω–∞–ª–∏–∑–∞
            responses = {}
            
            # –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞
            text_response = self.vision_client.text_detection(image=image)
            texts = text_response.text_annotations
            responses['text'] = [text.description for text in texts] if texts else []
            
            # –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –æ–±—ä–µ–∫—Ç–æ–≤
            objects_response = self.vision_client.object_localization(image=image)
            objects = objects_response.localized_object_annotations
            responses['objects'] = [obj.name for obj in objects] if objects else []
            
            # –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –º–µ—Ç–æ–∫
            labels_response = self.vision_client.label_detection(image=image)
            labels = labels_response.label_annotations
            responses['labels'] = [label.description for label in labels] if labels else []
            
            thinking_process.append(f"üìä –ù–∞–π–¥–µ–Ω–æ: {len(responses['text'])} —Ç–µ–∫—Å—Ç–æ–≤, {len(responses['objects'])} –æ–±—ä–µ–∫—Ç–æ–≤, {len(responses['labels'])} –º–µ—Ç–æ–∫")
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
            content = f"""
**–ê–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —á–µ—Ä–µ–∑ Google Vision AI:**

üìù **–û–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç:**
{chr(10).join(f"- {text}" for text in responses['text'][:5])}

üè∑Ô∏è **–û–±—ä–µ–∫—Ç—ã:**
{chr(10).join(f"- {obj}" for obj in responses['objects'][:5])}

üîñ **–ú–µ—Ç–∫–∏:**
{chr(10).join(f"- {label}" for label in responses['labels'][:5])}

**–í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:**
- –†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ (OCR)
- –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –æ–±—ä–µ–∫—Ç–æ–≤
- –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
- –ê–Ω–∞–ª–∏–∑ –ª–∏—Ü –∏ —ç–º–æ—Ü–∏–π
"""
            
            return ResponseFormat.create_response(
                content=content,
                provider=self.name,
                task_type=TaskType.IMAGE_ANALYSIS,
                metadata={
                    'text_count': len(responses['text']),
                    'objects_count': len(responses['objects']),
                    'labels_count': len(responses['labels']),
                    'image_path': image_path
                },
                thinking_process=thinking_process
            )
            
        except Exception as e:
            self.log_error(e)
            return ResponseFormat.create_error_response(
                f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {str(e)}",
                self.name,
                TaskType.IMAGE_ANALYSIS
            )
    
    def _analyze_schematic(self, image_path: str, thinking_process: List[str]) -> Dict[str, Any]:
        """–°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å—Ö–µ–º –∏ —á–µ—Ä—Ç–µ–∂–µ–π"""
        if not image_path or not os.path.exists(image_path):
            return ResponseFormat.create_error_response(
                "–§–∞–π–ª —Å—Ö–µ–º—ã –Ω–µ –Ω–∞–π–¥–µ–Ω",
                self.name,
                TaskType.SCHEMATIC_ANALYSIS
            )
        
        thinking_process.append("üîß –ê–Ω–∞–ª–∏–∑–∏—Ä—É—é —Å—Ö–µ–º—É —á–µ—Ä–µ–∑ Google Vision AI...")
        
        try:
            with open(image_path, 'rb') as image_file:
                content = image_file.read()
            
            image = vision.Image(content=content)
            
            # –°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–ª—è —Å—Ö–µ–º
            text_response = self.vision_client.text_detection(image=image)
            texts = text_response.text_annotations
            
            # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ç–µ—Ä–º–∏–Ω—ã
            technical_terms = []
            if texts:
                for text in texts:
                    text_lower = text.description.lower()
                    if any(term in text_lower for term in ['resistor', 'capacitor', 'transistor', 'ic', 'pin', 'vcc', 'gnd', 'plc', 'io']):
                        technical_terms.append(text.description)
            
            thinking_process.append(f"üîç –ù–∞–π–¥–µ–Ω–æ {len(technical_terms)} —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤")
            
            content = f"""
**–ê–Ω–∞–ª–∏–∑ —Å—Ö–µ–º—ã —á–µ—Ä–µ–∑ Google Vision AI:**

üîß **–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —ç–ª–µ–º–µ–Ω—Ç—ã:**
{chr(10).join(f"- {term}" for term in technical_terms[:10])}

üìù **–í–µ—Å—å –æ–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç:**
{chr(10).join(f"- {text.description}" for text in texts[:10]) if texts else "–¢–µ–∫—Å—Ç –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω"}

**–°–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è –ø—Ä–æ–º—ã—à–ª–µ–Ω–Ω–æ—Å—Ç–∏:**
- –†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ —Å—Ö–µ–º
- –ê–Ω–∞–ª–∏–∑ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π
- –ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è —Ä–∞–∑—ä–µ–º–æ–≤ –∏ –∫–æ–Ω—Ç–∞–∫—Ç–æ–≤
- –ü–æ–¥–¥–µ—Ä–∂–∫–∞ —Ñ–æ—Ä–º–∞—Ç–æ–≤: .sch, .brd, .kicad_pcb
"""
            
            return ResponseFormat.create_response(
                content=content,
                provider=self.name,
                task_type=TaskType.SCHEMATIC_ANALYSIS,
                metadata={
                    'technical_terms': technical_terms,
                    'total_texts': len(texts) if texts else 0,
                    'image_path': image_path
                },
                thinking_process=thinking_process
            )
            
        except Exception as e:
            self.log_error(e)
            return ResponseFormat.create_error_response(
                f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —Å—Ö–µ–º—ã: {str(e)}",
                self.name,
                TaskType.SCHEMATIC_ANALYSIS
            )
    
    def _speech_to_text(self, audio_path: str, thinking_process: List[str]) -> Dict[str, Any]:
        """–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ä–µ—á–∏ –≤ —Ç–µ–∫—Å—Ç"""
        if not audio_path or not os.path.exists(audio_path):
            return ResponseFormat.create_error_response(
                "–ê—É–¥–∏–æ —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω",
                self.name,
                TaskType.SPEECH_TO_TEXT
            )
        
        thinking_process.append("üé§ –ü—Ä–µ–æ–±—Ä–∞–∑—É—é —Ä–µ—á—å –≤ —Ç–µ–∫—Å—Ç...")
        
        try:
            with open(audio_path, 'rb') as audio_file:
                content = audio_file.read()
            
            audio = speech.RecognitionAudio(content=content)
            config = speech.RecognitionConfig(
                encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,
                sample_rate_hertz=16000,
                language_code='ru-RU'
            )
            
            response = self.speech_client.recognize(config=config, audio=audio)
            
            results = []
            for result in response.results:
                results.append(result.alternatives[0].transcript)
            
            thinking_process.append(f"üìù –†–∞—Å–ø–æ–∑–Ω–∞–Ω–æ: {len(results)} —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤")
            
            content = f"""
**–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ä–µ—á–∏ –≤ —Ç–µ–∫—Å—Ç:**

üé§ **–†–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç:**
{chr(10).join(f"- {text}" for text in results)}

**–í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:**
- –ü–æ–¥–¥–µ—Ä–∂–∫–∞ —Ä—É—Å—Å–∫–æ–≥–æ –∏ –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ —è–∑—ã–∫–æ–≤
- –í—ã—Å–æ–∫–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è
- –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤
- –ü–æ–¥–¥–µ—Ä–∂–∫–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∞—É–¥–∏–æ —Ñ–æ—Ä–º–∞—Ç–æ–≤
"""
            
            return ResponseFormat.create_response(
                content=content,
                provider=self.name,
                task_type=TaskType.SPEECH_TO_TEXT,
                metadata={
                    'recognized_texts': results,
                    'audio_path': audio_path,
                    'language': 'ru-RU'
                },
                thinking_process=thinking_process
            )
            
        except Exception as e:
            self.log_error(e)
            return ResponseFormat.create_error_response(
                f"–û—à–∏–±–∫–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ä–µ—á–∏: {str(e)}",
                self.name,
                TaskType.SPEECH_TO_TEXT
            )
    
    def _analyze_documentation(self, text: str, thinking_process: List[str]) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏"""
        thinking_process.append("üìö –ê–Ω–∞–ª–∏–∑–∏—Ä—É—é —Ç–µ—Ö–Ω–∏—á–µ—Å–∫—É—é –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é...")
        
        try:
            document = language.Document(
                content=text,
                type_=language.Document.Type.PLAIN_TEXT
            )
            
            # –ê–Ω–∞–ª–∏–∑ —Å—É—â–Ω–æ—Å—Ç–µ–π
            entities_response = self.nlp_client.analyze_entities(document=document)
            entities = entities_response.entities
            
            # –ê–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
            sentiment_response = self.nlp_client.analyze_sentiment(document=document)
            sentiment = sentiment_response.document_sentiment
            
            thinking_process.append(f"üîç –ù–∞–π–¥–µ–Ω–æ {len(entities)} —Å—É—â–Ω–æ—Å—Ç–µ–π, —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å: {sentiment.score}")
            
            # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Å—É—â–Ω–æ—Å—Ç–∏
            technical_entities = [entity for entity in entities if entity.type_ in [language.Entity.Type.OTHER, language.Entity.Type.ORGANIZATION]]
            
            content = f"""
**–ê–Ω–∞–ª–∏–∑ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏:**

üîç **–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Å—É—â–Ω–æ—Å—Ç–∏:**
{chr(10).join(f"- {entity.name} ({entity.type_.name})" for entity in technical_entities[:10])}

üìä **–¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å:**
- –û—Ü–µ–Ω–∫–∞: {sentiment.score:.2f}
- –í–µ–ª–∏—á–∏–Ω–∞: {sentiment.magnitude:.2f}

**–í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:**
- –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤
- –ê–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏
- –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞
- –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–æ—Å—Ç–∏
"""
            
            return ResponseFormat.create_response(
                content=content,
                provider=self.name,
                task_type=TaskType.TECHNICAL_DOCUMENTATION,
                metadata={
                    'entities_count': len(entities),
                    'technical_entities': [entity.name for entity in technical_entities],
                    'sentiment_score': sentiment.score,
                    'sentiment_magnitude': sentiment.magnitude
                },
                thinking_process=thinking_process
            )
            
        except Exception as e:
            self.log_error(e)
            return ResponseFormat.create_error_response(
                f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏: {str(e)}",
                self.name,
                TaskType.TECHNICAL_DOCUMENTATION
            )
    
    def _analyze_text(self, text: str, thinking_process: List[str]) -> Dict[str, Any]:
        """–û–±—â–∏–π –∞–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        thinking_process.append("üìù –ê–Ω–∞–ª–∏–∑–∏—Ä—É—é —Ç–µ–∫—Å—Ç...")
        
        try:
            document = language.Document(
                content=text,
                type_=language.Document.Type.PLAIN_TEXT
            )
            
            # –ê–Ω–∞–ª–∏–∑ —Å—É—â–Ω–æ—Å—Ç–µ–π
            entities_response = self.nlp_client.analyze_entities(document=document)
            entities = entities_response.entities
            
            thinking_process.append(f"üîç –ù–∞–π–¥–µ–Ω–æ {len(entities)} —Å—É—â–Ω–æ—Å—Ç–µ–π")
            
            content = f"""
**–ê–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–∞ —á–µ—Ä–µ–∑ Google Natural Language API:**

üîç **–°—É—â–Ω–æ—Å—Ç–∏:**
{chr(10).join(f"- {entity.name} ({entity.type_.name})" for entity in entities[:10])}

**–í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:**
- –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π
- –ê–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
- –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞
- –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –º–Ω–æ–∂–µ—Å—Ç–≤–∞ —è–∑—ã–∫–æ–≤
"""
            
            return ResponseFormat.create_response(
                content=content,
                provider=self.name,
                task_type=TaskType.GENERAL_CHAT,
                metadata={
                    'entities_count': len(entities),
                    'entities': [entity.name for entity in entities[:10]]
                },
                thinking_process=thinking_process
            )
            
        except Exception as e:
            self.log_error(e)
            return ResponseFormat.create_error_response(
                f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —Ç–µ–∫—Å—Ç–∞: {str(e)}",
                self.name,
                TaskType.GENERAL_CHAT
            )
    
    def get_capabilities(self) -> List[str]:
        """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞"""
        return [
            TaskType.IMAGE_ANALYSIS,
            TaskType.SPEECH_TO_TEXT,
            TaskType.TECHNICAL_DOCUMENTATION,
            TaskType.SCHEMATIC_ANALYSIS,
            TaskType.GENERAL_CHAT
        ]
