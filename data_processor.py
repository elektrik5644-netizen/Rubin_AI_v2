#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
–ú–æ–¥—É–ª—å –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è Rubin AI v2.0
–°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞, –∞–Ω–∞–ª–∏–∑ –∏ –æ—á–∏—Å—Ç–∫–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ—Ç –¥–∏—Å–ø–µ—Ç—á–µ—Ä–∞
"""

import re
import logging
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass
import chardet
import langdetect
from langdetect import detect, DetectorFactory
from rubin_text_preprocessor import RubinTextPreprocessor # –î–æ–±–∞–≤–ª—è–µ–º –¥–ª—è —Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–π –æ—á–∏—Å—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞

# –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ä–µ–∂–∏–º –¥–ª—è langdetect
DetectorFactory.seed = 0

@dataclass
class ProcessedContent:
    """–û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç"""
    original_content: str
    cleaned_content: str
    language: str
    format_type: str
    quality_score: float
    readability_score: float
    metadata: Dict[str, Any]
    filtered_sections: List[str]
    valid_sections: List[str]

class DataProcessor:
    """–ü—Ä–æ—Ü–µ—Å—Å–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ –∏ –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
    
    def __init__(self):
        self.setup_logging()
        self.text_preprocessor = RubinTextPreprocessor() # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞ —Ç–µ–∫—Å—Ç–∞
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
        self.noise_patterns = [
            r'^[\s\-_=*]+$',  # –¢–æ–ª—å–∫–æ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏
            r'^\d+\.?\s*$',   # –¢–æ–ª—å–∫–æ –Ω–æ–º–µ—Ä–∞
            r'^[;#]+.*$',     # –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏
            r'^[\s]*$',       # –ü—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
            r'^.*\.(txt|pdf|docx):.*$',  # –ù–∞–∑–≤–∞–Ω–∏—è —Ñ–∞–π–ª–æ–≤
            r'^üìÑ.*$',        # –≠–º–æ–¥–∑–∏ —Ñ–∞–π–ª–æ–≤
            r'^.*\*\*.*\.txt\*\*.*$',    # Markdown –Ω–∞–∑–≤–∞–Ω–∏—è —Ñ–∞–π–ª–æ–≤
        ]
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ñ–æ—Ä–º–∞—Ç–∞
        self.format_patterns = {
            'code': [
                r'^\s*[{}()\[\]]',  # –°–∫–æ–±–∫–∏ –≤ –Ω–∞—á–∞–ª–µ —Å—Ç—Ä–æ–∫–∏
                r'^\s*(if|for|while|def|class|import|from)\s',  # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ Python
                r'^\s*[a-zA-Z_][a-zA-Z0-9_]*\s*[=<>!]',  # –ü—Ä–∏—Å–≤–∞–∏–≤–∞–Ω–∏—è
            ],
            'markdown': [
                r'^#{1,6}\s+',  # –ó–∞–≥–æ–ª–æ–≤–∫–∏
                r'^\*\*.*\*\*$',  # –ñ–∏—Ä–Ω—ã–π —Ç–µ–∫—Å—Ç
                r'^\*.*\*$',  # –ö—É—Ä—Å–∏–≤
                r'^\- ',  # –°–ø–∏—Å–∫–∏
            ],
            'technical': [
                r'^\s*[A-Z][A-Z_\s]+$',  # –ó–∞–≥–æ–ª–æ–≤–∫–∏ –≤ –≤–µ—Ä—Ö–Ω–µ–º —Ä–µ–≥–∏—Å—Ç—Ä–µ
                r'^\s*\d+\.\d+\.\d+',  # –í–µ—Ä—Å–∏–∏
                r'^\s*[A-Z][a-z]+\s*:',  # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã
            ],
            'plain_text': []  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é
        }
        
        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞
        self.quality_indicators = {
            'high': ['–ø—Ä–∏–Ω—Ü–∏–ø', '–º–µ—Ç–æ–¥', '–∞–ª–≥–æ—Ä–∏—Ç–º', '—Ñ—É–Ω–∫—Ü–∏—è', '–ø—Ä–æ—Ü–µ—Å—Å', '—Å–∏—Å—Ç–µ–º–∞'],
            'medium': ['–æ–ø–∏—Å–∞–Ω–∏–µ', '—Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞', '–ø–∞—Ä–∞–º–µ—Ç—Ä', '–Ω–∞—Å—Ç—Ä–æ–π–∫–∞'],
            'low': ['–∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π', '–ø—Ä–∏–º–µ—á–∞–Ω–∏–µ', '–∑–∞–º–µ—Ç–∫–∞', '–≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–π']
        }
    
    def setup_logging(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        self.logger = logging.getLogger(__name__)
    
    def process_search_results(self, search_results: List[Dict]) -> ProcessedContent:
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞"""
        self.logger.info(f"üîÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ {len(search_results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞")
        
        # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –≤—Å–µ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        all_content = self._extract_content(search_results)
        
        # –û—á–∏—Å—Ç–∫–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        cleaned_content = self._clean_content(all_content)
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —è–∑—ã–∫–∞ –∏ —Ñ–æ—Ä–º–∞—Ç–∞
        language = self._detect_language(cleaned_content)
        format_type = self._detect_format(cleaned_content)
        
        # –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
        quality_score = self._assess_quality(cleaned_content)
        readability_score = self._assess_readability(cleaned_content)
        
        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Å–µ–∫—Ü–∏–π
        filtered_sections, valid_sections = self._filter_sections(cleaned_content)
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
        metadata = {
            'original_sections': len(search_results),
            'filtered_sections': len(filtered_sections),
            'valid_sections': len(valid_sections),
            'processing_time': 0,  # –ë—É–¥–µ—Ç –∑–∞–ø–æ–ª–Ω–µ–Ω–æ
            'confidence': quality_score
        }
        
        processed_content = ProcessedContent(
            original_content=all_content,
            cleaned_content=cleaned_content,
            language=language,
            format_type=format_type,
            quality_score=quality_score,
            readability_score=readability_score,
            metadata=metadata,
            filtered_sections=filtered_sections,
            valid_sections=valid_sections
        )
        
        self.logger.info(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞: –∫–∞—á–µ—Å—Ç–≤–æ {quality_score:.2f}, —á–∏—Ç–∞–µ–º–æ—Å—Ç—å {readability_score:.2f}")
        
        return processed_content
    
    def _extract_content(self, search_results: List[Dict]) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞"""
        content_parts = []
        
        for result in search_results:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –ø–æ–ª–Ω–æ–º—É –∫–æ–Ω—Ç–µ–Ω—Ç—É)
            content = result.get('content', '')
            if not content:
                content = result.get('content_preview', '')
            
            if content:
                # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∏—Å—Ç–æ—á–Ω–∏–∫–µ (—Å–∫—Ä—ã—Ç–æ)
                source_info = f"[–ò—Å—Ç–æ—á–Ω–∏–∫: {result.get('file_name', 'Unknown')}]"
                content_parts.append(f"{content}\n{source_info}")
        
        return '\n\n'.join(content_parts)
    
    def _clean_content(self, content: str) -> str:
        """–û—á–∏—Å—Ç–∫–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –æ—Ç –ª–∏—à–Ω–µ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏"""
        if not content:
            return ""
        
        lines = content.split('\n')
        cleaned_lines = []
        
        for line in lines:
            line = line.strip()
            
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
            if not line:
                continue
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã —à—É–º–∞
            is_noise = False
            for pattern in self.noise_patterns:
                if re.match(pattern, line, re.IGNORECASE):
                    is_noise = True
                    break
            
            if not is_noise:
                # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞
                cleaned_line = self._clean_line(line)
                if cleaned_line:
                    cleaned_lines.append(cleaned_line)
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å—Ç—Ä–æ–∫–∏
        cleaned_content = '\n'.join(cleaned_lines)
        
        # –£–¥–∞–ª—è–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã –∏ –ø–µ—Ä–µ–Ω–æ—Å—ã
        cleaned_content = self.text_preprocessor.remove_extra_spaces(cleaned_content) # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–æ–≤—ã–π –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä
        cleaned_content = re.sub(r'\n{3,}', '\n\n', cleaned_content)
        
        return cleaned_content.strip()
    
    def _clean_line(self, line: str) -> str:
        """–û—á–∏—Å—Ç–∫–∞ –æ—Ç–¥–µ–ª—å–Ω–æ–π —Å—Ç—Ä–æ–∫–∏"""
        # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ —Å–∏–º–≤–æ–ª—ã –≤ –Ω–∞—á–∞–ª–µ
        line = re.sub(r'^[\s\-_=*]+', '', line)
        
        # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ —Å–∏–º–≤–æ–ª—ã –≤ –∫–æ–Ω—Ü–µ
        line = re.sub(r'[\s\-_=*]+$', '', line)
        
        # –£–¥–∞–ª—è–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã
        line = re.sub(r'\s+', ' ', line)
        
        # –£–¥–∞–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –∏—Å—Ç–æ—á–Ω–∏–∫–µ (—Å–∫—Ä—ã—Ç–æ)
        line = re.sub(r'\[–ò—Å—Ç–æ—á–Ω–∏–∫:.*?\]', '', line)
        
        return line.strip()
    
    def _detect_language(self, content: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —è–∑—ã–∫–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        try:
            if not content or len(content.strip()) < 10:
                return 'unknown'
            
            # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 1000 —Å–∏–º–≤–æ–ª–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            sample = content[:1000]
            language = detect(sample)
            return language
        except Exception as e:
            self.logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å —è–∑—ã–∫: {e}")
            return 'unknown'
    
    def _detect_format(self, content: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ñ–æ—Ä–º–∞—Ç–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        lines = content.split('\n')
        format_scores = {format_name: 0 for format_name in self.format_patterns.keys()}
        
        for line in lines[:50]:  # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–µ—Ä–≤—ã–µ 50 —Å—Ç—Ä–æ–∫
            for format_name, patterns in self.format_patterns.items():
                for pattern in patterns:
                    if re.search(pattern, line):
                        format_scores[format_name] += 1
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ñ–æ—Ä–º–∞—Ç —Å –Ω–∞–∏–±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π
        if format_scores:
            return max(format_scores, key=format_scores.get)
        return 'plain_text'
    
    def _assess_quality(self, content: str) -> float:
        """–û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        if not content:
            return 0.0
        
        quality_score = 0.5  # –ë–∞–∑–æ–≤—ã–π score
        
        # –ê–Ω–∞–ª–∏–∑ –¥–ª–∏–Ω—ã –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        content_length = len(content)
        if content_length > 5000:
            quality_score += 0.2
        elif content_length > 2000:
            quality_score += 0.1
        
        # –ê–Ω–∞–ª–∏–∑ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        content_lower = content.lower()
        for level, keywords in self.quality_indicators.items():
            for keyword in keywords:
                if keyword in content_lower:
                    if level == 'high':
                        quality_score += 0.1
                    elif level == 'medium':
                        quality_score += 0.05
                    else:
                        quality_score += 0.02
        
        # –ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
        lines = content.split('\n')
        if len(lines) > 5:
            quality_score += 0.1
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –Ω–∞–ª–∏—á–∏–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
        technical_indicators = ['–ø–∞—Ä–∞–º–µ—Ç—Ä', '–Ω–∞—Å—Ç—Ä–æ–π–∫–∞', '–∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è', '–∞–ª–≥–æ—Ä–∏—Ç–º', '—Ñ—É–Ω–∫—Ü–∏—è']
        for indicator in technical_indicators:
            if indicator in content_lower:
                quality_score += 0.05
                break
        
        return min(1.0, quality_score)
    
    def _assess_readability(self, content: str) -> float:
        """–û—Ü–µ–Ω–∫–∞ —á–∏—Ç–∞–µ–º–æ—Å—Ç–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        if not content:
            return 0.0
        
        lines = content.split('\n')
        readability_score = 1.0  # –ë–∞–∑–æ–≤—ã–π score
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            # –®—Ç—Ä–∞—Ñ –∑–∞ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏
            if len(line) > 100:
                readability_score -= 0.05
            
            # –®—Ç—Ä–∞—Ñ –∑–∞ —Å—Ç—Ä–æ–∫–∏ —Ç–æ–ª—å–∫–æ –∏–∑ —Å–∏–º–≤–æ–ª–æ–≤
            if re.match(r'^[^\w\s]+$', line):
                readability_score -= 0.1
            
            # –®—Ç—Ä–∞—Ñ –∑–∞ —Å—Ç—Ä–æ–∫–∏ —Å –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–º–∏ —Å–∏–º–≤–æ–ª–∞–º–∏
            special_chars = len(re.findall(r'[^\w\s]', line))
            if special_chars > len(line) * 0.3:
                readability_score -= 0.05
        
        return max(0.0, readability_score)
    
    def _filter_sections(self, content: str) -> Tuple[List[str], List[str]]:
        """–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Å–µ–∫—Ü–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        sections = content.split('\n\n')
        filtered_sections = []
        valid_sections = []
        
        for section in sections:
            section = section.strip()
            if not section:
                continue
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞—á–µ—Å—Ç–≤–æ —Å–µ–∫—Ü–∏–∏
            section_quality = self._assess_quality(section)
            section_readability = self._assess_readability(section)
            
            if section_quality >= 0.3 and section_readability >= 0.5:
                valid_sections.append(section)
            else:
                filtered_sections.append(section)
        
        return filtered_sections, valid_sections
    
    def prepare_for_llm(self, processed_content: ProcessedContent) -> Dict[str, Any]:
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–µ—Ä–µ–¥–∞—á–∏ –≤ –ª–æ–∫–∞–ª—å–Ω—É—é LLM"""
        return {
            'content': processed_content.cleaned_content,
            'language': processed_content.language,
            'format': processed_content.format_type,
            'quality_score': processed_content.quality_score,
            'readability_score': processed_content.readability_score,
            'valid_sections_count': len(processed_content.valid_sections),
            'metadata': processed_content.metadata
        }
    
    def validate_llm_response(self, response: str) -> Dict[str, Any]:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –æ—Ç–≤–µ—Ç–∞ –æ—Ç LLM"""
        if not response:
            return {
                'valid': False,
                'reason': '–ü—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç',
                'quality_score': 0.0
            }
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–ª–∏–Ω—ã
        if len(response) < 50:
            return {
                'valid': False,
                'reason': '–°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π –æ—Ç–≤–µ—Ç',
                'quality_score': 0.3
            }
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –Ω–∞–ª–∏—á–∏–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
        technical_indicators = ['–ø—Ä–∏–Ω—Ü–∏–ø', '–º–µ—Ç–æ–¥', '—Ñ—É–Ω–∫—Ü–∏—è', '–∞–ª–≥–æ—Ä–∏—Ç–º', '—Å–∏—Å—Ç–µ–º–∞']
        has_technical_info = any(indicator in response.lower() for indicator in technical_indicators)
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
        has_structure = bool(re.search(r'\n|‚Ä¢|\*|\d+\.', response))
        
        # –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
        quality_score = 0.5
        if has_technical_info:
            quality_score += 0.3
        if has_structure:
            quality_score += 0.2
        if len(response) > 200:
            quality_score += 0.1
        
        return {
            'valid': quality_score >= 0.6,
            'reason': '–ö–∞—á–µ—Å—Ç–≤–æ –æ—Ç–≤–µ—Ç–∞' if quality_score >= 0.6 else '–ù–∏–∑–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ',
            'quality_score': min(1.0, quality_score),
            'has_technical_info': has_technical_info,
            'has_structure': has_structure,
            'length': len(response)
        }

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞
data_processor = DataProcessor()

def get_data_processor() -> DataProcessor:
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ —ç–∫–∑–µ–º–ø–ª—è—Ä–∞ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö"""
    return data_processor

if __name__ == "__main__":
    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö
    test_results = [
        {
            'file_name': 'test.txt',
            'content_preview': '–≠—Ç–æ —Ç–µ—Å—Ç–æ–≤—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ä–∞–±–æ—Ç—ã –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö.',
            'category': 'test'
        },
        {
            'file_name': 'test2.txt',
            'content_preview': '–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø—Ä–∏–Ω—Ü–∏–ø–∞—Ö —Ä–∞–±–æ—Ç—ã —Å–∏—Å—Ç–µ–º—ã.',
            'category': 'test'
        }
    ]
    
    processor = DataProcessor()
    processed = processor.process_search_results(test_results)
    
    print(f"–Ø–∑—ã–∫: {processed.language}")
    print(f"–§–æ—Ä–º–∞—Ç: {processed.format_type}")
    print(f"–ö–∞—á–µ—Å—Ç–≤–æ: {processed.quality_score:.2f}")
    print(f"–ß–∏—Ç–∞–µ–º–æ—Å—Ç—å: {processed.readability_score:.2f}")
    print(f"–û—á–∏—â–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç:\n{processed.cleaned_content}")
