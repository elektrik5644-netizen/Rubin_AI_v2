#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Smart Rubin AI - –û—Ö–æ—Ç–Ω–∏–∫ –∑–∞ –∑–Ω–∞–Ω–∏—è–º–∏ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ
–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∏—â–µ—Ç –∏ —Å–∫–∞—á–∏–≤–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –ø–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É –¥–ª—è –ø–æ–ø–æ–ª–Ω–µ–Ω–∏—è –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π
"""

import requests
import sqlite3
import json
import os
import datetime
import time
import re
from urllib.parse import quote_plus, urljoin, urlparse
from bs4 import BeautifulSoup
from typing import Dict, List, Any, Optional, Tuple
import hashlib

class RubinInternetKnowledgeHunter:
    """–ö–ª–∞—Å—Å –¥–ª—è –ø–æ–∏—Å–∫–∞ –∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è –∑–Ω–∞–Ω–∏–π –∏–∑ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞"""
    
    def __init__(self, db_path: str = "rubin_knowledge_base.db"):
        self.db_path = db_path
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
        })
        
        # –ü–æ–∏—Å–∫–æ–≤—ã–µ —Å–∏—Å—Ç–µ–º—ã
        self.search_engines = {
            'google': 'https://www.google.com/search?q={}',
            'bing': 'https://www.bing.com/search?q={}',
            'duckduckgo': 'https://duckduckgo.com/?q={}',
            'yandex': 'https://yandex.ru/search/?text={}'
        }
        
        # –ö—ç—à –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
        self.cache = {}
        self.cache_file = "search_cache.json"
        self.load_cache()
    
    def load_cache(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∫—ç—à –ø–æ–∏—Å–∫–∞"""
        try:
            if os.path.exists(self.cache_file):
                with open(self.cache_file, 'r', encoding='utf-8') as f:
                    self.cache = json.load(f)
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫—ç—à–∞: {e}")
            self.cache = {}
    
    def save_cache(self):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∫—ç—à –ø–æ–∏—Å–∫–∞"""
        try:
            with open(self.cache_file, 'w', encoding='utf-8') as f:
                json.dump(self.cache, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∫—ç—à–∞: {e}")
    
    def search_internet(self, query: str, max_results: int = 5) -> List[Dict[str, Any]]:
        """–ò—â–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ"""
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
        cache_key = hashlib.md5(query.encode()).hexdigest()
        if cache_key in self.cache:
            print(f"üìã –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è: {query}")
            return self.cache[cache_key]
        
        print(f"üîç –ü–æ–∏—Å–∫ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ: {query}")
        results = []
        
        # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –ø–æ–∏—Å–∫–æ–≤–∏–∫–∏
        for engine_name, search_url in self.search_engines.items():
            try:
                print(f"üîÑ –ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ {engine_name}")
                encoded_query = quote_plus(query)
                url = search_url.format(encoded_query)
                
                response = self.session.get(url, timeout=10)
                if response.status_code == 200:
                    search_results = self.parse_search_results(response.text, engine_name)
                    results.extend(search_results)
                    
                    if len(results) >= max_results:
                        break
                        
                time.sleep(1)  # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ —á–µ—Ä–µ–∑ {engine_name}: {e}")
                continue
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        results = results[:max_results]
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à
        self.cache[cache_key] = results
        self.save_cache()
        
        return results
    
    def parse_search_results(self, html: str, engine: str) -> List[Dict[str, Any]]:
        """–ü–∞—Ä—Å–∏—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞"""
        results = []
        try:
            soup = BeautifulSoup(html, 'html.parser')
            
            if engine == 'google':
                results = self.parse_google_results(soup)
            elif engine == 'bing':
                results = self.parse_bing_results(soup)
            elif engine == 'duckduckgo':
                results = self.parse_duckduckgo_results(soup)
            elif engine == 'yandex':
                results = self.parse_yandex_results(soup)
                
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ {engine}: {e}")
        
        return results
    
    def parse_google_results(self, soup: BeautifulSoup) -> List[Dict[str, Any]]:
        """–ü–∞—Ä—Å–∏—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã Google"""
        results = []
        try:
            # –ò—â–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞
            search_results = soup.find_all('div', class_='g')
            
            for result in search_results:
                try:
                    # –ó–∞–≥–æ–ª–æ–≤–æ–∫
                    title_elem = result.find('h3')
                    title = title_elem.get_text() if title_elem else "–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞"
                    
                    # –°—Å—ã–ª–∫–∞
                    link_elem = result.find('a')
                    url = link_elem.get('href') if link_elem else ""
                    
                    # –û–ø–∏—Å–∞–Ω–∏–µ
                    desc_elem = result.find('span', class_='aCOpRe')
                    if not desc_elem:
                        desc_elem = result.find('div', class_='VwiC3b')
                    snippet = desc_elem.get_text() if desc_elem else ""
                    
                    if url and title:
                        results.append({
                            'title': title,
                            'url': url,
                            'snippet': snippet,
                            'source': 'google',
                            'timestamp': datetime.datetime.now().isoformat()
                        })
                        
                except Exception as e:
                    continue
                    
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ Google: {e}")
        
        return results
    
    def parse_bing_results(self, soup: BeautifulSoup) -> List[Dict[str, Any]]:
        """–ü–∞—Ä—Å–∏—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã Bing"""
        results = []
        try:
            search_results = soup.find_all('li', class_='b_algo')
            
            for result in search_results:
                try:
                    title_elem = result.find('h2')
                    title = title_elem.get_text() if title_elem else "–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞"
                    
                    link_elem = result.find('a')
                    url = link_elem.get('href') if link_elem else ""
                    
                    desc_elem = result.find('p')
                    snippet = desc_elem.get_text() if desc_elem else ""
                    
                    if url and title:
                        results.append({
                            'title': title,
                            'url': url,
                            'snippet': snippet,
                            'source': 'bing',
                            'timestamp': datetime.datetime.now().isoformat()
                        })
                        
                except Exception as e:
                    continue
                    
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ Bing: {e}")
        
        return results
    
    def parse_duckduckgo_results(self, soup: BeautifulSoup) -> List[Dict[str, Any]]:
        """–ü–∞—Ä—Å–∏—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã DuckDuckGo"""
        results = []
        try:
            search_results = soup.find_all('div', class_='result')
            
            for result in search_results:
                try:
                    title_elem = result.find('a', class_='result__a')
                    title = title_elem.get_text() if title_elem else "–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞"
                    url = title_elem.get('href') if title_elem else ""
                    
                    desc_elem = result.find('a', class_='result__snippet')
                    snippet = desc_elem.get_text() if desc_elem else ""
                    
                    if url and title:
                        results.append({
                            'title': title,
                            'url': url,
                            'snippet': snippet,
                            'source': 'duckduckgo',
                            'timestamp': datetime.datetime.now().isoformat()
                        })
                        
                except Exception as e:
                    continue
                    
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ DuckDuckGo: {e}")
        
        return results
    
    def parse_yandex_results(self, soup: BeautifulSoup) -> List[Dict[str, Any]]:
        """–ü–∞—Ä—Å–∏—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã Yandex"""
        results = []
        try:
            search_results = soup.find_all('div', class_='serp-item')
            
            for result in search_results:
                try:
                    title_elem = result.find('h2')
                    if not title_elem:
                        title_elem = result.find('a', class_='link')
                    title = title_elem.get_text() if title_elem else "–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞"
                    
                    link_elem = result.find('a', class_='link')
                    url = link_elem.get('href') if link_elem else ""
                    
                    desc_elem = result.find('div', class_='text-container')
                    snippet = desc_elem.get_text() if desc_elem else ""
                    
                    if url and title:
                        results.append({
                            'title': title,
                            'url': url,
                            'snippet': snippet,
                            'source': 'yandex',
                            'timestamp': datetime.datetime.now().isoformat()
                        })
                        
                except Exception as e:
                    continue
                    
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ Yandex: {e}")
        
        return results
    
    def download_page_content(self, url: str) -> Optional[str]:
        """–°–∫–∞—á–∏–≤–∞–µ—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        try:
            print(f"üì• –°–∫–∞—á–∏–≤–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É: {url}")
            
            response = self.session.get(url, timeout=15)
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # –£–¥–∞–ª—è–µ–º —Å–∫—Ä–∏–ø—Ç—ã –∏ —Å—Ç–∏–ª–∏
                for script in soup(["script", "style"]):
                    script.decompose()
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç
                text = soup.get_text()
                
                # –û—á–∏—â–∞–µ–º —Ç–µ–∫—Å—Ç
                lines = (line.strip() for line in text.splitlines())
                chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
                text = ' '.join(chunk for chunk in chunks if chunk)
                
                return text[:5000]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä
                
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è {url}: {e}")
        
        return None
    
    def extract_knowledge_from_content(self, content: str, url: str, title: str) -> Optional[Dict[str, Any]]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∑–Ω–∞–Ω–∏—è –∏–∑ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        try:
            if not content or len(content) < 100:
                return None
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é –ø–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É
            category = self.detect_category(content, title)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
            keywords = self.extract_keywords(content, title)
            
            # –°–æ–∑–¥–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∑–Ω–∞–Ω–∏–µ
            knowledge = {
                'title': title,
                'content': content,
                'category': category,
                'tags': ', '.join(keywords[:5]),  # –ü–µ—Ä–≤—ã–µ 5 –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
                'keywords': ', '.join(keywords),
                'source_url': url,
                'source_type': 'internet',
                'created_at': datetime.datetime.now().isoformat(),
                'usage_count': 0,
                'relevance_score': 0.8
            }
            
            return knowledge
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∑–Ω–∞–Ω–∏–π: {e}")
            return None
    
    def detect_category(self, content: str, title: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏—é –ø–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É"""
        content_lower = content.lower()
        title_lower = title.lower()
        
        # –ú–∞—Ç–µ–º–∞—Ç–∏–∫–∞
        if any(word in content_lower for word in ['–º–∞—Ç–µ–º–∞—Ç–∏–∫–∞', '—Ñ–æ—Ä–º—É–ª–∞', '—É—Ä–∞–≤–Ω–µ–Ω–∏–µ', '–∏–Ω—Ç–µ–≥—Ä–∞–ª', '–ø—Ä–æ–∏–∑–≤–æ–¥–Ω–∞—è', '–∞–ª–≥–µ–±—Ä–∞', '–≥–µ–æ–º–µ—Ç—Ä–∏—è']):
            return 'mathematics'
        
        # –§–∏–∑–∏–∫–∞
        if any(word in content_lower for word in ['—Ñ–∏–∑–∏–∫–∞', '–º–µ—Ö–∞–Ω–∏–∫–∞', '—ç–ª–µ–∫—Ç—Ä–∏—á–µ—Å—Ç–≤–æ', '–º–∞–≥–Ω–µ—Ç–∏–∑–º', '–æ–ø—Ç–∏–∫–∞', '—Ç–µ—Ä–º–æ–¥–∏–Ω–∞–º–∏–∫–∞']):
            return 'physics'
        
        # –ü—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ
        if any(word in content_lower for word in ['–ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ', '–∫–æ–¥', '–∞–ª–≥–æ—Ä–∏—Ç–º', 'python', 'c++', 'java', 'javascript']):
            return 'programming'
        
        # –ê–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è
        if any(word in content_lower for word in ['–∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è', 'plc', 'pmac', '–∫–æ–Ω—Ç—Ä–æ–ª–ª–µ—Ä', '–¥–∞—Ç—á–∏–∫', '–ø—Ä–∏–≤–æ–¥']):
            return 'automation'
        
        # –≠–ª–µ–∫—Ç—Ä–æ–Ω–∏–∫–∞
        if any(word in content_lower for word in ['—ç–ª–µ–∫—Ç—Ä–æ–Ω–∏–∫–∞', '—Å—Ö–µ–º–∞', '—Ç—Ä–∞–Ω–∑–∏—Å—Ç–æ—Ä', '—Ä–µ–∑–∏—Å—Ç–æ—Ä', '–∫–æ–Ω–¥–µ–Ω—Å–∞—Ç–æ—Ä']):
            return 'electronics'
        
        return 'general'
    
    def extract_keywords(self, content: str, title: str) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∏ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ
        text = f"{title} {content}".lower()
        
        # –£–¥–∞–ª—è–µ–º –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è
        text = re.sub(r'[^\w\s]', ' ', text)
        
        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Å–ª–æ–≤–∞
        words = text.split()
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–ª–æ–≤–∞ –∏ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
        stop_words = {'–∏', '–≤', '–Ω–∞', '—Å', '–ø–æ', '–¥–ª—è', '–æ—Ç', '–¥–æ', '–∏–∑', '–∫', '–æ', '—É', '–∑–∞', '–ø–æ–¥', '–Ω–∞–¥', '–ø—Ä–∏', '—á–µ—Ä–µ–∑', '–±–µ–∑', '–º–µ–∂–¥—É', '—Å—Ä–µ–¥–∏', '–æ–∫–æ–ª–æ', '–≤–æ–∫—Ä—É–≥', '–≤–Ω—É—Ç—Ä–∏', '–≤–Ω–µ', '–ø–µ—Ä–µ–¥', '–ø–æ—Å–ª–µ', '–≤–æ', '—Å–æ', '–æ–±', '–ø—Ä–æ', '—á—Ç–æ', '–∫–∞–∫', '–≥–¥–µ', '–∫–æ–≥–¥–∞', '–ø–æ—á–µ–º—É', '–∑–∞—á–µ–º', '–∫–æ—Ç–æ—Ä—ã–π', '–∫–æ—Ç–æ—Ä–∞—è', '–∫–æ—Ç–æ—Ä–æ–µ', '–∫–æ—Ç–æ—Ä—ã–µ', '—ç—Ç–æ', '—ç—Ç–æ—Ç', '—ç—Ç–∞', '—ç—Ç–æ', '—ç—Ç–∏', '—Ç–æ—Ç', '—Ç–∞', '—Ç–æ', '—Ç–µ', '–æ–Ω', '–æ–Ω–∞', '–æ–Ω–æ', '–æ–Ω–∏', '–º—ã', '–≤—ã', '—è', '—Ç—ã', '–æ–Ω', '–æ–Ω–∞', '–æ–Ω–æ', '–æ–Ω–∏', '–º–æ–π', '–º–æ—è', '–º–æ–µ', '–º–æ–∏', '—Ç–≤–æ–π', '—Ç–≤–æ—è', '—Ç–≤–æ–µ', '—Ç–≤–æ–∏', '–µ–≥–æ', '–µ–µ', '–∏—Ö', '–Ω–∞—à', '–Ω–∞—à–∞', '–Ω–∞—à–µ', '–Ω–∞—à–∏', '–≤–∞—à', '–≤–∞—à–∞', '–≤–∞—à–µ', '–≤–∞—à–∏'}
        
        keywords = [word for word in words if len(word) > 3 and word not in stop_words]
        
        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —á–∞—Å—Ç–æ—Ç—É
        word_count = {}
        for word in keywords:
            word_count[word] = word_count.get(word, 0) + 1
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —á–∞—Å—Ç–æ—Ç–µ
        sorted_words = sorted(word_count.items(), key=lambda x: x[1], reverse=True)
        
        return [word for word, count in sorted_words[:10]]  # –¢–æ–ø 10 –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
    
    def hunt_knowledge(self, topic: str, max_pages: int = 3) -> List[Dict[str, Any]]:
        """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ—Ö–æ—Ç—ã –∑–∞ –∑–Ω–∞–Ω–∏—è–º–∏"""
        print(f"üéØ –û—Ö–æ—Ç–∞ –∑–∞ –∑–Ω–∞–Ω–∏—è–º–∏ –ø–æ —Ç–µ–º–µ: {topic}")
        
        # –ò—â–µ–º –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ
        search_results = self.search_internet(topic, max_results=max_pages * 2)
        
        if not search_results:
            print("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞")
            return []
        
        print(f"üìã –ù–∞–π–¥–µ–Ω–æ {len(search_results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞")
        
        # –°–∫–∞—á–∏–≤–∞–µ–º –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        knowledge_items = []
        
        for i, result in enumerate(search_results[:max_pages]):
            try:
                print(f"üì• –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º {i+1}/{min(max_pages, len(search_results))}: {result['title']}")
                
                # –°–∫–∞—á–∏–≤–∞–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ
                content = self.download_page_content(result['url'])
                
                if content:
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–Ω–∞–Ω–∏—è
                    knowledge = self.extract_knowledge_from_content(
                        content, result['url'], result['title']
                    )
                    
                    if knowledge:
                        knowledge_items.append(knowledge)
                        print(f"‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω—ã –∑–Ω–∞–Ω–∏—è: {knowledge['title']}")
                    else:
                        print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –∑–Ω–∞–Ω–∏—è –∏–∑: {result['title']}")
                
                time.sleep(2)  # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {result['title']}: {e}")
                continue
        
        print(f"üéâ –ò–∑–≤–ª–µ—á–µ–Ω–æ {len(knowledge_items)} –∑–Ω–∞–Ω–∏–π")
        return knowledge_items
    
    def save_knowledge_to_database(self, knowledge_items: List[Dict[str, Any]]) -> int:
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∑–Ω–∞–Ω–∏—è –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö"""
        if not knowledge_items:
            return 0
        
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            saved_count = 0
            
            for knowledge in knowledge_items:
                try:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ —É–∂–µ —Ç–∞–∫–∞—è –∑–∞–ø–∏—Å—å
                    cursor.execute("""
                        SELECT COUNT(*) FROM knowledge_base 
                        WHERE title = ? AND source_url = ?
                    """, (knowledge['title'], knowledge['source_url']))
                    
                    if cursor.fetchone()[0] == 0:
                        # –í—Å—Ç–∞–≤–ª—è–µ–º –Ω–æ–≤—É—é –∑–∞–ø–∏—Å—å
                        cursor.execute("""
                            INSERT INTO knowledge_base 
                            (title, content, category, tags, keywords, created_at, usage_count, relevance_score)
                            VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                        """, (
                            knowledge['title'],
                            knowledge['content'],
                            knowledge['category'],
                            knowledge['tags'],
                            knowledge['keywords'],
                            knowledge['created_at'],
                            knowledge['usage_count'],
                            knowledge['relevance_score']
                        ))
                        saved_count += 1
                        print(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {knowledge['title']}")
                    else:
                        print(f"‚ö†Ô∏è –£–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {knowledge['title']}")
                        
                except Exception as e:
                    print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è {knowledge['title']}: {e}")
                    continue
            
            conn.commit()
            conn.close()
            
            return saved_count
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Ä–∞–±–æ—Ç—ã —Å –±–∞–∑–æ–π –¥–∞–Ω–Ω—ã—Ö: {e}")
            return 0
    
    def auto_hunt_knowledge(self, topics: List[str]) -> Dict[str, int]:
        """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ—Ö–æ—Ç–∏—Ç—Å—è –∑–∞ –∑–Ω–∞–Ω–∏—è–º–∏ –ø–æ —Å–ø–∏—Å–∫—É —Ç–µ–º"""
        print("üöÄ –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–ê–Ø –û–•–û–¢–ê –ó–ê –ó–ù–ê–ù–ò–Ø–ú–ò")
        print("=" * 50)
        
        results = {}
        
        for topic in topics:
            print(f"\nüéØ –¢–µ–º–∞: {topic}")
            print("-" * 30)
            
            # –û—Ö–æ—Ç–∏–º—Å—è –∑–∞ –∑–Ω–∞–Ω–∏—è–º–∏
            knowledge_items = self.hunt_knowledge(topic, max_pages=2)
            
            if knowledge_items:
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –±–∞–∑—É
                saved_count = self.save_knowledge_to_database(knowledge_items)
                results[topic] = saved_count
                print(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {saved_count} –∑–Ω–∞–Ω–∏–π –ø–æ —Ç–µ–º–µ '{topic}'")
            else:
                results[topic] = 0
                print(f"‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ –∑–Ω–∞–Ω–∏–π –ø–æ —Ç–µ–º–µ '{topic}'")
            
            time.sleep(3)  # –ü–∞—É–∑–∞ –º–µ–∂–¥—É —Ç–µ–º–∞–º–∏
        
        return results

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print("üåê SMART RUBIN AI - –û–•–û–¢–ù–ò–ö –ó–ê –ó–ù–ê–ù–ò–Ø–ú–ò –í –ò–ù–¢–ï–†–ù–ï–¢–ï")
    print("=" * 60)
    
    # –°–æ–∑–¥–∞–µ–º –æ—Ö–æ—Ç–Ω–∏–∫–∞ –∑–∞ –∑–Ω–∞–Ω–∏—è–º–∏
    hunter = RubinInternetKnowledgeHunter()
    
    # –°–ø–∏—Å–æ–∫ —Ç–µ–º –¥–ª—è –ø–æ–∏—Å–∫–∞
    topics = [
        "–ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ Python –æ—Å–Ω–æ–≤—ã",
        "–º–∞—Ç–µ–º–∞—Ç–∏–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–Ω—ã–µ –∏–Ω—Ç–µ–≥—Ä–∞–ª—ã",
        "—Ñ–∏–∑–∏–∫–∞ –∑–∞–∫–æ–Ω—ã –ù—å—é—Ç–æ–Ω–∞ –º–µ—Ö–∞–Ω–∏–∫–∞",
        "–∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è PLC –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ",
        "—ç–ª–µ–∫—Ç—Ä–æ–Ω–∏–∫–∞ —Å—Ö–µ–º—ã —Ç—Ä–∞–Ω–∑–∏—Å—Ç–æ—Ä—ã"
    ]
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫—É—é –æ—Ö–æ—Ç—É
    results = hunter.auto_hunt_knowledge(topics)
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    print("\nüéØ –ò–¢–û–ì–û–í–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´:")
    print("=" * 30)
    
    total_saved = 0
    for topic, count in results.items():
        print(f"üìö {topic}: {count} –∑–Ω–∞–Ω–∏–π")
        total_saved += count
    
    print(f"\nüéâ –í–°–ï–ì–û –°–û–•–†–ê–ù–ï–ù–û: {total_saved} –∑–Ω–∞–Ω–∏–π")
    
    if total_saved > 0:
        print("\n‚úÖ –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π Smart Rubin AI —É—Å–ø–µ—à–Ω–æ –ø–æ–ø–æ–ª–Ω–µ–Ω–∞!")
        print("üß† –¢–µ–ø–µ—Ä—å Rubin –º–æ–∂–µ—Ç –¥–∞–≤–∞—Ç—å –±–æ–ª–µ–µ —Ç–æ—á–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã –ø–æ –Ω–∞–π–¥–µ–Ω–Ω—ã–º —Ç–µ–º–∞–º!")
    else:
        print("\n‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –∏ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –Ω–æ–≤—ã–µ –∑–Ω–∞–Ω–∏—è")
        print("üí° –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –∏–∑–º–µ–Ω–∏—Ç—å –ø–æ–∏—Å–∫–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã –∏–ª–∏ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –∏–Ω—Ç–µ—Ä–Ω–µ—Ç-—Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ")

if __name__ == "__main__":
    main()
