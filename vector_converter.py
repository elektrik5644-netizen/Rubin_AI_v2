#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–ö–æ–Ω–≤–µ—Ä—Ç–µ—Ä –≤–µ–∫—Ç–æ—Ä–æ–≤ –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö Rubin AI v2.0
–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ –≤–µ–∫—Ç–æ—Ä–æ–≤
"""

import os
import sys
import sqlite3
import numpy as np
import logging
import json
from typing import List, Tuple, Dict, Optional, Union
from pathlib import Path
import hashlib
from datetime import datetime
import argparse

try:
    from sentence_transformers import SentenceTransformer
    SENTENCE_TRANSFORMERS_AVAILABLE = True
except ImportError:
    SENTENCE_TRANSFORMERS_AVAILABLE = False
    print("‚ö†Ô∏è sentence-transformers –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install sentence-transformers")

class VectorConverter:
    """–ö–æ–Ω–≤–µ—Ä—Ç–µ—Ä –≤–µ–∫—Ç–æ—Ä–æ–≤ –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤ –∏ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–µ–π"""
    
    def __init__(self, db_path="rubin_ai_documents.db", backup_path=None):
        self.db_path = db_path
        self.backup_path = backup_path or f"{db_path}.backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        self.setup_logging()
        
    def setup_logging(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('vector_converter.log', encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
        
    def backup_database(self) -> bool:
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ä–µ–∑–µ—Ä–≤–Ω–æ–π –∫–æ–ø–∏–∏ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö"""
        try:
            if not os.path.exists(self.db_path):
                self.logger.error(f"‚ùå –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö {self.db_path} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
                return False
                
            import shutil
            shutil.copy2(self.db_path, self.backup_path)
            self.logger.info(f"‚úÖ –†–µ–∑–µ—Ä–≤–Ω–∞—è –∫–æ–ø–∏—è —Å–æ–∑–¥–∞–Ω–∞: {self.backup_path}")
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —Ä–µ–∑–µ—Ä–≤–Ω–æ–π –∫–æ–ø–∏–∏: {e}")
            return False
            
    def analyze_vectors(self) -> Dict:
        """–ê–Ω–∞–ª–∏–∑ —Ç–µ–∫—É—â–∏—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤ –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è —Ç–∞–±–ª–∏—Ü
            cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='document_vectors'")
            if not cursor.fetchone():
                self.logger.warning("‚ö†Ô∏è –¢–∞–±–ª–∏—Ü–∞ document_vectors –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
                return {}
                
            # –ü–æ–¥—Å—á–µ—Ç –≤–µ–∫—Ç–æ—Ä–æ–≤
            cursor.execute("SELECT COUNT(*) FROM document_vectors")
            vector_count = cursor.fetchone()[0]
            
            # –ê–Ω–∞–ª–∏–∑ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –≤–µ–∫—Ç–æ—Ä–æ–≤
            cursor.execute("SELECT vector_data FROM document_vectors LIMIT 1")
            sample_vector = cursor.fetchone()
            
            if sample_vector:
                vector_data = np.frombuffer(sample_vector[0], dtype=np.float32)
                dimension = len(vector_data)
            else:
                dimension = 0
                
            # –ê–Ω–∞–ª–∏–∑ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
            cursor.execute("SELECT COUNT(*) FROM vector_metadata")
            metadata_count = cursor.fetchone()[0]
            
            # –ê–Ω–∞–ª–∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            cursor.execute("SELECT COUNT(*) FROM documents")
            document_count = cursor.fetchone()[0]
            
            conn.close()
            
            analysis = {
                'vector_count': vector_count,
                'dimension': dimension,
                'metadata_count': metadata_count,
                'document_count': document_count,
                'vector_size_bytes': dimension * 4 if dimension > 0 else 0,  # float32 = 4 bytes
                'total_vector_size_mb': (vector_count * dimension * 4) / (1024 * 1024) if dimension > 0 else 0
            }
            
            self.logger.info("üìä –ê–Ω–∞–ª–∏–∑ –≤–µ–∫—Ç–æ—Ä–æ–≤:")
            for key, value in analysis.items():
                self.logger.info(f"  {key}: {value}")
                
            return analysis
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –≤–µ–∫—Ç–æ—Ä–æ–≤: {e}")
            return {}
            
    def convert_vectors_to_csv(self, output_file: str = "vectors_export.csv") -> bool:
        """–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤–µ–∫—Ç–æ—Ä–æ–≤ –≤ CSV —Ñ–æ—Ä–º–∞—Ç"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute("""
                SELECT dv.id, dv.document_id, dv.vector_data, dv.vector_hash, dv.content_hash,
                       d.file_name, d.content, vm.content_preview, vm.keywords, vm.category
                FROM document_vectors dv
                LEFT JOIN documents d ON dv.document_id = d.id
                LEFT JOIN vector_metadata vm ON dv.document_id = vm.document_id
            """)
            
            vectors = cursor.fetchall()
            
            if not vectors:
                self.logger.warning("‚ö†Ô∏è –í–µ–∫—Ç–æ—Ä—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
                return False
                
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ CSV
            import csv
            
            with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:
                writer = csv.writer(csvfile)
                
                # –ó–∞–≥–æ–ª–æ–≤–∫–∏
                sample_vector = np.frombuffer(vectors[0][2], dtype=np.float32)
                headers = ['id', 'document_id', 'vector_hash', 'content_hash', 'file_name', 'content_preview', 'keywords', 'category']
                headers.extend([f'vector_{i}' for i in range(len(sample_vector))])
                writer.writerow(headers)
                
                # –î–∞–Ω–Ω—ã–µ
                for row in vectors:
                    vector_data = np.frombuffer(row[2], dtype=np.float32)
                    csv_row = [
                        row[0],  # id
                        row[1],  # document_id
                        row[3],  # vector_hash
                        row[4],  # content_hash
                        row[5] or '',  # title
                        row[7] or '',  # content_preview
                        row[8] or '',  # keywords
                        row[9] or ''   # category
                    ]
                    csv_row.extend(vector_data.tolist())
                    writer.writerow(csv_row)
                    
            conn.close()
            self.logger.info(f"‚úÖ –í–µ–∫—Ç–æ—Ä—ã —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã –≤ {output_file}")
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ —ç–∫—Å–ø–æ—Ä—Ç–∞ –≤ CSV: {e}")
            return False
            
    def convert_vectors_to_json(self, output_file: str = "vectors_export.json") -> bool:
        """–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤–µ–∫—Ç–æ—Ä–æ–≤ –≤ JSON —Ñ–æ—Ä–º–∞—Ç"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute("""
                SELECT dv.id, dv.document_id, dv.vector_data, dv.vector_hash, dv.content_hash,
                       d.file_name, d.content, vm.content_preview, vm.keywords, vm.category
                FROM document_vectors dv
                LEFT JOIN documents d ON dv.document_id = d.id
                LEFT JOIN vector_metadata vm ON dv.document_id = vm.document_id
            """)
            
            vectors = cursor.fetchall()
            
            if not vectors:
                self.logger.warning("‚ö†Ô∏è –í–µ–∫—Ç–æ—Ä—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
                return False
                
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ JSON
            json_data = []
            
            for row in vectors:
                vector_data = np.frombuffer(row[2], dtype=np.float32)
                
                vector_info = {
                    'id': row[0],
                    'document_id': row[1],
                    'vector_hash': row[3],
                    'content_hash': row[4],
                    'file_name': row[5] or '',
                    'content_preview': row[7] or '',
                    'keywords': row[8] or '',
                    'category': row[9] or '',
                    'vector': vector_data.tolist(),
                    'dimension': len(vector_data)
                }
                json_data.append(vector_info)
                
            with open(output_file, 'w', encoding='utf-8') as jsonfile:
                json.dump(json_data, jsonfile, ensure_ascii=False, indent=2)
                
            conn.close()
            self.logger.info(f"‚úÖ –í–µ–∫—Ç–æ—Ä—ã —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã –≤ {output_file}")
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ —ç–∫—Å–ø–æ—Ä—Ç–∞ –≤ JSON: {e}")
            return False
            
    def convert_vectors_to_numpy(self, output_file: str = "vectors_export.npz") -> bool:
        """–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤–µ–∫—Ç–æ—Ä–æ–≤ –≤ NumPy —Ñ–æ—Ä–º–∞—Ç"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute("""
                SELECT dv.id, dv.document_id, dv.vector_data, dv.vector_hash, dv.content_hash,
                       d.file_name, d.content, vm.content_preview, vm.keywords, vm.category
                FROM document_vectors dv
                LEFT JOIN documents d ON dv.document_id = d.id
                LEFT JOIN vector_metadata vm ON dv.document_id = vm.document_id
            """)
            
            vectors = cursor.fetchall()
            
            if not vectors:
                self.logger.warning("‚ö†Ô∏è –í–µ–∫—Ç–æ—Ä—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
                return False
                
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ NumPy –º–∞—Å—Å–∏–≤–æ–≤
            vector_list = []
            metadata_list = []
            
            for row in vectors:
                vector_data = np.frombuffer(row[2], dtype=np.float32)
                vector_list.append(vector_data)
                
                metadata = {
                    'id': row[0],
                    'document_id': row[1],
                    'vector_hash': row[3],
                    'content_hash': row[4],
                    'file_name': row[5] or '',
                    'content_preview': row[7] or '',
                    'keywords': row[8] or '',
                    'category': row[9] or ''
                }
                metadata_list.append(metadata)
                
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ NPZ
            vectors_array = np.array(vector_list)
            np.savez_compressed(
                output_file,
                vectors=vectors_array,
                metadata=metadata_list
            )
            
            conn.close()
            self.logger.info(f"‚úÖ –í–µ–∫—Ç–æ—Ä—ã —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã –≤ {output_file}")
            self.logger.info(f"üìä –†–∞–∑–º–µ—Ä –º–∞—Å—Å–∏–≤–∞ –≤–µ–∫—Ç–æ—Ä–æ–≤: {vectors_array.shape}")
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ —ç–∫—Å–ø–æ—Ä—Ç–∞ –≤ NumPy: {e}")
            return False
            
    def convert_vectors_to_text(self, output_file: str = "vectors_export.txt") -> bool:
        """–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤–µ–∫—Ç–æ—Ä–æ–≤ –≤ —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–æ—Ä–º–∞—Ç"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute("""
                SELECT dv.id, dv.document_id, dv.vector_data, dv.vector_hash, dv.content_hash,
                       d.file_name, d.content, vm.content_preview, vm.keywords, vm.category
                FROM document_vectors dv
                LEFT JOIN documents d ON dv.document_id = d.id
                LEFT JOIN vector_metadata vm ON dv.document_id = vm.document_id
            """)
            
            vectors = cursor.fetchall()
            
            if not vectors:
                self.logger.warning("‚ö†Ô∏è –í–µ–∫—Ç–æ—Ä—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
                return False
                
            with open(output_file, 'w', encoding='utf-8') as txtfile:
                txtfile.write("# Rubin AI Vectors Export\n")
                txtfile.write(f"# Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                txtfile.write(f"# Total vectors: {len(vectors)}\n\n")
                
                for row in vectors:
                    vector_data = np.frombuffer(row[2], dtype=np.float32)
                    
                    txtfile.write(f"=== Vector ID: {row[0]} ===\n")
                    txtfile.write(f"Document ID: {row[1]}\n")
                    txtfile.write(f"Vector Hash: {row[3]}\n")
                    txtfile.write(f"Content Hash: {row[4]}\n")
                    txtfile.write(f"File Name: {row[5] or 'N/A'}\n")
                    txtfile.write(f"Content Preview: {row[7] or 'N/A'}\n")
                    txtfile.write(f"Keywords: {row[8] or 'N/A'}\n")
                    txtfile.write(f"Category: {row[9] or 'N/A'}\n")
                    txtfile.write(f"Dimension: {len(vector_data)}\n")
                    txtfile.write(f"Vector: {vector_data.tolist()}\n\n")
                    
            conn.close()
            self.logger.info(f"‚úÖ –í–µ–∫—Ç–æ—Ä—ã —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã –≤ {output_file}")
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ —ç–∫—Å–ø–æ—Ä—Ç–∞ –≤ —Ç–µ–∫—Å—Ç: {e}")
            return False
            
    def convert_dimension(self, new_dimension: int, model_name: str = None) -> bool:
        """–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤–µ–∫—Ç–æ—Ä–æ–≤ –≤ –Ω–æ–≤—É—é —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å"""
        if not SENTENCE_TRANSFORMERS_AVAILABLE:
            self.logger.error("‚ùå sentence-transformers –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω –¥–ª—è –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏")
            return False
            
        try:
            # –ó–∞–≥—Ä—É–∑–∫–∞ –Ω–æ–≤–æ–π –º–æ–¥–µ–ª–∏
            if model_name:
                self.logger.info(f"üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –¥–ª—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ {new_dimension}: {model_name}")
                new_model = SentenceTransformer(model_name)
            else:
                # –í—ã–±–æ—Ä –º–æ–¥–µ–ª–∏ –ø–æ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏
                if new_dimension == 384:
                    model_name = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
                elif new_dimension == 768:
                    model_name = "sentence-transformers/all-MiniLM-L6-v2"
                elif new_dimension == 1536:
                    model_name = "sentence-transformers/all-mpnet-base-v2"
                else:
                    self.logger.error(f"‚ùå –ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {new_dimension}")
                    return False
                    
                self.logger.info(f"üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏: {model_name}")
                new_model = SentenceTransformer(model_name)
                
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # –ü–æ–ª—É—á–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
            cursor.execute("""
                SELECT d.id, d.content
                FROM documents d
                INNER JOIN document_vectors dv ON d.id = dv.document_id
            """)
            
            documents = cursor.fetchall()
            
            if not documents:
                self.logger.warning("‚ö†Ô∏è –î–æ–∫—É–º–µ–Ω—Ç—ã –¥–ª—è –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
                return False
                
            # –ü–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –≤–µ–∫—Ç–æ—Ä–æ–≤
            updated_count = 0
            
            for doc_id, content in documents:
                try:
                    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –Ω–æ–≤–æ–≥–æ –≤–µ–∫—Ç–æ—Ä–∞
                    new_embedding = new_model.encode(content)
                    
                    # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ BLOB
                    new_vector_blob = new_embedding.tobytes()
                    new_vector_hash = hashlib.md5(new_vector_blob).hexdigest()
                    
                    # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö
                    cursor.execute("""
                        UPDATE document_vectors 
                        SET vector_data = ?, vector_hash = ?
                        WHERE document_id = ?
                    """, (new_vector_blob, new_vector_hash, doc_id))
                    
                    updated_count += 1
                    
                    if updated_count % 10 == 0:
                        self.logger.info(f"üîÑ –û–±–Ω–æ–≤–ª–µ–Ω–æ {updated_count} –≤–µ–∫—Ç–æ—Ä–æ–≤...")
                        
                except Exception as e:
                    self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –≤–µ–∫—Ç–æ—Ä–∞ –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–∞ {doc_id}: {e}")
                    
            conn.commit()
            conn.close()
            
            self.logger.info(f"‚úÖ –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –û–±–Ω–æ–≤–ª–µ–Ω–æ {updated_count} –≤–µ–∫—Ç–æ—Ä–æ–≤")
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏: {e}")
            return False
            
    def restore_from_backup(self, backup_path: str = None) -> bool:
        """–í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –∏–∑ —Ä–µ–∑–µ—Ä–≤–Ω–æ–π –∫–æ–ø–∏–∏"""
        try:
            backup_file = backup_path or self.backup_path
            
            if not os.path.exists(backup_file):
                self.logger.error(f"‚ùå –†–µ–∑–µ—Ä–≤–Ω–∞—è –∫–æ–ø–∏—è {backup_file} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
                return False
                
            import shutil
            shutil.copy2(backup_file, self.db_path)
            self.logger.info(f"‚úÖ –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ –∏–∑ {backup_file}")
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è: {e}")
            return False

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    parser = argparse.ArgumentParser(description="–ö–æ–Ω–≤–µ—Ä—Ç–µ—Ä –≤–µ–∫—Ç–æ—Ä–æ–≤ Rubin AI v2.0")
    parser.add_argument("--db", default="rubin_ai_documents.db", help="–ü—É—Ç—å –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö")
    parser.add_argument("--action", choices=[
        "analyze", "csv", "json", "numpy", "text", "dimension", "backup", "restore"
    ], required=True, help="–î–µ–π—Å—Ç–≤–∏–µ –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è")
    parser.add_argument("--output", help="–§–∞–π–ª –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞")
    parser.add_argument("--dimension", type=int, help="–ù–æ–≤–∞—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤–µ–∫—Ç–æ—Ä–æ–≤")
    parser.add_argument("--model", help="–ú–æ–¥–µ–ª—å –¥–ª—è –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏")
    parser.add_argument("--backup", help="–ü—É—Ç—å –∫ —Ä–µ–∑–µ—Ä–≤–Ω–æ–π –∫–æ–ø–∏–∏")
    
    args = parser.parse_args()
    
    converter = VectorConverter(args.db, args.backup)
    
    if args.action == "analyze":
        converter.analyze_vectors()
        
    elif args.action == "backup":
        converter.backup_database()
        
    elif args.action == "restore":
        converter.restore_from_backup(args.backup)
        
    elif args.action == "csv":
        output_file = args.output or "vectors_export.csv"
        converter.convert_vectors_to_csv(output_file)
        
    elif args.action == "json":
        output_file = args.output or "vectors_export.json"
        converter.convert_vectors_to_json(output_file)
        
    elif args.action == "numpy":
        output_file = args.output or "vectors_export.npz"
        converter.convert_vectors_to_numpy(output_file)
        
    elif args.action == "text":
        output_file = args.output or "vectors_export.txt"
        converter.convert_vectors_to_text(output_file)
        
    elif args.action == "dimension":
        if not args.dimension:
            print("‚ùå –¢—Ä–µ–±—É–µ—Ç—Å—è –ø–∞—Ä–∞–º–µ—Ç—Ä --dimension")
            return
        converter.convert_dimension(args.dimension, args.model)

if __name__ == "__main__":
    main()
