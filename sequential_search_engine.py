#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–π –ø–æ–∏—Å–∫–æ–≤–∏–∫ –¥–ª—è Rubin AI v2.0
–¶–µ–ø–æ—á–∫–∞: –í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ ‚Üí –¢–µ–∫—Å—Ç–æ–≤—ã–π –ø–æ–∏—Å–∫ ‚Üí –î–∏—Å–ø–µ—Ç—á–µ—Ä ‚Üí LLM
"""

import logging
import time
from typing import List, Dict, Optional, Tuple
from vector_search import VectorSearchEngine
from document_loader import DocumentLoader

class SequentialSearchEngine:
    """–ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–π –ø–æ–∏—Å–∫–æ–≤–∏–∫ —Å —Ü–µ–ø–æ—á–∫–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
    
    def __init__(self, db_path="rubin_ai_v2.db"):
        self.db_path = db_path
        self.setup_logging()
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
        self.text_search = DocumentLoader(db_path)
        self.vector_search = VectorSearchEngine(db_path)
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–æ–∏—Å–∫–∞
        self.vector_candidates_limit = 20  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –æ—Ç –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
        self.final_results_limit = 5       # –§–∏–Ω–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        self.min_similarity = 0.6          # –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ –¥–ª—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
        
    def setup_logging(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('sequential_search.log', encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
        
    def search(self, query: str, limit: int = 5) -> List[Dict]:
        """
        –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ —Å —Ü–µ–ø–æ—á–∫–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
        
        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            limit: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            
        Returns:
            –°–ø–∏—Å–æ–∫ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
        """
        start_time = time.time()
        
        try:
            self.logger.info(f"üîÑ –ù–∞—á–∏–Ω–∞–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞: '{query}'")
            
            # –≠—Ç–∞–ø 1: –í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ –Ω–∞—Ö–æ–¥–∏—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ñ–∞–π–ª—ã
            vector_candidates = self._stage1_vector_search(query)
            self.logger.info(f"üìÅ –≠—Ç–∞–ø 1: –í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ –Ω–∞—à–µ–ª {len(vector_candidates)} –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤")
            
            if not vector_candidates:
                self.logger.warning("‚ùå –í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ –Ω–µ –Ω–∞—à–µ–ª –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤")
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º fallback –ø–æ–∏—Å–∫
                fallback_results = self._fallback_text_search(query)
                if fallback_results:
                    # –ü—Ä–∏–º–µ–Ω—è–µ–º —ç—Ç–∞–ø—ã 2 –∏ 3 –∫ fallback —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º
                    text_results = self._stage2_text_search(query, fallback_results)
                    final_results = self._stage3_ranking(query, text_results, limit)
                    
                    elapsed = time.time() - start_time
                    self.logger.info(f"‚úÖ –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {elapsed:.3f}—Å")
                    return final_results
                else:
                    return []
            
            # –≠—Ç–∞–ø 2: –¢–µ–∫—Å—Ç–æ–≤—ã–π –ø–æ–∏—Å–∫ –∏–∑–≤–ª–µ–∫–∞–µ—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç
            text_results = self._stage2_text_search(query, vector_candidates)
            self.logger.info(f"üìù –≠—Ç–∞–ø 2: –¢–µ–∫—Å—Ç–æ–≤—ã–π –ø–æ–∏—Å–∫ –æ–±—Ä–∞–±–æ—Ç–∞–ª {len(text_results)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
            
            if not text_results:
                self.logger.warning("‚ùå –¢–µ–∫—Å—Ç–æ–≤—ã–π –ø–æ–∏—Å–∫ –Ω–µ –Ω–∞—à–µ–ª —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞")
                return []
            
            # –≠—Ç–∞–ø 3: –†–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            final_results = self._stage3_ranking(query, text_results, limit)
            self.logger.info(f"üéØ –≠—Ç–∞–ø 3: –§–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–±–æ—Ä {len(final_results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
            
            search_time = time.time() - start_time
            self.logger.info(f"‚úÖ –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {search_time:.3f}—Å")
            
            return final_results
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞: {e}")
            return []
    
    def _stage1_vector_search(self, query: str) -> List[Dict]:
        """
        –≠—Ç–∞–ø 1: –í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ –Ω–∞—Ö–æ–¥–∏—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ñ–∞–π–ª—ã
        
        Returns:
            –°–ø–∏—Å–æ–∫ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏ —Ñ–∞–π–ª–æ–≤
        """
        self.logger.info("üß† –≠—Ç–∞–ø 1: –í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ —Ñ–∞–π–ª–æ–≤...")
        
        try:
            # –ü–æ–ª—É—á–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
            vector_results = self.vector_search.search_similar(
                query, 
                top_k=self.vector_candidates_limit, 
                threshold=self.min_similarity
            )
            
            # –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤
            candidates = []
            for result in vector_results:
                doc_id = result['document_id']
                similarity = result['similarity']
                metadata = result['metadata']
                
                # –ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Ñ–∞–π–ª–µ
                doc_info = self._get_document_info(doc_id)
                
                candidates.append({
                    'document_id': doc_id,
                    'file_name': doc_info.get('file_name', 'Unknown'),
                    'category': doc_info.get('category', metadata.get('category', 'Unknown')),
                    'similarity': similarity,
                    'metadata': doc_info.get('metadata', ''),
                    'stage': 'vector_candidate'
                })
            
            self.logger.info(f"üìÅ –ù–∞–π–¥–µ–Ω–æ {len(candidates)} —Ñ–∞–π–ª–æ–≤-–∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤")
            
            # –ï—Å–ª–∏ –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ –Ω–µ –Ω–∞—à–µ–ª –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback
            if len(candidates) == 0:
                self.logger.warning("‚ùå –í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ –Ω–µ –Ω–∞—à–µ–ª –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback")
                return self._fallback_text_search(query)
            
            return candidates
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞: {e}")
            # Fallback: –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–π –ø–æ–∏—Å–∫ –¥–ª—è –≤—Å–µ—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            return self._fallback_text_search(query)
    
    def _stage2_text_search(self, query: str, candidates: List[Dict]) -> List[Dict]:
        """
        –≠—Ç–∞–ø 2: –¢–µ–∫—Å—Ç–æ–≤—ã–π –ø–æ–∏—Å–∫ –∏–∑–≤–ª–µ–∫–∞–µ—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç –∏–∑ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
        
        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            candidates: –°–ø–∏—Å–æ–∫ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –æ—Ç –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
            
        Returns:
            –°–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–Ω—Ç–æ–º
        """
        self.logger.info("üìù –≠—Ç–∞–ø 2: –¢–µ–∫—Å—Ç–æ–≤—ã–π –ø–æ–∏—Å–∫ –∫–æ–Ω—Ç–µ–Ω—Ç–∞...")
        
        try:
            results = []
            
            for candidate in candidates:
                doc_id = candidate['document_id']
                file_name = candidate['file_name']
                
                # –ü–æ–ª—É—á–µ–Ω–∏–µ –ø–æ–ª–Ω–æ–≥–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ —Ñ–∞–π–ª–∞
                content = self._get_document_content(doc_id)
                
                if not content:
                    self.logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ñ–∞–π–ª–∞ {file_name}")
                    continue
                
                # –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –≤ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º
                relevant_fragments = self._extract_relevant_fragments(query, content)
                
                if relevant_fragments:
                    # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –ø–æ–∏—Å–∫–∞
                    text_relevance = self._calculate_text_relevance(query, relevant_fragments)
                    
                    results.append({
                        'document_id': doc_id,
                        'file_name': file_name,
                        'category': candidate['category'],
                        'content_preview': ' '.join(relevant_fragments[:3]),  # –ü–µ—Ä–≤—ã–µ 3 —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞
                        'full_content': content,
                        'relevant_fragments': relevant_fragments,
                        'vector_similarity': candidate['similarity'],
                        'text_relevance': text_relevance,
                        'combined_score': self._calculate_combined_score(
                            candidate['similarity'], 
                            text_relevance
                        ),
                        'metadata': candidate['metadata'],
                        'stage': 'text_processed'
                    })
                    
                    self.logger.debug(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω —Ñ–∞–π–ª {file_name}: {len(relevant_fragments)} —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤")
                else:
                    self.logger.debug(f"‚ö†Ô∏è –í —Ñ–∞–π–ª–µ {file_name} –Ω–µ –Ω–∞–π–¥–µ–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤")
            
            self.logger.info(f"üìù –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(results)} —Ñ–∞–π–ª–æ–≤ —Å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–º –∫–æ–Ω—Ç–µ–Ω—Ç–æ–º")
            return results
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –ø–æ–∏—Å–∫–∞: {e}")
            return []
    
    def _stage3_ranking(self, query: str, text_results: List[Dict], limit: int) -> List[Dict]:
        """
        –≠—Ç–∞–ø 3: –†–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–±–æ—Ä —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        
        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            text_results: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –ø–æ–∏—Å–∫–∞
            limit: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            
        Returns:
            –§–∏–Ω–∞–ª—å–Ω—ã–π —Å–ø–∏—Å–æ–∫ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        """
        self.logger.info("üéØ –≠—Ç–∞–ø 3: –†–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤...")
        
        try:
            # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–º—É score
            sorted_results = sorted(
                text_results, 
                key=lambda x: x['combined_score'], 
                reverse=True
            )
            
            # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            final_results = sorted_results[:limit]
            
            # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
            for i, result in enumerate(final_results):
                result['rank'] = i + 1
                result['stage'] = 'final'
                result['search_type'] = 'sequential'
                
                # –£–¥–∞–ª–µ–Ω–∏–µ –ø–æ–ª–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
                if 'full_content' in result:
                    del result['full_content']
            
            self.logger.info(f"üéØ –û—Ç–æ–±—Ä–∞–Ω–æ {len(final_results)} —Ñ–∏–Ω–∞–ª—å–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
            return final_results
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è: {e}")
            return []
    
    def _extract_relevant_fragments(self, query: str, content: str) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –∏–∑ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        try:
            import re
            
            # –†–∞–∑–±–∏–≤–∞–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
            sentences = re.split(r'[.!?]+', content)
            
            # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ –∑–∞–ø—Ä–æ—Å–∞
            query_words = [word.lower() for word in query.split() if len(word) > 2]
            
            relevant_sentences = []
            for sentence in sentences:
                sentence = sentence.strip()
                if len(sentence) < 10:  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
                    continue
                
                sentence_lower = sentence.lower()
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
                if any(word in sentence_lower for word in query_words):
                    relevant_sentences.append(sentence)
            
            # –ï—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π, –±–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ
            if not relevant_sentences:
                relevant_sentences = [s.strip() for s in sentences[:5] if len(s.strip()) > 10]
            
            return relevant_sentences[:10]  # –ú–∞–∫—Å–∏–º—É–º 10 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤: {e}")
            return []
    
    def _calculate_text_relevance(self, query: str, fragments: List[str]) -> float:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –ø–æ–∏—Å–∫–∞"""
        try:
            if not fragments:
                return 0.0
            
            query_words = set(query.lower().split())
            total_score = 0.0
            
            for fragment in fragments:
                fragment_words = set(fragment.lower().split())
                common_words = query_words.intersection(fragment_words)
                
                if query_words:
                    fragment_score = len(common_words) / len(query_words)
                    total_score += fragment_score
            
            return min(total_score / len(fragments), 1.0)
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏: {e}")
            return 0.0
    
    def _calculate_combined_score(self, vector_similarity: float, text_relevance: float) -> float:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ score"""
        # –í–µ—Å–∞: 60% –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫, 40% —Ç–µ–∫—Å—Ç–æ–≤—ã–π –ø–æ–∏—Å–∫
        vector_weight = 0.6
        text_weight = 0.4
        
        return (vector_weight * vector_similarity) + (text_weight * text_relevance)
    
    def _get_document_content(self, doc_id: int) -> Optional[str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        try:
            import sqlite3
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute('SELECT content FROM documents WHERE id = ?', (doc_id,))
            result = cursor.fetchone()
            conn.close()
            
            return result[0] if result else None
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ {doc_id}: {e}")
            return None
    
    def _get_document_info(self, doc_id: int) -> Dict:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –¥–æ–∫—É–º–µ–Ω—Ç–µ"""
        try:
            import sqlite3
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute('''
                SELECT file_name, category, metadata 
                FROM documents 
                WHERE id = ?
            ''', (doc_id,))
            
            result = cursor.fetchone()
            conn.close()
            
            if result:
                return {
                    'file_name': result[0],
                    'category': result[1],
                    'metadata': result[2] or ''
                }
            else:
                return {
                    'file_name': 'Unknown',
                    'category': 'Unknown',
                    'metadata': ''
                }
                
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –¥–æ–∫—É–º–µ–Ω—Ç–µ {doc_id}: {e}")
            return {
                'file_name': 'Unknown',
                'category': 'Unknown',
                'metadata': ''
            }
    
    def _fallback_text_search(self, query: str) -> List[Dict]:
        """
        Fallback —Ç–µ–∫—Å—Ç–æ–≤—ã–π –ø–æ–∏—Å–∫ –∫–æ–≥–¥–∞ –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω
        
        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            
        Returns:
            –°–ø–∏—Å–æ–∫ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –ø–æ–∏—Å–∫–∞
        """
        self.logger.info("üîÑ Fallback: –¢–µ–∫—Å—Ç–æ–≤—ã–π –ø–æ–∏—Å–∫ –ø–æ –≤—Å–µ–º –¥–æ–∫—É–º–µ–Ω—Ç–∞–º...")
        
        try:
            # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
            all_docs = self.text_search.get_all_documents()
            
            if not all_docs:
                self.logger.warning("‚ö†Ô∏è –ù–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö")
                return []
            
            candidates = []
            query_words = query.lower().split()
            
            for doc in all_docs:
                doc_id = doc['id']
                content = doc.get('content', '').lower()
                file_name = doc.get('file_name', 'Unknown')
                category = doc.get('category', 'Unknown')
                
                # –ü—Ä–æ—Å—Ç–æ–π –ø–æ–¥—Å—á–µ—Ç —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π —Å–ª–æ–≤
                matches = sum(1 for word in query_words if word in content)
                
                if matches > 0:
                    # –í—ã—á–∏—Å–ª—è–µ–º –ø—Ä–æ—Å—Ç—É—é —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å
                    relevance = matches / len(query_words)
                    
                    candidates.append({
                        'document_id': doc_id,
                        'file_name': file_name,
                        'category': category,
                        'similarity': relevance,
                        'metadata': doc.get('metadata', ''),
                        'stage': 'text_fallback'
                    })
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
            candidates.sort(key=lambda x: x['similarity'], reverse=True)
            
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤
            candidates = candidates[:self.vector_candidates_limit]
            
            self.logger.info(f"üìÅ Fallback –ø–æ–∏—Å–∫ –Ω–∞—à–µ–ª {len(candidates)} –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤")
            return candidates
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ fallback –ø–æ–∏—Å–∫–∞: {e}")
            return []

    def get_search_statistics(self) -> Dict:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ–∏—Å–∫–∞"""
        return {
            'vector_candidates_limit': self.vector_candidates_limit,
            'final_results_limit': self.final_results_limit,
            'min_similarity': self.min_similarity,
            'search_type': 'sequential'
        }
