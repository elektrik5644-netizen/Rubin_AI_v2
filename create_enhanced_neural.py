#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è —É–≤–µ–ª–∏—á–µ–Ω–∏—è –≤—Ö–æ–¥–Ω–æ–≥–æ —Å–ª–æ—è Rubin AI
"""

def create_enhanced_neural_rubin():
    """–°–æ–∑–¥–∞–µ—Ç —É–ª—É—á—à–µ–Ω–Ω—É—é –≤–µ—Ä—Å–∏—é neural_rubin.py —Å —É–≤–µ–ª–∏—á–µ–Ω–Ω—ã–º –≤—Ö–æ–¥–Ω—ã–º —Å–ª–æ–µ–º"""
    
    enhanced_code = '''#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Enhanced Neural Rubin AI - –£–ª—É—á—à–µ–Ω–Ω–∞—è –Ω–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å —Å —É–≤–µ–ª–∏—á–µ–Ω–Ω—ã–º –≤—Ö–æ–¥–Ω—ã–º —Å–ª–æ–µ–º
"""

# –ü–æ–ø—ã—Ç–∫–∞ –∏–º–ø–æ—Ä—Ç–∞ ML –±–∏–±–ª–∏–æ—Ç–µ–∫ —Å fallback
try:
    import torch
    import torch.nn as nn
    import torch.optim as optim
    ML_AVAILABLE = True
except ImportError:
    ML_AVAILABLE = False
    # Mock –∫–ª–∞—Å—Å—ã –¥–ª—è —Ä–∞–±–æ—Ç—ã –±–µ–∑ ML –±–∏–±–ª–∏–æ—Ç–µ–∫
    class torch:
        @staticmethod
        def device(name):
            return name
        
        @staticmethod
        def cuda_is_available():
            return False
        
        @staticmethod
        def FloatTensor(data):
            return data
        
        @staticmethod
        def randn(*args):
            return [0.1] * (args[0] * args[1] if len(args) > 1 else args[0])
        
        @staticmethod
        def argmax(tensor, dim=None):
            return 0  # Mock category index
        
        @staticmethod
        def max(tensor):
            return 0.85  # Mock confidence
        
        @staticmethod
        def no_grad():
            class NoGradContext:
                def __enter__(self): return self
                def __exit__(self, *args): pass
            return NoGradContext()
    
    class nn:
        class Module:
            def __init__(self): pass
            def to(self, device): return self
            def state_dict(self): return {}
            def load_state_dict(self, state_dict): pass

try:
    from transformers import AutoTokenizer, AutoModel
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False

try:
    from sentence_transformers import SentenceTransformer
    SENTENCE_TRANSFORMERS_AVAILABLE = True
except ImportError:
    SENTENCE_TRANSFORMERS_AVAILABLE = False

import json
import numpy as np
from datetime import datetime
import logging
import pickle
import os

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# –î–æ—Å—Ç—É–ø–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏
ACTIVATION_FUNCTIONS = {
    'ReLU': nn.ReLU,
    'Tanh': nn.Tanh,
    'Softsign': nn.Softsign,
    'Sigmoid': nn.Sigmoid,
    'ELU': nn.ELU,
    'LeakyReLU': nn.LeakyReLU,
}

class EnhancedRubinNeuralNetwork(nn.Module):
    """–£–ª—É—á—à–µ–Ω–Ω–∞—è –Ω–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å Rubin AI —Å —É–≤–µ–ª–∏—á–µ–Ω–Ω—ã–º –≤—Ö–æ–¥–Ω—ã–º —Å–ª–æ–µ–º"""
    
    def __init__(self, input_size=768, hidden_sizes=[1536, 768, 384], num_classes=10, activations=None, dropout_rates=None):
        super(EnhancedRubinNeuralNetwork, self).__init__()
        
        if activations is None:
            activations = ['ReLU'] * len(hidden_sizes)

        if dropout_rates is None:
            dropout_rates = [0.2] * (len(hidden_sizes) - 1)
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –¥–ª–∏–Ω
        if len(activations) != len(hidden_sizes):
            raise ValueError("–î–ª–∏–Ω–∞ —Å–ø–∏—Å–∫–∞ –∞–∫—Ç–∏–≤–∞—Ü–∏–π –¥–æ–ª–∂–Ω–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–æ–≤–∞—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —Å–∫—Ä—ã—Ç—ã—Ö —Å–ª–æ–µ–≤.")
        if len(dropout_rates) != (len(hidden_sizes) - 1):
            logger.warning("‚ö†Ô∏è –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ dropout-—Å–ª–æ–µ–≤ –Ω–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —Å–∫—Ä—ã—Ç—ã—Ö —Å–ª–æ–µ–≤ - 1. –ë—É–¥—É—Ç –ø—Ä–∏–º–µ–Ω–µ–Ω—ã –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é.")
            dropout_rates = [0.2] * (len(hidden_sizes) - 1)

        layers = []
        current_size = input_size
        
        for i, hidden_size in enumerate(hidden_sizes):
            layers.append(nn.Linear(current_size, hidden_size))
            
            activation_name = activations[i]
            if activation_name not in ACTIVATION_FUNCTIONS:
                raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∞–∫—Ç–∏–≤–∞—Ü–∏–∏: {activation_name}. –î–æ—Å—Ç—É–ø–Ω—ã–µ: {list(ACTIVATION_FUNCTIONS.keys())}")
            layers.append(ACTIVATION_FUNCTIONS[activation_name]())

            if i < len(hidden_sizes) - 1 and dropout_rates[i] > 0:
                layers.append(nn.Dropout(dropout_rates[i]))
            
            current_size = hidden_size
            
        layers.append(nn.Linear(current_size, num_classes))
        
        self.encoder = nn.Sequential(*layers)
        
        # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Ç–∏–ø–æ–≤ –≤–æ–ø—Ä–æ—Å–æ–≤
        self.classifier = nn.Softmax(dim=1)
        
    def forward(self, x):
        """–ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥ —á–µ—Ä–µ–∑ —Å–µ—Ç—å"""
        encoded = self.encoder(x)
        classified = self.classifier(encoded)
        return classified

class EnhancedNeuralRubinAI:
    """–£–ª—É—á—à–µ–Ω–Ω—ã–π –≥–ª–∞–≤–Ω—ã–π –∫–ª–∞—Å—Å –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏ Rubin AI"""
    
    def __init__(self, input_size=768):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        logger.info(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device}")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
        self.sentence_model = None
        self.neural_network = None
        self.tokenizer = None
        self.knowledge_base = {}
        self.conversation_history = []
        self.input_size = input_size
        
        # –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ –≤–æ–ø—Ä–æ—Å–æ–≤
        self.categories = [
            '–º–∞—Ç–µ–º–∞—Ç–∏–∫–∞', '—Ñ–∏–∑–∏–∫–∞', '—ç–ª–µ–∫—Ç—Ä–æ—Ç–µ—Ö–Ω–∏–∫–∞', '–ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ',
            '–≥–µ–æ–º–µ—Ç—Ä–∏—è', '—Ö–∏–º–∏—è', '–æ–±—â–∏–µ_–≤–æ–ø—Ä–æ—Å—ã', '—Ç–µ—Ö–Ω–∏–∫–∞', '–Ω–∞—É–∫–∞', '–¥—Ä—É–≥–æ–µ',
            'time_series', 'graph_analysis', 'data_visualization', 'formula_calculation'
        ]
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é physics, –µ—Å–ª–∏ –µ–µ –Ω–µ—Ç
        if 'physics' not in self.categories:
            self.categories.insert(1, 'physics')
        
        self.initialize_enhanced_models()
    
    def initialize_enhanced_models(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —É–ª—É—á—à–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —Å —É–≤–µ–ª–∏—á–µ–Ω–Ω—ã–º –≤—Ö–æ–¥–Ω—ã–º —Å–ª–æ–µ–º"""
        try:
            logger.info("üß† –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —É–ª—É—á—à–µ–Ω–Ω–æ–π –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏...")
            
            if SENTENCE_TRANSFORMERS_AVAILABLE:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –±–æ–ª–µ–µ –º–æ—â–Ω—É—é –º–æ–¥–µ–ª—å –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è embeddings
                if self.input_size == 768:
                    self.sentence_model = SentenceTransformer('all-mpnet-base-v2')
                    logger.info("‚úÖ –£–ª—É—á—à–µ–Ω–Ω—ã–π Sentence Transformer –∑–∞–≥—Ä—É–∂–µ–Ω (768 —Ä–∞–∑–º–µ—Ä)")
                elif self.input_size == 1152:
                    self.sentence_model = SentenceTransformer('all-MiniLM-L12-v2')
                    logger.info("‚úÖ –£–ª—É—á—à–µ–Ω–Ω—ã–π Sentence Transformer –∑–∞–≥—Ä—É–∂–µ–Ω (1152 —Ä–∞–∑–º–µ—Ä)")
                else:
                    self.sentence_model = SentenceTransformer('all-MiniLM-L6-v2')
                    logger.info("‚úÖ –ë–∞–∑–æ–≤—ã–π Sentence Transformer –∑–∞–≥—Ä—É–∂–µ–Ω (384 —Ä–∞–∑–º–µ—Ä)")
            else:
                logger.warning("‚ö†Ô∏è SentenceTransformer –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º mock")
                self.sentence_model = None
            
            if ML_AVAILABLE:
                # –£–ª—É—á—à–µ–Ω–Ω–∞—è –Ω–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å —Å —É–≤–µ–ª–∏—á–µ–Ω–Ω—ã–º –≤—Ö–æ–¥–Ω—ã–º —Å–ª–æ–µ–º
                hidden_sizes = [self.input_size * 2, self.input_size, self.input_size // 2]
                
                self.neural_network = EnhancedRubinNeuralNetwork(
                    input_size=self.input_size,
                    hidden_sizes=hidden_sizes,
                    num_classes=len(self.categories),
                    activations=['ReLU', 'ReLU', 'ReLU'],
                    dropout_rates=[0.2, 0.2]
                ).to(self.device)
                
                logger.info("‚úÖ –£–ª—É—á—à–µ–Ω–Ω–∞—è –Ω–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")
                logger.info(f"üìä –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞: {self.input_size} -> {hidden_sizes} -> {len(self.categories)}")
                
            else:
                logger.warning("‚ö†Ô∏è PyTorch –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º mock –Ω–µ–π—Ä–æ–Ω–Ω—É—é —Å–µ—Ç—å")
                self.neural_network = None
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ —É–ª—É—á—à–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏: {e}")
            # Fallback –∫ –ø—Ä–æ—Å—Ç–æ–π –º–æ–¥–µ–ª–∏
            self.sentence_model = None
            self.neural_network = None
    
    def create_enhanced_embedding(self, text):
        """–°–æ–∑–¥–∞–µ—Ç —É–ª—É—á—à–µ–Ω–Ω—ã–π —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ç–µ–∫—Å—Ç–∞"""
        try:
            if self.sentence_model and SENTENCE_TRANSFORMERS_AVAILABLE:
                embedding = self.sentence_model.encode(text)
                if ML_AVAILABLE:
                    return torch.FloatTensor(embedding).unsqueeze(0).to(self.device)
                else:
                    return embedding
            else:
                # –ü—Ä–æ—Å—Ç–æ–π fallback —ç–º–±–µ–¥–¥–∏–Ω–≥ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–ª–∏–Ω—ã —Ç–µ–∫—Å—Ç–∞ –∏ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
                import random
                random.seed(len(text))  # –î–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —ç–º–±–µ–¥–¥–∏–Ω–≥
                return [random.random() for _ in range(self.input_size)]
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞: {e}")
            return [0.1] * self.input_size
    
    def classify_question(self, text):
        """–ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç –≤–æ–ø—Ä–æ—Å —Å –ø–æ–º–æ—â—å—é —É–ª—É—á—à–µ–Ω–Ω–æ–π –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏"""
        try:
            if not self.neural_network or not ML_AVAILABLE:
                return self._simple_classification(text)
            
            # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥
            embedding = self.create_enhanced_embedding(text)
            
            # –ü—Ä–æ–≥–æ–Ω—è–µ–º —á–µ—Ä–µ–∑ –Ω–µ–π—Ä–æ–Ω–Ω—É—é —Å–µ—Ç—å
            with torch.no_grad():
                output = self.neural_network(embedding)
                predicted_class = torch.argmax(output, dim=1).item()
            
            category = self.categories[predicted_class]
            confidence = float(torch.max(output).item())
            
            logger.info(f"üéØ –£–ª—É—á—à–µ–Ω–Ω–∞—è –Ω–µ–π—Ä–æ–Ω–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è: {category} (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.2f})")
            return category, confidence
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —É–ª—É—á—à–µ–Ω–Ω–æ–π –Ω–µ–π—Ä–æ–Ω–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏: {e}")
            return self._simple_classification(text)
    
    def _simple_classification(self, text):
        """–ü—Ä–æ—Å—Ç–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –±–µ–∑ –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏"""
        text_lower = text.lower()
        
        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–π
        keywords = {
            '–º–∞—Ç–µ–º–∞—Ç–∏–∫–∞': ['—Å–∫–æ–ª—å–∫–æ', '+', '-', '*', '/', '–≤—ã—á–∏—Å–ª–∏', '—Ä–µ—à–∏'],
            '—Ñ–∏–∑–∏–∫–∞': ['—Å–∫–æ—Ä–æ—Å—Ç—å', '–≤—Ä–µ–º—è', '—Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ', '—Å–∏–ª–∞', '—ç–Ω–µ—Ä–≥–∏—è'],
            '—ç–ª–µ–∫—Ç—Ä–æ—Ç–µ—Ö–Ω–∏–∫–∞': ['—Ç—Ä–∞–Ω–∑–∏—Å—Ç–æ—Ä', '–¥–∏–æ–¥', '—Ç–æ–∫', '–Ω–∞–ø—Ä—è–∂–µ–Ω–∏–µ', '—Å–æ–ø—Ä–æ—Ç–∏–≤–ª–µ–Ω–∏–µ'],
            '–ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ': ['–∫–æ–¥', 'python', 'c++', '–∞–ª–≥–æ—Ä–∏—Ç–º', '–ø—Ä–æ–≥—Ä–∞–º–º–∞'],
            '–≥–µ–æ–º–µ—Ç—Ä–∏—è': ['—É–≥–æ–ª', '—Ç—Ä–µ—É–≥–æ–ª—å–Ω–∏–∫', '–ø–ª–æ—â–∞–¥—å', '–ø–µ—Ä–∏–º–µ—Ç—Ä']
        }
        
        for category, words in keywords.items():
            if any(word in text_lower for word in words):
                return category, 0.8
        
        return '–æ–±—â–∏–µ_–≤–æ–ø—Ä–æ—Å—ã', 0.5
    
    def generate_response(self, question):
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —É–ª—É—á—à–µ–Ω–Ω–æ–π –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏"""
        try:
            logger.info(f"üß† –£–ª—É—á—à–µ–Ω–Ω–∞—è –Ω–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç: {question[:50]}...")
            
            category, confidence = self.classify_question(question)
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
            if category == '–º–∞—Ç–µ–º–∞—Ç–∏–∫–∞':
                response = f"üßÆ **–£–ª—É—á—à–µ–Ω–Ω–∞—è –Ω–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å —Ä–µ—à–∞–µ—Ç –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫—É—é –∑–∞–¥–∞—á—É:**\\n\\n{question}\\n\\n**–ö–∞—Ç–µ–≥–æ—Ä–∏—è:** {category}\\n**–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å:** {confidence:.1%}\\n\\n*–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —É–ª—É—á—à–µ–Ω–Ω–æ–π –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç—å—é —Å {self.input_size} –≤—Ö–æ–¥–Ω—ã–º–∏ –Ω–µ–π—Ä–æ–Ω–∞–º–∏*"
            elif category == '—Ñ–∏–∑–∏–∫–∞':
                response = f"‚ö° **–£–ª—É—á—à–µ–Ω–Ω–∞—è –Ω–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ñ–∏–∑–∏—á–µ—Å–∫—É—é –∑–∞–¥–∞—á—É:**\\n\\n{question}\\n\\n**–ö–∞—Ç–µ–≥–æ—Ä–∏—è:** {category}\\n**–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å:** {confidence:.1%}\\n\\n*–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —É–ª—É—á—à–µ–Ω–Ω–æ–π –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç—å—é —Å {self.input_size} –≤—Ö–æ–¥–Ω—ã–º–∏ –Ω–µ–π—Ä–æ–Ω–∞–º–∏*"
            elif category == '—ç–ª–µ–∫—Ç—Ä–æ—Ç–µ—Ö–Ω–∏–∫–∞':
                response = f"üîå **–£–ª—É—á—à–µ–Ω–Ω–∞—è –Ω–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —ç–ª–µ–∫—Ç—Ä–æ—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π –≤–æ–ø—Ä–æ—Å:**\\n\\n{question}\\n\\n**–ö–∞—Ç–µ–≥–æ—Ä–∏—è:** {category}\\n**–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å:** {confidence:.1%}\\n\\n*–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —É–ª—É—á—à–µ–Ω–Ω–æ–π –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç—å—é —Å {self.input_size} –≤—Ö–æ–¥–Ω—ã–º–∏ –Ω–µ–π—Ä–æ–Ω–∞–º–∏*"
            elif category == '–ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ':
                response = f"üíª **–£–ª—É—á—à–µ–Ω–Ω–∞—è –Ω–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≤–æ–ø—Ä–æ—Å –ø–æ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—é:**\\n\\n{question}\\n\\n**–ö–∞—Ç–µ–≥–æ—Ä–∏—è:** {category}\\n**–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å:** {confidence:.1%}\\n\\n*–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —É–ª—É—á—à–µ–Ω–Ω–æ–π –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç—å—é —Å {self.input_size} –≤—Ö–æ–¥–Ω—ã–º–∏ –Ω–µ–π—Ä–æ–Ω–∞–º–∏*"
            else:
                response = f"ü§ñ **–£–ª—É—á—à–µ–Ω–Ω–∞—è –Ω–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–ª–∞ –≤–æ–ø—Ä–æ—Å:**\\n\\n{question}\\n\\n**–ö–∞—Ç–µ–≥–æ—Ä–∏—è:** {category}\\n**–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å:** {confidence:.1%}\\n\\n*–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —É–ª—É—á—à–µ–Ω–Ω–æ–π –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç—å—é —Å {self.input_size} –≤—Ö–æ–¥–Ω—ã–º–∏ –Ω–µ–π—Ä–æ–Ω–∞–º–∏*"
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∏—Å—Ç–æ—Ä–∏—é
            self.conversation_history.append({
                'timestamp': datetime.now().isoformat(),
                'question': question,
                'category': category,
                'confidence': confidence,
                'input_size': self.input_size
            })
            
            return {
                'response': response,
                'category': category,
                'confidence': confidence,
                'neural_network': True,
                'enhanced': True,
                'input_size': self.input_size,
                'timestamp': datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞: {e}")
            return {
                'response': f'–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –≤ —É–ª—É—á—à–µ–Ω–Ω–æ–π –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏: {str(e)}',
                'category': 'error',
                'confidence': 0.0,
                'neural_network': False,
                'enhanced': False
            }
    
    def get_enhanced_stats(self):
        """–ü–æ–ª—É—á–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —É–ª—É—á—à–µ–Ω–Ω–æ–π –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏"""
        total_neurons = self.input_size + (self.input_size * 2) + self.input_size + (self.input_size // 2) + len(self.categories)
        total_weights = (self.input_size * (self.input_size * 2) + 
                        (self.input_size * 2) * self.input_size + 
                        self.input_size * (self.input_size // 2) + 
                        (self.input_size // 2) * len(self.categories))
        
        return {
            'device': str(self.device),
            'input_size': self.input_size,
            'neural_network_active': self.neural_network is not None,
            'sentence_model_active': self.sentence_model is not None,
            'categories': self.categories,
            'conversation_count': len(self.conversation_history),
            'total_neurons': total_neurons,
            'total_weights': total_weights,
            'model_parameters': sum(p.numel() for p in self.neural_network.parameters()) if self.neural_network else 0
        }

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä —É–ª—É—á—à–µ–Ω–Ω–æ–π –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏
enhanced_neural_rubin = None

def get_enhanced_neural_rubin(input_size=768):
    """–ü–æ–ª—É—á–∞–µ—Ç –≥–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä —É–ª—É—á—à–µ–Ω–Ω–æ–π –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏"""
    global enhanced_neural_rubin
    if enhanced_neural_rubin is None or enhanced_neural_rubin.input_size != input_size:
        enhanced_neural_rubin = EnhancedNeuralRubinAI(input_size)
    return enhanced_neural_rubin

if __name__ == "__main__":
    print("üöÄ –ó–∞–ø—É—Å–∫ Enhanced Rubin AI –≤ —Ç–µ—Å—Ç–æ–≤–æ–º —Ä–µ–∂–∏–º–µ")
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º —Ä–∞–∑–Ω—ã–µ —Ä–∞–∑–º–µ—Ä—ã –≤—Ö–æ–¥–Ω–æ–≥–æ —Å–ª–æ—è
    test_sizes = [768, 1152, 512]
    
    for size in test_sizes:
        print(f"\\n{'='*70}")
        print(f"–¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –° –í–•–û–î–ù–´–ú –°–õ–û–ï–ú: {size} –ù–ï–ô–†–û–ù–û–í")
        print(f"{'='*70}")
        
        ai = get_enhanced_neural_rubin(size)
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        stats = ai.get_enhanced_stats()
        print(f"üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –£–õ–£–ß–®–ï–ù–ù–û–ô –ù–ï–ô–†–û–ù–ù–û–ô –°–ï–¢–ò:")
        print(f"‚Ä¢ –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {stats['device']}")
        print(f"‚Ä¢ –†–∞–∑–º–µ—Ä –≤—Ö–æ–¥–Ω–æ–≥–æ —Å–ª–æ—è: {stats['input_size']}")
        print(f"‚Ä¢ –ù–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å –∞–∫—Ç–∏–≤–Ω–∞: {stats['neural_network_active']}")
        print(f"‚Ä¢ –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–µ–π—Ä–æ–Ω–æ–≤: {stats['total_neurons']:,}")
        print(f"‚Ä¢ –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–µ—Å–æ–≤: {stats['total_weights']:,}")
        print(f"‚Ä¢ –ü–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≤ –º–æ–¥–µ–ª–∏: {stats['model_parameters']:,}")
        
        # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
        test_questions = [
            "–†–µ—à–∏ —É—Ä–∞–≤–Ω–µ–Ω–∏–µ x^2 + 5x + 6 = 0",
            "–ß—Ç–æ —Ç–∞–∫–æ–µ —Ç—Ä–∞–Ω–∑–∏—Å—Ç–æ—Ä?",
            "–ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç —Ü–∏–∫–ª for –≤ Python?",
            "–û–±—ä—è—Å–Ω–∏ –∑–∞–∫–æ–Ω –ö–∏—Ä—Ö–≥–æ—Ñ–∞"
        ]
        
        for question in test_questions:
            print(f"\\n‚ùì –í–æ–ø—Ä–æ—Å: {question}")
            response = ai.generate_response(question)
            print(f"ü§ñ –û—Ç–≤–µ—Ç: {response['response']}")
            print(f"üìÇ –ö–∞—Ç–µ–≥–æ—Ä–∏—è: {response['category']}")
            print(f"üìä –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {response['confidence']:.1%}")
'''
    
    return enhanced_code

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    
    print("=" * 70)
    print("–°–û–ó–î–ê–ù–ò–ï –£–õ–£–ß–®–ï–ù–ù–û–ô –ù–ï–ô–†–û–ù–ù–û–ô –°–ï–¢–ò RUBIN AI")
    print("=" * 70)
    
    # –°–æ–∑–¥–∞–µ–º —É–ª—É—á—à–µ–Ω–Ω—É—é –≤–µ—Ä—Å–∏—é
    enhanced_code = create_enhanced_neural_rubin()
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ —Ñ–∞–π–ª
    with open('enhanced_neural_rubin.py', 'w', encoding='utf-8') as f:
        f.write(enhanced_code)
    
    print("‚úÖ –°–æ–∑–¥–∞–Ω —Ñ–∞–π–ª enhanced_neural_rubin.py")
    print()
    
    # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–π —Å–∫—Ä–∏–ø—Ç
    test_code = '''#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–¢–µ—Å—Ç —É–ª—É—á—à–µ–Ω–Ω–æ–π –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏ Rubin AI
"""

from enhanced_neural_rubin import get_enhanced_neural_rubin

def test_enhanced_networks():
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç —Ä–∞–∑–Ω—ã–µ —Ä–∞–∑–º–µ—Ä—ã –≤—Ö–æ–¥–Ω–æ–≥–æ —Å–ª–æ—è"""
    
    print("=" * 70)
    print("–¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –£–õ–£–ß–®–ï–ù–ù–´–• –ù–ï–ô–†–û–ù–ù–´–• –°–ï–¢–ï–ô")
    print("=" * 70)
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º —Ä–∞–∑–Ω—ã–µ —Ä–∞–∑–º–µ—Ä—ã
    test_sizes = [768, 1152, 512]
    
    for size in test_sizes:
        print(f"\\n{'='*50}")
        print(f"–¢–ï–°–¢ –° –í–•–û–î–ù–´–ú –°–õ–û–ï–ú: {size} –ù–ï–ô–†–û–ù–û–í")
        print(f"{'='*50}")
        
        try:
            ai = get_enhanced_neural_rubin(size)
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            stats = ai.get_enhanced_stats()
            print(f"üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
            print(f"‚Ä¢ –†–∞–∑–º–µ—Ä –≤—Ö–æ–¥–Ω–æ–≥–æ —Å–ª–æ—è: {stats['input_size']}")
            print(f"‚Ä¢ –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–µ–π—Ä–æ–Ω–æ–≤: {stats['total_neurons']:,}")
            print(f"‚Ä¢ –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–µ—Å–æ–≤: {stats['total_weights']:,}")
            print(f"‚Ä¢ –ü–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≤ –º–æ–¥–µ–ª–∏: {stats['model_parameters']:,}")
            
            # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
            test_questions = [
                "–†–µ—à–∏ —É—Ä–∞–≤–Ω–µ–Ω–∏–µ x^2 + 5x + 6 = 0",
                "–ß—Ç–æ —Ç–∞–∫–æ–µ —Ç—Ä–∞–Ω–∑–∏—Å—Ç–æ—Ä?",
                "–ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç —Ü–∏–∫–ª for –≤ Python?",
                "–û–±—ä—è—Å–Ω–∏ –∑–∞–∫–æ–Ω –ö–∏—Ä—Ö–≥–æ—Ñ–∞"
            ]
            
            for question in test_questions:
                response = ai.generate_response(question)
                print(f"\\n‚ùì {question}")
                print(f"üìÇ –ö–∞—Ç–µ–≥–æ—Ä–∏—è: {response['category']}")
                print(f"üìä –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {response['confidence']:.1%}")
                
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ —Ä–∞–∑–º–µ—Ä–∞ {size}: {e}")
    
    print("\\n" + "=" * 70)
    print("–¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û")
    print("=" * 70)

if __name__ == "__main__":
    test_enhanced_networks()
'''
    
    with open('test_enhanced_neural.py', 'w', encoding='utf-8') as f:
        f.write(test_code)
    
    print("‚úÖ –°–æ–∑–¥–∞–Ω —Ñ–∞–π–ª test_enhanced_neural.py")
    print()
    
    print("=" * 70)
    print("–ò–ù–°–¢–†–£–ö–¶–ò–ò –ü–û –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–Æ")
    print("=" * 70)
    
    instructions = [
        "1. –ó–∞–ø—É—Å—Ç–∏—Ç–µ test_enhanced_neural.py –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —Ä–∞–∑–Ω—ã—Ö —Ä–∞–∑–º–µ—Ä–æ–≤ –≤—Ö–æ–¥–Ω–æ–≥–æ —Å–ª–æ—è",
        "2. –°—Ä–∞–≤–Ω–∏—Ç–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏ –∫–∞—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏",
        "3. –í—ã–±–µ—Ä–∏—Ç–µ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –≤—Ö–æ–¥–Ω–æ–≥–æ —Å–ª–æ—è –¥–ª—è –≤–∞—à–∏—Ö –∑–∞–¥–∞—á",
        "4. –ò–Ω—Ç–µ–≥—Ä–∏—Ä—É–π—Ç–µ enhanced_neural_rubin.py –≤ –æ—Å–Ω–æ–≤–Ω—É—é —Å–∏—Å—Ç–µ–º—É Rubin AI",
        "5. –û–±–Ω–æ–≤–∏—Ç–µ Sentence Transformer –Ω–∞ –±–æ–ª–µ–µ –º–æ—â–Ω—É—é –º–æ–¥–µ–ª—å",
        "6. –ü–µ—Ä–µ–æ–±—É—á–∏—Ç–µ –º–æ–¥–µ–ª—å –Ω–∞ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö"
    ]
    
    for instruction in instructions:
        print(f"‚Ä¢ {instruction}")
    
    print()
    print("=" * 70)
    print("–†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò")
    print("=" * 70)
    
    recommendations = [
        "‚Ä¢ –ù–∞—á–Ω–∏—Ç–µ —Å 768 –Ω–µ–π—Ä–æ–Ω–æ–≤ - –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –±–∞–ª–∞–Ω—Å –∫–∞—á–µ—Å—Ç–≤–∞ –∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏",
        "‚Ä¢ –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ 'all-mpnet-base-v2' –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è 768-–º–µ—Ä–Ω—ã—Ö embeddings",
        "‚Ä¢ –ú–æ–Ω–∏—Ç–æ—Ä—å—Ç–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏ –∫–∞—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏",
        "‚Ä¢ –î–æ–±–∞–≤—å—Ç–µ –±–æ–ª—å—à–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –ø—Ä–∏ —É–≤–µ–ª–∏—á–µ–Ω–∏–∏ —Ä–∞–∑–º–µ—Ä–∞ —Å–µ—Ç–∏",
        "‚Ä¢ –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ GPU –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è –±–æ–ª—å—à–∏—Ö —Å–µ—Ç–µ–π"
    ]
    
    for rec in recommendations:
        print(rec)

if __name__ == "__main__":
    main()










