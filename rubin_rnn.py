#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
RNN (Recurrent Neural Network) –¥–ª—è Rubin AI
–†–µ–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω—ã—Ö –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π
"""

import logging
from typing import Dict, List, Any, Tuple, Optional
import numpy as np

# –ü–æ–ø—ã—Ç–∫–∞ –∏–º–ø–æ—Ä—Ç–∞ ML –±–∏–±–ª–∏–æ—Ç–µ–∫ —Å fallback
try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    import torch.optim as optim
    from torch.utils.data import DataLoader, TensorDataset
    ML_AVAILABLE = True
except ImportError:
    ML_AVAILABLE = False
    # Mock –∫–ª–∞—Å—Å—ã –¥–ª—è —Ä–∞–±–æ—Ç—ã –±–µ–∑ ML –±–∏–±–ª–∏–æ—Ç–µ–∫
    class torch:
        @staticmethod
        def device(name):
            return name
        
        @staticmethod
        def cuda_is_available():
            return False
        
        @staticmethod
        def FloatTensor(data):
            return data
        
        @staticmethod
        def randn(*args):
            return np.random.randn(*args)
    
    class nn:
        class Module:
            def __init__(self):
                pass
        
        class RNN(nn.Module):
            def __init__(self, input_size, hidden_size, num_layers=1, **kwargs):
                super().__init__()
                self.input_size = input_size
                self.hidden_size = hidden_size
                self.num_layers = num_layers
        
        class LSTM(nn.Module):
            def __init__(self, input_size, hidden_size, num_layers=1, **kwargs):
                super().__init__()
                self.input_size = input_size
                self.hidden_size = hidden_size
                self.num_layers = num_layers
        
        class GRU(nn.Module):
            def __init__(self, input_size, hidden_size, num_layers=1, **kwargs):
                super().__init__()
                self.input_size = input_size
                self.hidden_size = hidden_size
                self.num_layers = num_layers
        
        class Linear(nn.Modinear):
            def __init__(self, in_features, out_features):
                super().__init__()
                self.in_features = in_features
                self.out_features = out_features
        
        class Dropout(nn.Module):
            def __init__(self, p=0.5):
                super().__init__()
                self.p = p
        
        class BatchNorm1d(nn.Module):
            def __init__(self, num_features):
                super().__init__()
                self.num_features = num_features

logging.basicConfig(level=logging.INFO, format='%(levelname)s:%(name)s:%(message)s')
logger = logging.getLogger(__name__)

class RubinRNN(nn.Module):
    """–ë–∞–∑–æ–≤—ã–π RNN –¥–ª—è Rubin AI"""
    
    def __init__(self, input_size, hidden_size, num_layers=1, dropout=0.0, bidirectional=False):
        super(RubinRNN, self).__init__()
        
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.dropout = dropout
        self.bidirectional = bidirectional
        
        if ML_AVAILABLE:
            self.rnn = nn.RNN(
                input_size=input_size,
                hidden_size=hidden_size,
                num_layers=num_layers,
                dropout=dropout if num_layers > 1 else 0,
                bidirectional=bidirectional,
                batch_first=True
            )
        else:
            self.rnn = nn.RNN(input_size, hidden_size, num_layers)
    
    def forward(self, x, hidden=None):
        """–ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥ —á–µ—Ä–µ–∑ RNN"""
        if ML_AVAILABLE:
            output, hidden = self.rnn(x, hidden)
            return output, hidden
        else:
            # Mock forward pass
            batch_size, seq_len, _ = x.shape
            output = torch.randn(batch_size, seq_len, self.hidden_size)
            hidden = torch.randn(self.num_layers, batch_size, self.hidden_size)
            return output, hidden
    
    def init_hidden(self, batch_size):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è"""
        if ML_AVAILABLE:
            num_directions = 2 if self.bidirectional else 1
            hidden = torch.zeros(
                self.num_layers * num_directions,
                batch_size,
                self.hidden_size
            )
            return hidden
        else:
            return torch.randn(self.num_layers, batch_size, self.hidden_size)

class RubinLSTM(nn.Module):
    """LSTM –¥–ª—è Rubin AI"""
    
    def __init__(self, input_size, hidden_size, num_layers=1, dropout=0.0, bidirectional=False):
        super(RubinLSTM, self).__init__()
        
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.dropout = dropout
        self.bidirectional = bidirectional
        
        if ML_AVAILABLE:
            self.lstm = nn.LSTM(
                input_size=input_size,
                hidden_size=hidden_size,
                num_layers=num_layers,
                dropout=dropout if num_layers > 1 else 0,
                bidirectional=bidirectional,
                batch_first=True
            )
        else:
            self.lstm = nn.LSTM(input_size, hidden_size, num_layers)
    
    def forward(self, x, hidden=None):
        """–ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥ —á–µ—Ä–µ–∑ LSTM"""
        if ML_AVAILABLE:
            output, (hidden, cell) = self.lstm(x, hidden)
            return output, (hidden, cell)
        else:
            # Mock forward pass
            batch_size, seq_len, _ = x.shape
            output = torch.randn(batch_size, seq_len, self.hidden_size)
            hidden = torch.randn(self.num_layers, batch_size, self.hidden_size)
            cell = torch.randn(self.num_layers, batch_size, self.hidden_size)
            return output, (hidden, cell)
    
    def init_hidden(self, batch_size):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è –∏ —è—á–µ–π–∫–∏"""
        if ML_AVAILABLE:
            num_directions = 2 if self.bidirectional else 1
            hidden = torch.zeros(
                self.num_layers * num_directions,
                batch_size,
                self.hidden_size
            )
            cell = torch.zeros(
                self.num_layers * num_directions,
                batch_size,
                self.hidden_size
            )
            return (hidden, cell)
        else:
            hidden = torch.randn(self.num_layers, batch_size, self.hidden_size)
            cell = torch.randn(self.num_layers, batch_size, self.hidden_size)
            return (hidden, cell)

class RubinGRU(nn.Module):
    """GRU –¥–ª—è Rubin AI"""
    
    def __init__(self, input_size, hidden_size, num_layers=1, dropout=0.0, bidirectional=False):
        super(RubinGRU, self).__init__()
        
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.dropout = dropout
        self.bidirectional = bidirectional
        
        if ML_AVAILABLE:
            self.gru = nn.GRU(
                input_size=input_size,
                hidden_size=hidden_size,
                num_layers=num_layers,
                dropout=dropout if num_layers > 1 else 0,
                bidirectional=bidirectional,
                batch_first=True
            )
        else:
            self.gru = nn.GRU(input_size, hidden_size, num_layers)
    
    def forward(self, x, hidden=None):
        """–ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥ —á–µ—Ä–µ–∑ GRU"""
        if ML_AVAILABLE:
            output, hidden = self.gru(x, hidden)
            return output, hidden
        else:
            # Mock forward pass
            batch_size, seq_len, _ = x.shape
            output = torch.randn(batch_size, seq_len, self.hidden_size)
            hidden = torch.randn(self.num_layers, batch_size, self.hidden_size)
            return output, hidden
    
    def init_hidden(self, batch_size):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è"""
        if ML_AVAILABLE:
            num_directions = 2 if self.bidirectional else 1
            hidden = torch.zeros(
                self.num_layers * num_directions,
                batch_size,
                self.hidden_size
            )
            return hidden
        else:
            return torch.randn(self.num_layers, batch_size, self.hidden_size)

class RubinRNNClassifier(nn.Module):
    """RNN –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –¥–ª—è Rubin AI"""
    
    def __init__(self, input_size, hidden_size, num_classes, rnn_type="lstm", 
                 num_layers=1, dropout=0.0, bidirectional=False):
        super(RubinRNNClassifier, self).__init__()
        
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.num_classes = num_classes
        self.rnn_type = rnn_type
        self.num_layers = num_layers
        self.dropout = dropout
        self.bidirectional = bidirectional
        
        # –í—ã–±–∏—Ä–∞–µ–º —Ç–∏–ø RNN
        if rnn_type.lower() == "lstm":
            self.rnn = RubinLSTM(input_size, hidden_size, num_layers, dropout, bidirectional)
        elif rnn_type.lower() == "gru":
            self.rnn = RubinGRU(input_size, hidden_size, num_layers, dropout, bidirectional)
        else:
            self.rnn = RubinRNN(input_size, hidden_size, num_layers, dropout, bidirectional)
        
        # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä
        rnn_output_size = hidden_size * (2 if bidirectional else 1)
        self.classifier = nn.Sequential(
            nn.Dropout(dropout),
            nn.Linear(rnn_output_size, hidden_size // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_size // 2, num_classes)
        )
    
    def forward(self, x):
        """–ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥ —á–µ—Ä–µ–∑ RNN –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä"""
        batch_size = x.size(0)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–∫—Ä—ã—Ç–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
        hidden = self.rnn.init_hidden(batch_size)
        
        # –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥ —á–µ—Ä–µ–∑ RNN
        if self.rnn_type.lower() == "lstm":
            output, (hidden, cell) = self.rnn(x, hidden)
        else:
            output, hidden = self.rnn(x, hidden)
        
        # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –≤—ã—Ö–æ–¥ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
        last_output = output[:, -1, :]
        
        # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
        logits = self.classifier(last_output)
        return logits

class RubinRNNRegressor(nn.Module):
    """RNN —Ä–µ–≥—Ä–µ—Å—Å–æ—Ä –¥–ª—è Rubin AI"""
    
    def __init__(self, input_size, hidden_size, output_size, rnn_type="lstm", 
                 num_layers=1, dropout=0.0, bidirectional=False):
        super(RubinRNNRegressor, self).__init__()
        
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.output_size = output_size
        self.rnn_type = rnn_type
        self.num_layers = num_layers
        self.dropout = dropout
        self.bidirectional = bidirectional
        
        # –í—ã–±–∏—Ä–∞–µ–º —Ç–∏–ø RNN
        if rnn_type.lower() == "lstm":
            self.rnn = RubinLSTM(input_size, hidden_size, num_layers, dropout, bidirectional)
        elif rnn_type.lower() == "gru":
            self.rnn = RubinGRU(input_size, hidden_size, num_layers, dropout, bidirectional)
        else:
            self.rnn = RubinRNN(input_size, hidden_size, num_layers, dropout, bidirectional)
        
        # –†–µ–≥—Ä–µ—Å—Å–æ—Ä
        rnn_output_size = hidden_size * (2 if bidirectional else 1)
        self.regressor = nn.Sequential(
            nn.Dropout(dropout),
            nn.Linear(rnn_output_size, hidden_size // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_size // 2, output_size)
        )
    
    def forward(self, x):
        """–ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥ —á–µ—Ä–µ–∑ RNN —Ä–µ–≥—Ä–µ—Å—Å–æ—Ä"""
        batch_size = x.size(0)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–∫—Ä—ã—Ç–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
        hidden = self.rnn.init_hidden(batch_size)
        
        # –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥ —á–µ—Ä–µ–∑ RNN
        if self.rnn_type.lower() == "lstm":
            output, (hidden, cell) = self.rnn(x, hidden)
        else:
            output, hidden = self.rnn(x, hidden)
        
        # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –≤—ã—Ö–æ–¥ –¥–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏
        last_output = output[:, -1, :]
        
        # –†–µ–≥—Ä–µ—Å—Å–∏—è
        prediction = self.regressor(last_output)
        return prediction

class RubinRNNManager:
    """–ú–µ–Ω–µ–¥–∂–µ—Ä RNN –¥–ª—è Rubin AI"""
    
    def __init__(self):
        self.device = torch.device("cuda" if torch.cuda.is_available() and ML_AVAILABLE else "cpu")
        self.models = {}
        self.training_history = {}
        logger.info(f"üß† RNN Manager –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ: {self.device}")
    
    def create_rnn_classifier(self, input_size, hidden_size, num_classes, rnn_type="lstm", 
                            num_layers=1, dropout=0.0, bidirectional=False, model_name="rnn_classifier"):
        """–°–æ–∑–¥–∞–Ω–∏–µ RNN –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞"""
        try:
            model = RubinRNNClassifier(
                input_size=input_size,
                hidden_size=hidden_size,
                num_classes=num_classes,
                rnn_type=rnn_type,
                num_layers=num_layers,
                dropout=dropout,
                bidirectional=bidirectional
            )
            model = model.to(self.device)
            self.models[model_name] = model
            
            logger.info(f"‚úÖ RNN –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Å–æ–∑–¥–∞–Ω: {model_name}")
            logger.info(f"   –¢–∏–ø RNN: {rnn_type.upper()}")
            logger.info(f"   –í—Ö–æ–¥–Ω–æ–π —Ä–∞–∑–º–µ—Ä: {input_size}")
            logger.info(f"   –°–∫—Ä—ã—Ç—ã–π —Ä–∞–∑–º–µ—Ä: {hidden_size}")
            logger.info(f"   –ö–ª–∞—Å—Å—ã: {num_classes}")
            logger.info(f"   –°–ª–æ–∏: {num_layers}")
            logger.info(f"   –î–≤—É–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π: {bidirectional}")
            logger.info(f"   –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: {sum(p.numel() for p in model.parameters()):,}")
            
            return model
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è RNN –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞: {e}")
            return None
    
    def create_rnn_regressor(self, input_size, hidden_size, output_size, rnn_type="lstm", 
                           num_layers=1, dropout=0.0, bidirectional=False, model_name="rnn_regressor"):
        """–°–æ–∑–¥–∞–Ω–∏–µ RNN —Ä–µ–≥—Ä–µ—Å—Å–æ—Ä–∞"""
        try:
            model = RubinRNNRegressor(
                input_size=input_size,
                hidden_size=hidden_size,
                output_size=output_size,
                rnn_type=rnn_type,
                num_layers=num_layers,
                dropout=dropout,
                bidirectional=bidirectional
            )
            model = model.to(self.device)
            self.models[model_name] = model
            
            logger.info(f"‚úÖ RNN —Ä–µ–≥—Ä–µ—Å—Å–æ—Ä —Å–æ–∑–¥–∞–Ω: {model_name}")
            logger.info(f"   –¢–∏–ø RNN: {rnn_type.upper()}")
            logger.info(f"   –í—Ö–æ–¥–Ω–æ–π —Ä–∞–∑–º–µ—Ä: {input_size}")
            logger.info(f"   –°–∫—Ä—ã—Ç—ã–π —Ä–∞–∑–º–µ—Ä: {hidden_size}")
            logger.info(f"   –í—ã—Ö–æ–¥–Ω–æ–π —Ä–∞–∑–º–µ—Ä: {output_size}")
            logger.info(f"   –°–ª–æ–∏: {num_layers}")
            logger.info(f"   –î–≤—É–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π: {bidirectional}")
            logger.info(f"   –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: {sum(p.numel() for p in model.parameters()):,}")
            
            return model
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è RNN —Ä–µ–≥—Ä–µ—Å—Å–æ—Ä–∞: {e}")
            return None
    
    def train_model(self, model_name, train_data, epochs=10, learning_rate=0.001, task_type="classification"):
        """–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"""
        if model_name not in self.models:
            logger.error(f"‚ùå –ú–æ–¥–µ–ª—å {model_name} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
            return False
        
        model = self.models[model_name]
        optimizer = optim.Adam(model.parameters(), lr=learning_rate)
        
        if task_type == "classification":
            criterion = nn.CrossEntropyLoss()
        else:
            criterion = nn.MSELoss()
        
        model.train()
        training_losses = []
        
        logger.info(f"üöÄ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ {model_name}")
        logger.info(f"   –¢–∏–ø –∑–∞–¥–∞—á–∏: {task_type}")
        logger.info(f"   –≠–ø–æ—Ö–∏: {epochs}")
        logger.info(f"   –°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è: {learning_rate}")
        
        for epoch in range(epochs):
            epoch_loss = 0.0
            num_batches = 0
            
            # –ó–¥–µ—Å—å –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –ª–æ–≥–∏–∫–∞ –æ–±—É—á–µ–Ω–∏—è —Å –¥–∞–Ω–Ω—ã–º–∏
            # –î–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ —Å–æ–∑–¥–∞–µ–º —Ñ–∏–∫—Ç–∏–≤–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            if ML_AVAILABLE and train_data is not None:
                # –†–µ–∞–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ
                for batch_idx, (data, target) in enumerate(train_data):
                    data, target = data.to(self.device), target.to(self.device)
                    
                    optimizer.zero_grad()
                    output = model(data)
                    loss = criterion(output, target)
                    loss.backward()
                    optimizer.step()
                    
                    epoch_loss += loss.item()
                    num_batches += 1
            else:
                # Mock –æ–±—É—á–µ–Ω–∏–µ
                epoch_loss = np.random.uniform(0.1, 1.0)
                num_batches = 10
            
            avg_loss = epoch_loss / max(num_batches, 1)
            training_losses.append(avg_loss)
            
            if epoch % 5 == 0:
                logger.info(f"   –≠–ø–æ—Ö–∞ {epoch+1}/{epochs}, –ü–æ—Ç–µ—Ä—è: {avg_loss:.4f}")
        
        self.training_history[model_name] = training_losses
        logger.info(f"‚úÖ –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ {model_name} –∑–∞–≤–µ—Ä—à–µ–Ω–æ")
        
        return True
    
    def predict(self, model_name, data):
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å –ø–æ–º–æ—â—å—é –º–æ–¥–µ–ª–∏"""
        if model_name not in self.models:
            logger.error(f"‚ùå –ú–æ–¥–µ–ª—å {model_name} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
            return None
        
        model = self.models[model_name]
        model.eval()
        
        with torch.no_grad():
            if ML_AVAILABLE:
                data = data.to(self.device)
                output = model(data)
                
                if isinstance(model, RubinRNNClassifier):
                    predictions = torch.softmax(output, dim=1)
                    return predictions.cpu().numpy()
                else:
                    return output.cpu().numpy()
            else:
                # Mock –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
                batch_size = data.shape[0] if hasattr(data, 'shape') else 1
                if isinstance(model, RubinRNNClassifier):
                    num_classes = model.num_classes
                    return np.random.rand(batch_size, num_classes)
                else:
                    output_size = model.output_size
                    return np.random.rand(batch_size, output_size)
    
    def get_model_info(self, model_name):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –º–æ–¥–µ–ª–∏"""
        if model_name not in self.models:
            return None
        
        model = self.models[model_name]
        
        info = {
            "name": model_name,
            "type": "RNN Classifier" if isinstance(model, RubinRNNClassifier) else "RNN Regressor",
            "rnn_type": model.rnn_type,
            "input_size": model.input_size,
            "hidden_size": model.hidden_size,
            "num_layers": model.num_layers,
            "bidirectional": model.bidirectional,
            "dropout": model.dropout,
            "parameters": sum(p.numel() for p in model.parameters()),
            "device": str(self.device),
            "training_history": self.training_history.get(model_name, [])
        }
        
        if isinstance(model, RubinRNNClassifier):
            info["num_classes"] = model.num_classes
        else:
            info["output_size"] = model.output_size
        
        return info
    
    def get_all_models_info(self):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –≤—Å–µ—Ö –º–æ–¥–µ–ª—è—Ö"""
        return {name: self.get_model_info(name) for name in self.models.keys()}
    
    def save_model(self, model_name, filepath):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"""
        if model_name not in self.models:
            logger.error(f"‚ùå –ú–æ–¥–µ–ª—å {model_name} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
            return False
        
        try:
            if ML_AVAILABLE:
                torch.save(self.models[model_name].state_dict(), filepath)
            else:
                # Mock —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
                with open(filepath, 'w') as f:
                    f.write(f"Mock RNN model: {model_name}")
            
            logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å {model_name} —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {filepath}")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–∏: {e}")
            return False
    
    def load_model(self, model_name, filepath, input_size, hidden_size, num_classes=None, output_size=None, 
                   rnn_type="lstm", num_layers=1, dropout=0.0, bidirectional=False):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏"""
        try:
            if ML_AVAILABLE:
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –º–æ–¥–µ–ª–∏ –ø–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º
                if num_classes is not None:
                    model = RubinRNNClassifier(
                        input_size, hidden_size, num_classes, rnn_type, 
                        num_layers, dropout, bidirectional
                    )
                elif output_size is not None:
                    model = RubinRNNRegressor(
                        input_size, hidden_size, output_size, rnn_type, 
                        num_layers, dropout, bidirectional
                    )
                else:
                    raise ValueError("–ù–µ–æ–±—Ö–æ–¥–∏–º–æ —É–∫–∞–∑–∞—Ç—å num_classes –∏–ª–∏ output_size")
                
                model.load_state_dict(torch.load(filepath))
                model = model.to(self.device)
                self.models[model_name] = model
            else:
                # Mock –∑–∞–≥—Ä—É–∑–∫–∞
                if num_classes is not None:
                    model = RubinRNNClassifier(input_size, hidden_size, num_classes, rnn_type)
                else:
                    model = RubinRNNRegressor(input_size, hidden_size, output_size, rnn_type)
                self.models[model_name] = model
            
            logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å {model_name} –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ {filepath}")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
            return False

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä RNN Manager
_rnn_manager = None

def get_rnn_manager():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ —ç–∫–∑–µ–º–ø–ª—è—Ä–∞ RNN Manager"""
    global _rnn_manager
    if _rnn_manager is None:
        _rnn_manager = RubinRNNManager()
    return _rnn_manager

if __name__ == "__main__":
    print("üß† –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï RNN –î–õ–Ø RUBIN AI")
    print("=" * 50)
    
    # –°–æ–∑–¥–∞–µ–º –º–µ–Ω–µ–¥–∂–µ—Ä RNN
    rnn_manager = get_rnn_manager()
    
    # –¢–µ—Å—Ç —Å–æ–∑–¥–∞–Ω–∏—è RNN –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞
    print("\nüìä –¢–µ—Å—Ç —Å–æ–∑–¥–∞–Ω–∏—è RNN –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞:")
    rnn_classifier = rnn_manager.create_rnn_classifier(
        input_size=10,
        hidden_size=64,
        num_classes=5,
        rnn_type="lstm",
        num_layers=2,
        dropout=0.2,
        bidirectional=True,
        model_name="sequence_classifier"
    )
    
    if rnn_classifier:
        info = rnn_manager.get_model_info("sequence_classifier")
        print(f"‚úÖ RNN –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Å–æ–∑–¥–∞–Ω —É—Å–ø–µ—à–Ω–æ")
        print(f"   –¢–∏–ø RNN: {info['rnn_type']}")
        print(f"   –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: {info['parameters']:,}")
        print(f"   –°–ª–æ–∏: {info['num_layers']}")
        print(f"   –î–≤—É–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π: {info['bidirectional']}")
    
    # –¢–µ—Å—Ç —Å–æ–∑–¥–∞–Ω–∏—è RNN —Ä–µ–≥—Ä–µ—Å—Å–æ—Ä–∞
    print("\nüìä –¢–µ—Å—Ç —Å–æ–∑–¥–∞–Ω–∏—è RNN —Ä–µ–≥—Ä–µ—Å—Å–æ—Ä–∞:")
    rnn_regressor = rnn_manager.create_rnn_regressor(
        input_size=5,
        hidden_size=32,
        output_size=1,
        rnn_type="gru",
        num_layers=1,
        dropout=0.1,
        bidirectional=False,
        model_name="time_series_predictor"
    )
    
    if rnn_regressor:
        info = rnn_manager.get_model_info("time_series_predictor")
        print(f"‚úÖ RNN —Ä–µ–≥—Ä–µ—Å—Å–æ—Ä —Å–æ–∑–¥–∞–Ω —É—Å–ø–µ—à–Ω–æ")
        print(f"   –¢–∏–ø RNN: {info['rnn_type']}")
        print(f"   –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: {info['parameters']:,}")
        print(f"   –°–ª–æ–∏: {info['num_layers']}")
        print(f"   –î–≤—É–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π: {info['bidirectional']}")
    
    # –¢–µ—Å—Ç –æ–±—É—á–µ–Ω–∏—è
    print("\nüöÄ –¢–µ—Å—Ç –æ–±—É—á–µ–Ω–∏—è:")
    if rnn_classifier:
        success = rnn_manager.train_model("sequence_classifier", None, epochs=5, task_type="classification")
        if success:
            print("‚úÖ –û–±—É—á–µ–Ω–∏–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ")
    
    if rnn_regressor:
        success = rnn_manager.train_model("time_series_predictor", None, epochs=3, task_type="regression")
        if success:
            print("‚úÖ –û–±—É—á–µ–Ω–∏–µ —Ä–µ–≥—Ä–µ—Å—Å–æ—Ä–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ")
    
    # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –≤—Å–µ—Ö –º–æ–¥–µ–ª—è—Ö
    print("\nüìã –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –≤—Å–µ—Ö –º–æ–¥–µ–ª—è—Ö:")
    all_info = rnn_manager.get_all_models_info()
    for name, info in all_info.items():
        print(f"  {name}: {info['type']} - {info['parameters']:,} –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤")
    
    print("\n‚úÖ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ RNN –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")










