#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
–£–ª—É—á—à–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä PLC –∫–æ–¥–∞ –ø–æ –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏–∏:
1. –õ–µ–∫—Å–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ (—Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è)
2. –°–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ (–ø–∞—Ä—Å–∏–Ω–≥)
3. –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ (–ø—Ä–æ–≤–µ—Ä–∫–∞ —Å–º—ã—Å–ª–∞)
4. AI-–∞–Ω–∞–ª–∏–∑ (—ç–∫—Å–ø–µ—Ä—Ç–Ω–∞—è –æ—Ü–µ–Ω–∫–∞)
"""

import re
import json
from typing import List, Dict, Any, Tuple

class PLCTokenizer:
    """–®–∞–≥ 1: –õ–µ–∫—Å–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ - —Ä–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ —Ç–æ–∫–µ–Ω—ã"""
    
    def __init__(self):
        # –†–µ–≥—É–ª—è—Ä–Ω—ã–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è —Ç–æ–∫–µ–Ω–æ–≤ PLC
        self.token_patterns = [
            ('PROGRAM', r'\bPROGRAM\b'),
            ('FUNCTION', r'\bFUNCTION\b'),
            ('VAR', r'\bVAR\b'),
            ('END_VAR', r'\bEND_VAR\b'),
            ('END_PROGRAM', r'\bEND_PROGRAM\b'),
            ('END_FUNCTION', r'\bEND_FUNCTION\b'),
            ('IF', r'\bIF\b'),
            ('THEN', r'\bTHEN\b'),
            ('ELSE', r'\bELSE\b'),
            ('END_IF', r'\bEND_IF\b'),
            ('WHILE', r'\bWHILE\b'),
            ('DO', r'\bDO\b'),
            ('END_WHILE', r'\bEND_WHILE\b'),
            ('FOR', r'\bFOR\b'),
            ('TO', r'\bTO\b'),
            ('BY', r'\bBY\b'),
            ('END_FOR', r'\bEND_FOR\b'),
            ('AND', r'\bAND\b'),
            ('OR', r'\bOR\b'),
            ('NOT', r'\bNOT\b'),
            ('XOR', r'\bXOR\b'),
            ('BOOL', r'\bBOOL\b'),
            ('INT', r'\bINT\b'),
            ('DINT', r'\bDINT\b'),
            ('REAL', r'\bREAL\b'),
            ('STRING', r'\bSTRING\b'),
            ('TRUE', r'\bTRUE\b'),
            ('FALSE', r'\bFALSE\b'),
            ('ASSIGN', r':='),
            ('EQUALS', r'='),
            ('PLUS', r'\+'),
            ('MINUS', r'-'),
            ('MULTIPLY', r'\*'),
            ('DIVIDE', r'/'),
            ('MODULO', r'MOD'),
            ('GREATER', r'>'),
            ('LESS', r'<'),
            ('GREATER_EQUAL', r'>='),
            ('LESS_EQUAL', r'<='),
            ('NOT_EQUAL', r'<>'),
            ('LPAREN', r'\('),
            ('RPAREN', r'\)'),
            ('SEMICOLON', r';'),
            ('COLON', r':'),
            ('COMMA', r','),
            ('DOT', r'\.'),
            ('COMMENT', r'//.*'),
            ('COMMENT_BLOCK', r'/\*.*?\*/'),
            ('STRING_LITERAL', r'"[^"]*"'),
            ('NUMBER', r'\b\d+(\.\d+)?\b'),
            ('IO_ADDRESS', r'\b[IQM]\d+\.\d+\b'),
            ('DB_ADDRESS', r'\bDB\d+\.DB[XY]\d+\.\d+\b'),
            ('IDENTIFIER', r'\b[A-Za-z_][A-Za-z0-9_]*\b'),
            ('WHITESPACE', r'\s+'),
        ]
    
    def tokenize(self, code: str) -> List[Tuple[str, str]]:
        """–†–∞–∑–±–∏–≤–∞–µ—Ç –∫–æ–¥ –Ω–∞ —Ç–æ–∫–µ–Ω—ã"""
        tokens = []
        position = 0
        
        while position < len(code):
            matched = False
            
            for token_type, pattern in self.token_patterns:
                regex = re.compile(pattern, re.IGNORECASE)
                match = regex.match(code, position)
                
                if match:
                    token_value = match.group(0)
                    if token_type != 'WHITESPACE':  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—Ä–æ–±–µ–ª—ã
                        tokens.append((token_type, token_value))
                    position = match.end()
                    matched = True
                    break
            
            if not matched:
                # –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Å–∏–º–≤–æ–ª
                tokens.append(('UNKNOWN', code[position]))
                position += 1
        
        return tokens

class PLCParser:
    """–®–∞–≥ 2: –°–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ - –ø—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã"""
    
    def __init__(self):
        self.errors = []
        self.warnings = []
    
    def parse(self, tokens: List[Tuple[str, str]]) -> Dict[str, Any]:
        """–ü–∞—Ä—Å–∏—Ç —Ç–æ–∫–µ–Ω—ã –∏ –ø—Ä–æ–≤–µ—Ä—è–µ—Ç —Å–∏–Ω—Ç–∞–∫—Å–∏—Å"""
        self.errors = []
        self.warnings = []
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏
        self._check_program_structure(tokens)
        self._check_control_structures(tokens)
        self._check_brackets_balance(tokens)
        self._check_variable_declarations(tokens)
        
        return {
            'errors': self.errors,
            'warnings': self.warnings,
            'syntax_valid': len(self.errors) == 0
        }
    
    def _check_program_structure(self, tokens: List[Tuple[str, str]]):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É –ø—Ä–æ–≥—Ä–∞–º–º—ã"""
        program_count = sum(1 for token_type, _ in tokens if token_type == 'PROGRAM')
        end_program_count = sum(1 for token_type, _ in tokens if token_type == 'END_PROGRAM')
        
        if program_count != end_program_count:
            self.errors.append(f"–ù–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ PROGRAM/END_PROGRAM: {program_count} vs {end_program_count}")
    
    def _check_control_structures(self, tokens: List[Tuple[str, str]]):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —É–ø—Ä–∞–≤–ª—è—é—â–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã"""
        if_count = sum(1 for token_type, _ in tokens if token_type == 'IF')
        endif_count = sum(1 for token_type, _ in tokens if token_type == 'END_IF')
        
        if if_count != endif_count:
            self.errors.append(f"–ù–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ IF/END_IF: {if_count} vs {endif_count}")
        
        while_count = sum(1 for token_type, _ in tokens if token_type == 'WHILE')
        endwhile_count = sum(1 for token_type, _ in tokens if token_type == 'END_WHILE')
        
        if while_count != endwhile_count:
            self.errors.append(f"–ù–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ WHILE/END_WHILE: {while_count} vs {endwhile_count}")
    
    def _check_brackets_balance(self, tokens: List[Tuple[str, str]]):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –±–∞–ª–∞–Ω—Å —Å–∫–æ–±–æ–∫"""
        lparen_count = sum(1 for token_type, _ in tokens if token_type == 'LPAREN')
        rparen_count = sum(1 for token_type, _ in tokens if token_type == 'RPAREN')
        
        if lparen_count != rparen_count:
            self.errors.append(f"–ù–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å–∫–æ–±–∫–∏: {lparen_count} vs {rparen_count}")
    
    def _check_variable_declarations(self, tokens: List[Tuple[str, str]]):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –æ–±—ä—è–≤–ª–µ–Ω–∏—è –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö"""
        var_count = sum(1 for token_type, _ in tokens if token_type == 'VAR')
        end_var_count = sum(1 for token_type, _ in tokens if token_type == 'END_VAR')
        
        if var_count != end_var_count:
            self.errors.append(f"–ù–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ VAR/END_VAR: {var_count} vs {end_var_count}")

class PLCSemanticAnalyzer:
    """–®–∞–≥ 3: –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ - –ø—Ä–æ–≤–µ—Ä–∫–∞ —Å–º—ã—Å–ª–∞"""
    
    def __init__(self):
        self.variables = {}
        self.functions = {}
        self.errors = []
        self.warnings = []
    
    def analyze(self, tokens: List[Tuple[str, str]]) -> Dict[str, Any]:
        """–í—ã–ø–æ–ª–Ω—è–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑"""
        self.variables = {}
        self.functions = {}
        self.errors = []
        self.warnings = []
        
        self._extract_variables(tokens)
        self._extract_functions(tokens)
        self._check_variable_usage(tokens)
        self._check_function_calls(tokens)
        
        return {
            'variables': self.variables,
            'functions': self.functions,
            'errors': self.errors,
            'warnings': self.warnings,
            'semantic_valid': len(self.errors) == 0
        }
    
    def _extract_variables(self, tokens: List[Tuple[str, str]]):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –æ–±—ä—è–≤–ª–µ–Ω–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ"""
        in_var_block = False
        
        for i, (token_type, token_value) in enumerate(tokens):
            if token_type == 'VAR':
                in_var_block = True
            elif token_type == 'END_VAR':
                in_var_block = False
            elif in_var_block and token_type == 'IDENTIFIER':
                # –ò—â–µ–º —Ç–∏–ø –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
                if i + 2 < len(tokens) and tokens[i + 1][0] == 'COLON':
                    var_type = tokens[i + 2][1]
                    self.variables[token_value] = {
                        'type': var_type,
                        'declared': True,
                        'used': False
                    }
    
    def _extract_functions(self, tokens: List[Tuple[str, str]]):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –æ–±—ä—è–≤–ª–µ–Ω–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏"""
        for i, (token_type, token_value) in enumerate(tokens):
            if token_type == 'FUNCTION' and i + 1 < len(tokens):
                func_name = tokens[i + 1][1]
                self.functions[func_name] = {
                    'declared': True,
                    'called': False
                }
    
    def _check_variable_usage(self, tokens: List[Tuple[str, str]]):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö"""
        for token_type, token_value in tokens:
            if token_type == 'IDENTIFIER' and token_value in self.variables:
                self.variables[token_value]['used'] = True
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
        for var_name, var_info in self.variables.items():
            if not var_info['used']:
                self.warnings.append(f"–ù–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è: {var_name}")
    
    def _check_function_calls(self, tokens: List[Tuple[str, str]]):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –≤—ã–∑–æ–≤—ã —Ñ—É–Ω–∫—Ü–∏–π"""
        for token_type, token_value in tokens:
            if token_type == 'IDENTIFIER' and token_value in self.functions:
                self.functions[token_value]['called'] = True

class PLCAIAnalyzer:
    """–®–∞–≥ 4: AI-–∞–Ω–∞–ª–∏–∑ - —ç–∫—Å–ø–µ—Ä—Ç–Ω–∞—è –æ—Ü–µ–Ω–∫–∞"""
    
    def __init__(self):
        self.recommendations = []
        self.quality_score = 0
    
    def analyze(self, code: str, tokens: List[Tuple[str, str]], 
                syntax_result: Dict, semantic_result: Dict) -> Dict[str, Any]:
        """–í—ã–ø–æ–ª–Ω—è–µ—Ç AI-–∞–Ω–∞–ª–∏–∑ –∫–æ–¥–∞"""
        self.recommendations = []
        self.quality_score = 0
        
        self._analyze_code_quality(code)
        self._analyze_complexity(tokens)
        self._analyze_best_practices(code)
        self._analyze_safety_issues(code)
        
        return {
            'recommendations': self.recommendations,
            'quality_score': self.quality_score,
            'complexity_level': self._get_complexity_level(),
            'safety_issues': self._get_safety_issues(code)
        }
    
    def _analyze_code_quality(self, code: str):
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –∫–æ–¥–∞"""
        lines = code.split('\n')
        comment_lines = sum(1 for line in lines if line.strip().startswith('//'))
        total_lines = len([line for line in lines if line.strip()])
        
        comment_ratio = comment_lines / total_lines if total_lines > 0 else 0
        
        if comment_ratio < 0.1:
            self.recommendations.append("–î–æ–±–∞–≤—å—Ç–µ –±–æ–ª—å—à–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è >10%)")
            self.quality_score -= 2
        elif comment_ratio > 0.3:
            self.recommendations.append("–°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤, –≤–æ–∑–º–æ–∂–Ω–æ –∏–∑–±—ã—Ç–æ—á–Ω–æ—Å—Ç—å")
            self.quality_score -= 1
        else:
            self.quality_score += 2
    
    def _analyze_complexity(self, tokens: List[Tuple[str, str]]):
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å–ª–æ–∂–Ω–æ—Å—Ç—å –∫–æ–¥–∞"""
        if_count = sum(1 for token_type, _ in tokens if token_type == 'IF')
        while_count = sum(1 for token_type, _ in tokens if token_type == 'WHILE')
        for_count = sum(1 for token_type, _ in tokens if token_type == 'FOR')
        
        total_control_structures = if_count + while_count + for_count
        
        if total_control_structures > 20:
            self.recommendations.append("–í—ã—Å–æ–∫–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å - —Ä–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ —Ñ—É–Ω–∫—Ü–∏–∏")
            self.quality_score -= 3
        elif total_control_structures > 10:
            self.recommendations.append("–°—Ä–µ–¥–Ω—è—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å - –ø—Ä–æ–≤–µ—Ä—å—Ç–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å —É–ø—Ä–æ—â–µ–Ω–∏—è")
            self.quality_score -= 1
        else:
            self.quality_score += 1
    
    def _analyze_best_practices(self, code: str):
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å–æ–±–ª—é–¥–µ–Ω–∏–µ –ª—É—á—à–∏—Ö –ø—Ä–∞–∫—Ç–∏–∫"""
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –º–∞–≥–∏—á–µ—Å–∫–∏—Ö —á–∏—Å–µ–ª
        numbers = re.findall(r'\b\d+\b', code)
        magic_numbers = [n for n in numbers if int(n) > 10 and n not in ['100', '1000']]
        
        if len(magic_numbers) > 5:
            self.recommendations.append("–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã–µ –∫–æ–Ω—Å—Ç–∞–Ω—Ç—ã –≤–º–µ—Å—Ç–æ –º–∞–≥–∏—á–µ—Å–∫–∏—Ö —á–∏—Å–µ–ª")
            self.quality_score -= 2
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–ª–∏–Ω–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏
        lines = code.split('\n')
        long_lines = [i for i, line in enumerate(lines) if len(line) > 80]
        
        if len(long_lines) > len(lines) * 0.2:
            self.recommendations.append("–°–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏ - —É–ª—É—á—à–∏—Ç–µ —á–∏—Ç–∞–µ–º–æ—Å—Ç—å")
            self.quality_score -= 1
    
    def _analyze_safety_issues(self, code: str):
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏"""
        safety_issues = []
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –æ—à–∏–±–∫–∏
        if 'ERROR' in code.upper() and 'HANDLING' not in code.upper():
            safety_issues.append("–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã ERROR –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –±–µ–∑ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ—à–∏–±–æ–∫")
        
        if 'STOP' in code.upper() and 'SAFETY' not in code.upper():
            safety_issues.append("–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã STOP –∫–æ–º–∞–Ω–¥—ã –±–µ–∑ –ø—Ä–æ–≤–µ—Ä–∫–∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏")
        
        if safety_issues:
            self.recommendations.extend(safety_issues)
            self.quality_score -= 3
        
        return safety_issues
    
    def _get_complexity_level(self) -> str:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —É—Ä–æ–≤–µ–Ω—å —Å–ª–æ–∂–Ω–æ—Å—Ç–∏"""
        if self.quality_score >= 8:
            return "–ù–∏–∑–∫–∞—è"
        elif self.quality_score >= 5:
            return "–°—Ä–µ–¥–Ω—è—è"
        else:
            return "–í—ã—Å–æ–∫–∞—è"
    
    def _get_safety_issues(self, code: str) -> List[str]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –ø—Ä–æ–±–ª–µ–º –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏"""
        issues = []
        
        if 'ERROR' in code.upper():
            issues.append("–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã ERROR –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ")
        
        if 'STOP' in code.upper():
            issues.append("–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã STOP –∫–æ–º–∞–Ω–¥—ã")
        
        if 'FAULT' in code.upper():
            issues.append("–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã FAULT –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ")
        
        return issues

def analyze_plc_code(code: str) -> Dict[str, Any]:
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞ PLC –∫–æ–¥–∞"""
    print("üîç –ù–∞—á–∏–Ω–∞–µ–º –∞–Ω–∞–ª–∏–∑ PLC –∫–æ–¥–∞...")
    
    # –®–∞–≥ 1: –õ–µ–∫—Å–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑
    print("üìù –®–∞–≥ 1: –õ–µ–∫—Å–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ (—Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è)...")
    tokenizer = PLCTokenizer()
    tokens = tokenizer.tokenize(code)
    print(f"   –ù–∞–π–¥–µ–Ω–æ —Ç–æ–∫–µ–Ω–æ–≤: {len(tokens)}")
    
    # –®–∞–≥ 2: –°–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑
    print("üîß –®–∞–≥ 2: –°–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ (–ø–∞—Ä—Å–∏–Ω–≥)...")
    parser = PLCParser()
    syntax_result = parser.parse(tokens)
    print(f"   –°–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–∏—Ö –æ—à–∏–±–æ–∫: {len(syntax_result['errors'])}")
    print(f"   –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–π: {len(syntax_result['warnings'])}")
    
    # –®–∞–≥ 3: –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑
    print("üß† –®–∞–≥ 3: –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ (–ø—Ä–æ–≤–µ—Ä–∫–∞ —Å–º—ã—Å–ª–∞)...")
    semantic_analyzer = PLCSemanticAnalyzer()
    semantic_result = semantic_analyzer.analyze(tokens)
    print(f"   –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –æ—à–∏–±–æ–∫: {len(semantic_result['errors'])}")
    print(f"   –ü–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö: {len(semantic_result['variables'])}")
    print(f"   –§—É–Ω–∫—Ü–∏–π: {len(semantic_result['functions'])}")
    
    # –®–∞–≥ 4: AI-–∞–Ω–∞–ª–∏–∑
    print("ü§ñ –®–∞–≥ 4: AI-–∞–Ω–∞–ª–∏–∑ (—ç–∫—Å–ø–µ—Ä—Ç–Ω–∞—è –æ—Ü–µ–Ω–∫–∞)...")
    ai_analyzer = PLCAIAnalyzer()
    ai_result = ai_analyzer.analyze(code, tokens, syntax_result, semantic_result)
    print(f"   –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞: {ai_result['quality_score']}/10")
    print(f"   –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π: {len(ai_result['recommendations'])}")
    
    return {
        'tokens': tokens,
        'syntax': syntax_result,
        'semantic': semantic_result,
        'ai_analysis': ai_result,
        'summary': {
            'total_tokens': len(tokens),
            'syntax_errors': len(syntax_result['errors']),
            'semantic_errors': len(semantic_result['errors']),
            'variables_count': len(semantic_result['variables']),
            'functions_count': len(semantic_result['functions']),
            'quality_score': ai_result['quality_score'],
            'complexity_level': ai_result['complexity_level']
        }
    }

def main():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞"""
    print("üöÄ –£–ª—É—á—à–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä PLC –∫–æ–¥–∞")
    print("=" * 50)
    
    # –ß–∏—Ç–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª
    try:
        with open('test_simple.plc', 'r', encoding='utf-8') as f:
            code = f.read()
    except FileNotFoundError:
        print("‚ùå –§–∞–π–ª test_simple.plc –Ω–µ –Ω–∞–π–¥–µ–Ω")
        return
    
    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–æ–¥
    result = analyze_plc_code(code)
    
    print("\nüìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ê–ù–ê–õ–ò–ó–ê:")
    print("=" * 50)
    
    # –í—ã–≤–æ–¥–∏–º —Å–≤–æ–¥–∫—É
    summary = result['summary']
    print(f"üìà –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
    print(f"   ‚Ä¢ –¢–æ–∫–µ–Ω–æ–≤: {summary['total_tokens']}")
    print(f"   ‚Ä¢ –°–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–∏—Ö –æ—à–∏–±–æ–∫: {summary['syntax_errors']}")
    print(f"   ‚Ä¢ –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –æ—à–∏–±–æ–∫: {summary['semantic_errors']}")
    print(f"   ‚Ä¢ –ü–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö: {summary['variables_count']}")
    print(f"   ‚Ä¢ –§—É–Ω–∫—Ü–∏–π: {summary['functions_count']}")
    print(f"   ‚Ä¢ –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞: {summary['quality_score']}/10")
    print(f"   ‚Ä¢ –£—Ä–æ–≤–µ–Ω—å —Å–ª–æ–∂–Ω–æ—Å—Ç–∏: {summary['complexity_level']}")
    
    # –í—ã–≤–æ–¥–∏–º –æ—à–∏–±–∫–∏
    if result['syntax']['errors']:
        print(f"\n‚ùå –°–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–∏–µ –æ—à–∏–±–∫–∏:")
        for error in result['syntax']['errors']:
            print(f"   ‚Ä¢ {error}")
    
    if result['semantic']['errors']:
        print(f"\n‚ùå –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –æ—à–∏–±–∫–∏:")
        for error in result['semantic']['errors']:
            print(f"   ‚Ä¢ {error}")
    
    # –í—ã–≤–æ–¥–∏–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
    if result['ai_analysis']['recommendations']:
        print(f"\nüí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:")
        for rec in result['ai_analysis']['recommendations']:
            print(f"   ‚Ä¢ {rec}")
    
    # –í—ã–≤–æ–¥–∏–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
    if result['semantic']['variables']:
        print(f"\nüìã –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ:")
        for var_name, var_info in result['semantic']['variables'].items():
            print(f"   ‚Ä¢ {var_name}: {var_info['type']}")
    
    print(f"\n‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω!")

if __name__ == "__main__":
    main()
