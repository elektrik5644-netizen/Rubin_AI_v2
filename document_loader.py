#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–ó–∞–≥—Ä—É–∑—á–∏–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö Rubin AI v2.0
"""

import os
import sys
import sqlite3
import hashlib
import mimetypes
from pathlib import Path
import logging
from datetime import datetime
import json

class DocumentLoader:
    def __init__(self, db_path="rubin_ai_documents.db"):
        self.db_path = db_path
        self.setup_logging()
        self.setup_database()
        
    def setup_logging(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('document_loader.log', encoding='utf-8'),
                logging.StreamHandler(sys.stdout)
            ]
        )
        self.logger = logging.getLogger(__name__)
        
    def setup_database(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # –¢–∞–±–ª–∏—Ü–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS documents (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    file_path TEXT UNIQUE NOT NULL,
                    file_name TEXT NOT NULL,
                    file_size INTEGER,
                    file_type TEXT,
                    file_hash TEXT,
                    content TEXT,
                    metadata TEXT,
                    category TEXT,
                    tags TEXT,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            # –¢–∞–±–ª–∏—Ü–∞ –∏–Ω–¥–µ–∫—Å–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS document_index (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    document_id INTEGER,
                    keyword TEXT,
                    position INTEGER,
                    context TEXT,
                    FOREIGN KEY (document_id) REFERENCES documents (id)
                )
            ''')
            
            # –¢–∞–±–ª–∏—Ü–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–π
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS categories (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    name TEXT UNIQUE NOT NULL,
                    description TEXT,
                    parent_id INTEGER,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            conn.commit()
            conn.close()
            
            self.logger.info("‚úÖ –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö: {e}")
            
    def get_file_hash(self, file_path):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ö–µ—à–∞ —Ñ–∞–π–ª–∞"""
        try:
            hash_md5 = hashlib.md5()
            with open(file_path, "rb") as f:
                for chunk in iter(lambda: f.read(4096), b""):
                    hash_md5.update(chunk)
            return hash_md5.hexdigest()
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Ö–µ—à–∞ —Ñ–∞–π–ª–∞ {file_path}: {e}")
            return None
            
    def extract_text_content(self, file_path):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –∏–∑ —Ñ–∞–π–ª–∞"""
        try:
            file_type = mimetypes.guess_type(file_path)[0]
            
            if file_type is None:
                # –ü–æ–ø—Ä–æ–±—É–µ–º –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –ø–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—é
                ext = Path(file_path).suffix.lower()
                if ext in ['.txt', '.md', '.rst']:
                    file_type = 'text/plain'
                elif ext in ['.pdf']:
                    file_type = 'application/pdf'
                elif ext in ['.doc', '.docx']:
                    file_type = 'application/msword'
                    
            if file_type and file_type.startswith('text/'):
                # –¢–µ–∫—Å—Ç–æ–≤—ã–µ —Ñ–∞–π–ª—ã
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    return f.read()
                    
            elif file_type == 'application/pdf':
                # PDF —Ñ–∞–π–ª—ã
                try:
                    import PyPDF2
                    with open(file_path, 'rb') as f:
                        reader = PyPDF2.PdfReader(f)
                        text = ""
                        for page in reader.pages:
                            text += page.extract_text() + "\n"
                        return text
                except ImportError:
                    self.logger.warning("PyPDF2 –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º PDF —Ñ–∞–π–ª")
                    return None
                    
            elif file_type in ['application/msword', 'application/vnd.openxmlformats-officedocument.wordprocessingml.document']:
                # Word –¥–æ–∫—É–º–µ–Ω—Ç—ã
                try:
                    import docx
                    doc = docx.Document(file_path)
                    text = ""
                    for paragraph in doc.paragraphs:
                        text += paragraph.text + "\n"
                    return text
                except ImportError:
                    self.logger.warning("python-docx –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º Word –¥–æ–∫—É–º–µ–Ω—Ç")
                    return None
                    
            else:
                self.logger.warning(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ç–∏–ø —Ñ–∞–π–ª–∞: {file_type}")
                return None
                
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –∏–∑ {file_path}: {e}")
            return None
            
    def categorize_document(self, file_path, content):
        """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        file_name = Path(file_path).name.lower()
        content_lower = content.lower() if content else ""
        
        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏–∏
        categories = {
            '–≠–ª–µ–∫—Ç—Ä–æ—Ç–µ—Ö–Ω–∏–∫–∞': ['—ç–ª–µ–∫—Ç—Ä–∏—á–µ—Å—Ç–≤–æ', '—Ç–æ–∫', '–Ω–∞–ø—Ä—è–∂–µ–Ω–∏–µ', '—Å—Ö–µ–º–∞', '—Ä–µ–∑–∏—Å—Ç–æ—Ä', '–∫–æ–Ω–¥–µ–Ω—Å–∞—Ç–æ—Ä', '–∏–Ω–¥—É–∫—Ç–∏–≤–Ω–æ—Å—Ç—å', '–º–æ—â–Ω–æ—Å—Ç—å', '–∑–∞–∫–æ–Ω –æ–º–∞', '–∫–∏—Ä—Ö–≥–æ—Ñ'],
            '–ü—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ': ['–ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ', '–∫–æ–¥', '–∞–ª–≥–æ—Ä–∏—Ç–º', '—Ñ—É–Ω–∫—Ü–∏—è', '–ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è', '—Ü–∏–∫–ª', '—É—Å–ª–æ–≤–∏–µ', 'python', 'java', 'c++', 'javascript'],
            '–ê–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è': ['–∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è', 'plc', 'scada', '–∫–æ–Ω—Ç—Ä–æ–ª–ª–µ—Ä', '–¥–∞—Ç—á–∏–∫', '–∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π', '—Ä–µ–≥—É–ª–∏—Ä–æ–≤–∞–Ω–∏–µ', '–ø—Ä–æ–º—ã—à–ª–µ–Ω–Ω–æ—Å—Ç—å'],
            '–†–∞–¥–∏–æ—Ç–µ—Ö–Ω–∏–∫–∞': ['—Ä–∞–¥–∏–æ', '–∞–Ω—Ç–µ–Ω–Ω–∞', '—á–∞—Å—Ç–æ—Ç–∞', '–º–æ–¥—É–ª—è—Ü–∏—è', '–ø–µ—Ä–µ–¥–∞—Ç—á–∏–∫', '–ø—Ä–∏–µ–º–Ω–∏–∫', '—Å–∏–≥–Ω–∞–ª', '–≤–æ–ª–Ω–∞'],
            '–ú–µ—Ö–∞–Ω–∏–∫–∞': ['–º–µ—Ö–∞–Ω–∏–∫–∞', '–¥–≤–∏–∂–µ–Ω–∏–µ', '—Å–∏–ª–∞', '–º–∞—Å—Å–∞', '—Å–∫–æ—Ä–æ—Å—Ç—å', '—É—Å–∫–æ—Ä–µ–Ω–∏–µ', '–∫–∏–Ω–µ–º–∞—Ç–∏–∫–∞', '–¥–∏–Ω–∞–º–∏–∫–∞'],
            '–ú–∞—Ç–µ–º–∞—Ç–∏–∫–∞': ['–º–∞—Ç–µ–º–∞—Ç–∏–∫–∞', '—É—Ä–∞–≤–Ω–µ–Ω–∏–µ', '—Ñ—É–Ω–∫—Ü–∏—è', '–ø—Ä–æ–∏–∑–≤–æ–¥–Ω–∞—è', '–∏–Ω—Ç–µ–≥—Ä–∞–ª', '–≥–µ–æ–º–µ—Ç—Ä–∏—è', '–∞–ª–≥–µ–±—Ä–∞', '—Ç—Ä–∏–≥–æ–Ω–æ–º–µ—Ç—Ä–∏—è']
        }
        
        # –ü–æ–¥—Å—á–µ—Ç —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π
        category_scores = {}
        for category, keywords in categories.items():
            score = 0
            for keyword in keywords:
                if keyword in file_name:
                    score += 3  # –ë–æ–ª—å—à–∏–π –≤–µ—Å –¥–ª—è –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
                if keyword in content_lower:
                    score += 1
            category_scores[category] = score
            
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é —Å –Ω–∞–∏–±–æ–ª—å—à–∏–º —Å—á–µ—Ç–æ–º
        if category_scores:
            best_category = max(category_scores, key=category_scores.get)
            if category_scores[best_category] > 0:
                return best_category
                
        return '–û–±—â–∞—è —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–∞'
        
    def extract_keywords(self, content):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –∏–∑ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ"""
        if not content:
            return []
            
        # –ü—Ä–æ—Å—Ç–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        words = content.lower().split()
        
        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Å—Ç–æ–ø-—Å–ª–æ–≤
        stop_words = {'–∏', '–≤', '–Ω–∞', '—Å', '–ø–æ', '–¥–ª—è', '–æ—Ç', '–¥–æ', '–∏–∑', '–∫', '—É', '–æ', '–æ–±', '—á—Ç–æ', '–∫–∞–∫', '–≥–¥–µ', '–∫–æ–≥–¥–∞', '–ø–æ—á–µ–º—É', '—ç—Ç–æ', '—Ç–æ', '—Ç–∞', '—Ç–µ', '—Ç–∞–∫–æ–π', '—Ç–∞–∫–∞—è', '—Ç–∞–∫–æ–µ', '—Ç–∞–∫–∏–µ', '–∏–ª–∏', '–Ω–æ', '–∞', '–∂–µ', '–ª–∏', '–±—ã', '–Ω–µ', '–Ω–∏', '—É–∂–µ', '–µ—â–µ', '—Ç–æ–ª—å–∫–æ', '–¥–∞–∂–µ', '–≤—Å–µ', '–≤—Å—ë', '–≤—Å–µ–≥–æ', '–≤—Å–µ–π', '–≤—Å–µ–º', '–≤—Å–µ–º–∏', '–≤—Å—é', '–≤—Å–µ—Ö', '–≤—Å–µ–≥–æ', '–≤—Å–µ–π', '–≤—Å–µ–º', '–≤—Å–µ–º–∏', '–≤—Å—é', '–≤—Å–µ—Ö'}
        
        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –∫–æ—Ä–æ—Ç–∫–∏—Ö —Å–ª–æ–≤
        keywords = [word for word in words if len(word) > 3 and word not in stop_words]
        
        # –ü–æ–¥—Å—á–µ—Ç —á–∞—Å—Ç–æ—Ç—ã
        from collections import Counter
        word_counts = Counter(keywords)
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ø-20 –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        return [word for word, count in word_counts.most_common(20)]
        
    def load_document(self, file_path):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –æ–¥–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        try:
            file_path = Path(file_path)
            
            if not file_path.exists():
                self.logger.error(f"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_path}")
                return False
                
            # –ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Ñ–∞–π–ª–µ
            file_size = file_path.stat().st_size
            file_type = mimetypes.guess_type(str(file_path))[0] or 'unknown'
            file_hash = self.get_file_hash(file_path)
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ
            content = self.extract_text_content(file_path)
            if content is None:
                self.logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –∏–∑ {file_path}")
                return False
                
            # –ö–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è
            category = self.categorize_document(file_path, content)
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
            keywords = self.extract_keywords(content)
            
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
            metadata = {
                'file_extension': file_path.suffix,
                'file_created': datetime.fromtimestamp(file_path.stat().st_ctime).isoformat(),
                'file_modified': datetime.fromtimestamp(file_path.stat().st_mtime).isoformat(),
                'keywords': keywords[:10],  # –¢–æ–ø-10 –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
                'word_count': len(content.split()),
                'character_count': len(content)
            }
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ —É–∂–µ –¥–æ–∫—É–º–µ–Ω—Ç
            cursor.execute("SELECT id FROM documents WHERE file_path = ?", (str(file_path),))
            existing = cursor.fetchone()
            
            if existing:
                # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞
                cursor.execute('''
                    UPDATE documents SET
                        file_name = ?, file_size = ?, file_type = ?, file_hash = ?,
                        content = ?, metadata = ?, category = ?, tags = ?,
                        updated_at = CURRENT_TIMESTAMP
                    WHERE file_path = ?
                ''', (
                    file_path.name, file_size, file_type, file_hash,
                    content, json.dumps(metadata), category, json.dumps(keywords),
                    str(file_path)
                ))
                doc_id = existing[0]
                self.logger.info(f"üîÑ –û–±–Ω–æ–≤–ª–µ–Ω –¥–æ–∫—É–º–µ–Ω—Ç: {file_path.name}")
            else:
                # –í—Å—Ç–∞–≤–∫–∞ –Ω–æ–≤–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞
                cursor.execute('''
                    INSERT INTO documents 
                    (file_path, file_name, file_size, file_type, file_hash, 
                     content, metadata, category, tags)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
                ''', (
                    str(file_path), file_path.name, file_size, file_type, file_hash,
                    content, json.dumps(metadata), category, json.dumps(keywords)
                ))
                doc_id = cursor.lastrowid
                self.logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω –¥–æ–∫—É–º–µ–Ω—Ç: {file_path.name}")
                
            # –°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞
            self.create_search_index(cursor, doc_id, content)
            
            conn.commit()
            conn.close()
            
            return True
            
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞ {file_path}: {e}")
            return False
            
    def create_search_index(self, cursor, doc_id, content):
        """–°–æ–∑–¥–∞–Ω–∏–µ –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞"""
        try:
            # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–π –∏–Ω–¥–µ–∫—Å
            cursor.execute("DELETE FROM document_index WHERE document_id = ?", (doc_id,))
            
            # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π –∏–Ω–¥–µ–∫—Å
            words = content.lower().split()
            for i, word in enumerate(words):
                if len(word) > 2:  # –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Å–ª–æ–≤–∞ –¥–ª–∏–Ω–Ω–µ–µ 2 —Å–∏–º–≤–æ–ª–æ–≤
                    context = ' '.join(words[max(0, i-5):i+6])  # –ö–æ–Ω—Ç–µ–∫—Å—Ç ¬±5 —Å–ª–æ–≤
                    cursor.execute('''
                        INSERT INTO document_index (document_id, keyword, position, context)
                        VALUES (?, ?, ?, ?)
                    ''', (doc_id, word, i, context))
                    
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –∏–Ω–¥–µ–∫—Å–∞: {e}")
            
    def load_directory(self, directory_path):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –≤—Å–µ—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏"""
        directory_path = Path(directory_path)
        
        if not directory_path.exists():
            self.logger.error(f"–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {directory_path}")
            return False
            
        self.logger.info(f"üìÅ –ù–∞—á–∏–Ω–∞–µ–º –∑–∞–≥—Ä—É–∑–∫—É –∏–∑ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏: {directory_path}")
        
        # –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è —Ñ–∞–π–ª–æ–≤
        supported_extensions = {
            '.txt', '.md', '.rst', '.pdf', '.doc', '.docx',
            '.rtf', '.odt', '.html', '.htm', '.xml', '.json'
        }
        
        total_files = 0
        loaded_files = 0
        skipped_files = 0
        
        # –†–µ–∫—É—Ä—Å–∏–≤–Ω—ã–π –æ–±—Ö–æ–¥ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        for file_path in directory_path.rglob('*'):
            if file_path.is_file() and file_path.suffix.lower() in supported_extensions:
                total_files += 1
                
                try:
                    if self.load_document(file_path):
                        loaded_files += 1
                    else:
                        skipped_files += 1
                        
                except Exception as e:
                    self.logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞ {file_path}: {e}")
                    skipped_files += 1
                    
        self.logger.info(f"üìä –ó–∞–≥—Ä—É–∑–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞:")
        self.logger.info(f"   –í—Å–µ–≥–æ —Ñ–∞–π–ª–æ–≤: {total_files}")
        self.logger.info(f"   –ó–∞–≥—Ä—É–∂–µ–Ω–æ: {loaded_files}")
        self.logger.info(f"   –ü—Ä–æ–ø—É—â–µ–Ω–æ: {skipped_files}")
        
        return loaded_files > 0
        
    def search_documents(self, query, limit=10):
        """–ü–æ–∏—Å–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ –∑–∞–ø—Ä–æ—Å—É"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # –ü–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
            query_words = query.lower().split()
            
            # –ü–æ–∏—Å–∫ –≤ –∏–Ω–¥–µ–∫—Å–µ
            placeholders = ','.join(['?' for _ in query_words])
            cursor.execute(f'''
                SELECT DISTINCT d.id, d.file_name, d.category, d.metadata, 
                       GROUP_CONCAT(DISTINCT di.keyword) as matched_keywords
                FROM documents d
                JOIN document_index di ON d.id = di.document_id
                WHERE di.keyword IN ({placeholders})
                GROUP BY d.id
                ORDER BY COUNT(DISTINCT di.keyword) DESC
                LIMIT ?
            ''', query_words + [limit])
            
            results = cursor.fetchall()
            conn.close()
            
            return results
            
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {e}")
            return []
            
    def index_document_for_vector_search(self, document_id, content, metadata=None):
        """–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞ –¥–ª—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞"""
        try:
            # –ò–º–ø–æ—Ä—Ç –∑–¥–µ—Å—å, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å —Ü–∏–∫–ª–∏—á–µ—Å–∫–∏—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
            from vector_search import VectorSearchEngine
            
            vector_engine = VectorSearchEngine(self.db_path)
            success = vector_engine.index_document(document_id, content, metadata)
            
            if success:
                self.logger.info(f"‚úÖ –î–æ–∫—É–º–µ–Ω—Ç {document_id} –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω –¥–ª—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞")
            else:
                self.logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç {document_id} –¥–ª—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞")
                
            return success
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞ {document_id}: {e}")
            return False
            
    def get_document_stats(self):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            cursor.execute("SELECT COUNT(*) FROM documents")
            total_docs = cursor.fetchone()[0]
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
            cursor.execute('''
                SELECT category, COUNT(*) as count
                FROM documents
                GROUP BY category
                ORDER BY count DESC
            ''')
            categories = cursor.fetchall()
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ç–∏–ø–∞–º —Ñ–∞–π–ª–æ–≤
            cursor.execute('''
                SELECT file_type, COUNT(*) as count
                FROM documents
                GROUP BY file_type
                ORDER BY count DESC
            ''')
            file_types = cursor.fetchall()
            
            conn.close()
            
            return {
                'total_documents': total_docs,
                'categories': categories,
                'file_types': file_types
            }
            
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏: {e}")
            return None

    def get_all_documents(self):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –≤—Å–µ—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute('''
                SELECT id, file_name, content, 
                       COALESCE(category, '') as category, 
                       COALESCE(tags, '') as tags, 
                       COALESCE(difficulty_level, 'medium') as difficulty_level, 
                       COALESCE(file_type, '') as file_type, 
                       COALESCE(file_size, 0) as file_size, 
                       COALESCE(file_hash, '') as file_hash, 
                       created_at, updated_at
                FROM documents
                ORDER BY created_at DESC
            ''')
            
            documents = []
            for row in cursor.fetchall():
                documents.append({
                    'id': row[0],
                    'file_name': row[1],
                    'content': row[2],
                    'category': row[3],
                    'tags': row[4],
                    'difficulty_level': row[5],
                    'file_type': row[6],
                    'file_size': row[7],
                    'file_hash': row[8],
                    'created_at': row[9],
                    'updated_at': row[10],
                    'metadata': f"Category: {row[3]}, Tags: {row[4]}, Level: {row[5]}"
                })
            
            conn.close()
            return documents
            
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –≤—Å–µ—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {e}")
            return []

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print("üìö –ó–ê–ì–†–£–ó–ß–ò–ö –î–û–ö–£–ú–ï–ù–¢–û–í RUBIN AI v2.0")
    print("=" * 50)
    
    # –ü—É—Ç—å –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —Å —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–π –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–æ–π
    tech_literature_path = r"E:\03.–¢–µ—Ö.–ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–∞"
    
    if not os.path.exists(tech_literature_path):
        print(f"‚ùå –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {tech_literature_path}")
        print("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø—Ä–æ–≤–µ—Ä—å—Ç–µ –ø—É—Ç—å –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —Å —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–π –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–æ–π")
        return
        
    # –°–æ–∑–¥–∞–Ω–∏–µ –∑–∞–≥—Ä—É–∑—á–∏–∫–∞
    loader = DocumentLoader()
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    print(f"üìÅ –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑: {tech_literature_path}")
    success = loader.load_directory(tech_literature_path)
    
    if success:
        print("\n‚úÖ –ó–∞–≥—Ä—É–∑–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        stats = loader.get_document_stats()
        if stats:
            print(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
            print(f"   –í—Å–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {stats['total_documents']}")
            
            print(f"\nüìÇ –ö–∞—Ç–µ–≥–æ—Ä–∏–∏:")
            for category, count in stats['categories']:
                print(f"   {category}: {count} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
                
            print(f"\nüìÑ –¢–∏–ø—ã —Ñ–∞–π–ª–æ–≤:")
            for file_type, count in stats['file_types']:
                print(f"   {file_type}: {count} —Ñ–∞–π–ª–æ–≤")
                
        print(f"\nüîç –¢–µ–ø–µ—Ä—å –≤—ã –º–æ–∂–µ—Ç–µ –∏—Å–∫–∞—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç—ã —á–µ—Ä–µ–∑ Rubin AI!")
        print(f"   –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö: {loader.db_path}")
        
    else:
        print("‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")

if __name__ == "__main__":
    main()

