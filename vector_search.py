#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–ú–æ–¥—É–ª—å –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –¥–ª—è Rubin AI v2.0
–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º embeddings
"""

import os
import sys
import sqlite3
import numpy as np
import logging
from typing import List, Tuple, Dict, Optional
from pathlib import Path
import json
import hashlib
from datetime import datetime

try:
    from sentence_transformers import SentenceTransformer
    SENTENCE_TRANSFORMERS_AVAILABLE = True
except ImportError:
    SENTENCE_TRANSFORMERS_AVAILABLE = False
    print("‚ö†Ô∏è sentence-transformers –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install sentence-transformers")

try:
    import faiss
    FAISS_AVAILABLE = True
except ImportError:
    FAISS_AVAILABLE = False
    print("‚ö†Ô∏è faiss-cpu –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install faiss-cpu")

from rubin_text_preprocessor import RubinTextPreprocessor # –î–æ–±–∞–≤–ª—è–µ–º –¥–ª—è —Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–π –æ—á–∏—Å—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞

logger = logging.getLogger(__name__)

class VectorSearchEngine:
    """–î–≤–∏–∂–æ–∫ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
    
    def __init__(self, db_path="rubin_ai_documents.db", model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"):
        self.db_path = db_path
        self.model_name = model_name
        self.model = None
        self.index = None
        self.dimension = 384  # –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –¥–ª—è multilingual-MiniLM-L12-v2
        self.setup_logging()
        self.text_preprocessor = RubinTextPreprocessor() # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞ —Ç–µ–∫—Å—Ç–∞
        self.setup_database()
        self.load_model()
        
    def setup_logging(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('vector_search.log', encoding='utf-8'),
                logging.StreamHandler(sys.stdout)
            ]
        )
        self.logger = logging.getLogger(__name__)
        
    def setup_database(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü –¥–ª—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # –¢–∞–±–ª–∏—Ü–∞ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –≤–µ–∫—Ç–æ—Ä–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS document_vectors (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    document_id INTEGER NOT NULL,
                    vector_hash TEXT NOT NULL,
                    vector_data BLOB NOT NULL,
                    content_hash TEXT NOT NULL,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    FOREIGN KEY (document_id) REFERENCES documents (id)
                )
            ''')
            
            # –¢–∞–±–ª–∏—Ü–∞ –¥–ª—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS vector_metadata (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    document_id INTEGER NOT NULL,
                    content_preview TEXT,
                    keywords TEXT,
                    category TEXT,
                    similarity_threshold REAL DEFAULT 0.7,
                    FOREIGN KEY (document_id) REFERENCES documents (id)
                )
            ''')
            
            # –ò–Ω–¥–µ–∫—Å –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞
            cursor.execute('''
                CREATE INDEX IF NOT EXISTS idx_vector_hash ON document_vectors(vector_hash)
            ''')
            
            cursor.execute('''
                CREATE INDEX IF NOT EXISTS idx_document_id ON document_vectors(document_id)
            ''')
            
            conn.commit()
            conn.close()
            
            self.logger.info("‚úÖ –¢–∞–±–ª–∏—Ü—ã –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã")
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö —Ç–∞–±–ª–∏—Ü: {e}")
            
    def load_model(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ embeddings"""

        
        if not SENTENCE_TRANSFORMERS_AVAILABLE:
            self.logger.warning("‚ö†Ô∏è sentence-transformers –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ –æ—Ç–∫–ª—é—á–µ–Ω")
            return
            
        try:
            self.logger.info(f"üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏: {self.model_name}")
            
            # –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –æ—à–∏–±–∫–∏ PyTorch meta tensor
            import torch
            import os
            
            # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è –¥–ª—è –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –æ—à–∏–±–∫–∏
            os.environ['TOKENIZERS_PARALLELISM'] = 'false'
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å —Å —è–≤–Ω—ã–º —É–∫–∞–∑–∞–Ω–∏–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞
            self.model = SentenceTransformer(self.model_name, device='cpu')
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –∏ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ
            if hasattr(self.model, '_modules'):
                for name, module in self.model._modules.items():
                    if hasattr(module, 'to'):
                        try:
                            module.to('cpu')
                        except Exception as module_error:
                            self.logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–µ—Ä–µ–º–µ—â–µ–Ω–∏—è –º–æ–¥—É–ª—è {name}: {module_error}")
                            # –ü—ã—Ç–∞–µ–º—Å—è –∏—Å–ø—Ä–∞–≤–∏—Ç—å —á–µ—Ä–µ–∑ to_empty
                            try:
                                if hasattr(module, 'to_empty'):
                                    module.to_empty(device='cpu')
                            except:
                                pass
            
            self.dimension = self.model.get_sentence_embedding_dimension()
            self.logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞, —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {self.dimension}")
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
            
            # –ü–æ–ø—ã—Ç–∫–∞ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏
            try:
                self.logger.info("üîÑ –ü–æ–ø—ã—Ç–∫–∞ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏...")
                
                # –û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞
                import torch
                torch.cuda.empty_cache() if torch.cuda.is_available() else None
                
                # –ó–∞–≥—Ä—É–∑–∫–∞ —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏
                self.model = SentenceTransformer(
                    self.model_name, 
                    device='cpu',
                    trust_remote_code=True
                )
                
                # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞ CPU
                self.model.eval()
                self.dimension = self.model.get_sentence_embedding_dimension()
                self.logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–º —Å–ø–æ—Å–æ–±–æ–º, —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {self.dimension}")
                
            except Exception as e2:
                self.logger.error(f"‚ùå –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ —Ç–∞–∫–∂–µ –Ω–µ —É–¥–∞–ª–∞—Å—å: {e2}")
                self.model = None
            
    def generate_embedding(self, text: str) -> Optional[np.ndarray]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è embedding –¥–ª—è —Ç–µ–∫—Å—Ç–∞"""
        if not self.model:
            return None
            
        try:
            # –û—á–∏—Å—Ç–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ–∫—Å—Ç–∞
            clean_text = self.clean_text(text)
            if not clean_text:
                return None
                
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è embedding
            embedding = self.model.encode(clean_text, convert_to_numpy=True)
            return embedding.astype(np.float32)
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ embedding: {e}")
            return None
            
    def clean_text(self, text: str) -> str:
        """–û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –ª—É—á—à–µ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ embeddings —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º RubinTextPreprocessor"""
        return self.text_preprocessor.preprocess_text(
            text,
            to_lower=True,
            remove_spaces=True,
            remove_special=True,
            limit_len=512 # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–ª–∏–Ω—ã –¥–ª—è embeddings
        )
        
    def vector_to_blob(self, vector: np.ndarray) -> bytes:
        """–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤–µ–∫—Ç–æ—Ä–∞ –≤ BLOB –¥–ª—è SQLite"""
        return vector.tobytes()
        
    def blob_to_vector(self, blob: bytes) -> np.ndarray:
        """–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è BLOB –≤ –≤–µ–∫—Ç–æ—Ä"""
        return np.frombuffer(blob, dtype=np.float32)
        
    def get_content_hash(self, content: str) -> str:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ö–µ—à–∞ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ"""
        return hashlib.md5(content.encode('utf-8')).hexdigest()
        
    def index_document(self, document_id: int, content: str, metadata: Dict = None) -> bool:
        """–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞ –≤ –≤–µ–∫—Ç–æ—Ä–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ"""
        if not self.model:
            self.logger.warning("‚ö†Ô∏è –ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞, –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –ø—Ä–æ–ø—É—â–µ–Ω–∞")
            return False
            
        try:
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è embedding
            embedding = self.generate_embedding(content)
            if embedding is None:
                return False
                
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è
            content_hash = self.get_content_hash(content)
            vector_hash = hashlib.md5(embedding.tobytes()).hexdigest()
            
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞, –Ω–µ –∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω –ª–∏ —É–∂–µ –¥–æ–∫—É–º–µ–Ω—Ç
            cursor.execute('''
                SELECT id FROM document_vectors 
                WHERE document_id = ? AND content_hash = ?
            ''', (document_id, content_hash))
            
            if cursor.fetchone():
                self.logger.info(f"üìÑ –î–æ–∫—É–º–µ–Ω—Ç {document_id} —É–∂–µ –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω")
                conn.close()
                return True
                
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–∞
            vector_blob = self.vector_to_blob(embedding)
            cursor.execute('''
                INSERT INTO document_vectors 
                (document_id, vector_hash, vector_data, content_hash)
                VALUES (?, ?, ?, ?)
            ''', (document_id, vector_hash, vector_blob, content_hash))
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
            if metadata:
                cursor.execute('''
                    INSERT OR REPLACE INTO vector_metadata
                    (document_id, content_preview, keywords, category)
                    VALUES (?, ?, ?, ?)
                ''', (
                    document_id,
                    content[:200] + "..." if len(content) > 200 else content,
                    metadata.get('keywords', ''),
                    metadata.get('category', '')
                ))
            
            conn.commit()
            conn.close()
            
            self.logger.info(f"‚úÖ –î–æ–∫—É–º–µ–Ω—Ç {document_id} –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω –≤ –≤–µ–∫—Ç–æ—Ä–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ")
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞ {document_id}: {e}")
            return False
            
    def build_faiss_index(self) -> bool:
        """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ FAISS –∏–Ω–¥–µ–∫—Å–∞ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞"""
        if not FAISS_AVAILABLE:
            self.logger.warning("‚ö†Ô∏è FAISS –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –º–µ–¥–ª–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫")
            return False
            
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # –ü–æ–ª—É—á–µ–Ω–∏–µ –≤—Å–µ—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤
            cursor.execute('''
                SELECT dv.document_id, dv.vector_data, vm.content_preview, vm.category
                FROM document_vectors dv
                LEFT JOIN vector_metadata vm ON dv.document_id = vm.document_id
            ''')
            
            results = cursor.fetchall()
            conn.close()
            
            if not results:
                self.logger.warning("‚ö†Ô∏è –ù–µ—Ç –≤–µ–∫—Ç–æ—Ä–æ–≤ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏")
                return False
                
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
            vectors = []
            self.document_ids = []
            self.metadata = []
            
            for doc_id, vector_blob, preview, category in results:
                vector = self.blob_to_vector(vector_blob)
                vectors.append(vector)
                self.document_ids.append(doc_id)
                self.metadata.append({
                    'preview': preview,
                    'category': category
                })
                
            # –°–æ–∑–¥–∞–Ω–∏–µ FAISS –∏–Ω–¥–µ–∫—Å–∞
            vectors_array = np.vstack(vectors).astype(np.float32)
            self.index = faiss.IndexFlatIP(self.dimension)  # Inner Product –¥–ª—è –∫–æ—Å–∏–Ω—É—Å–Ω–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞
            
            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ–∫—Ç–æ—Ä–æ–≤ –¥–ª—è –∫–æ—Å–∏–Ω—É—Å–Ω–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞
            faiss.normalize_L2(vectors_array)
            self.index.add(vectors_array)
            
            self.logger.info(f"‚úÖ FAISS –∏–Ω–¥–µ–∫—Å –ø–æ—Å—Ç—Ä–æ–µ–Ω: {len(vectors)} –≤–µ–∫—Ç–æ—Ä–æ–≤")
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è FAISS –∏–Ω–¥–µ–∫—Å–∞: {e}")
            return False
            
    def search_similar(self, query: str, top_k: int = 5, threshold: float = 0.7) -> List[Dict]:
        """–ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ –∑–∞–ø—Ä–æ—Å—É"""
        if not self.model:
            self.logger.warning("‚ö†Ô∏è –ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
            return []
            
        try:
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è embedding –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞
            query_embedding = self.generate_embedding(query)
            if query_embedding is None:
                return []
                
            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è –∫–æ—Å–∏–Ω—É—Å–Ω–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞
            query_embedding = query_embedding.reshape(1, -1)
            faiss.normalize_L2(query_embedding)
            
            results = []
            
            if self.index and FAISS_AVAILABLE:
                # –ë—ã—Å—Ç—Ä—ã–π –ø–æ–∏—Å–∫ —á–µ—Ä–µ–∑ FAISS
                similarities, indices = self.index.search(query_embedding, min(top_k, len(self.document_ids)))
                
                for i, (similarity, idx) in enumerate(zip(similarities[0], indices[0])):
                    if similarity >= threshold:
                        results.append({
                            'document_id': self.document_ids[idx],
                            'similarity': float(similarity),
                            'metadata': self.metadata[idx]
                        })
            else:
                # –ú–µ–¥–ª–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ —á–µ—Ä–µ–∑ SQLite
                results = self._slow_search(query_embedding, top_k, threshold)
                
            # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ —Å—Ö–æ–¥—Å—Ç–≤—É
            results.sort(key=lambda x: x['similarity'], reverse=True)
            
            self.logger.info(f"üîç –ù–∞–π–¥–µ–Ω–æ {len(results)} –ø–æ—Ö–æ–∂–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
            return results[:top_k]
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞: {e}")
            return []
            
    def _slow_search(self, query_embedding: np.ndarray, top_k: int, threshold: float) -> List[Dict]:
        """–ú–µ–¥–ª–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ —á–µ—Ä–µ–∑ SQLite (fallback)"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute('''
                SELECT dv.document_id, dv.vector_data, vm.content_preview, vm.category
                FROM document_vectors dv
                LEFT JOIN vector_metadata vm ON dv.document_id = vm.document_id
            ''')
            
            results = []
            for doc_id, vector_blob, preview, category in cursor.fetchall():
                vector = self.blob_to_vector(vector_blob)
                similarity = np.dot(query_embedding[0], vector)
                
                if similarity >= threshold:
                    results.append({
                        'document_id': doc_id,
                        'similarity': float(similarity),
                        'metadata': {
                            'preview': preview,
                            'category': category
                        }
                    })
                    
            conn.close()
            return results
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –º–µ–¥–ª–µ–Ω–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞: {e}")
            return []
            
    def get_document_content(self, document_id: int) -> Optional[str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute('''
                SELECT content FROM documents WHERE id = ?
            ''', (document_id,))
            
            result = cursor.fetchone()
            conn.close()
            
            return result[0] if result else None
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞ {document_id}: {e}")
            return None
            
    def get_stats(self) -> Dict:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–µ–∫—Ç–æ—Ä–æ–≤
            cursor.execute("SELECT COUNT(*) FROM document_vectors")
            total_vectors = cursor.fetchone()[0]
            
            # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –≤–µ–∫—Ç–æ—Ä–∞–º–∏
            cursor.execute("SELECT COUNT(DISTINCT document_id) FROM document_vectors")
            documents_with_vectors = cursor.fetchone()[0]
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
            cursor.execute('''
                SELECT vm.category, COUNT(*) 
                FROM vector_metadata vm
                GROUP BY vm.category
            ''')
            categories = dict(cursor.fetchall())
            
            conn.close()
            
            return {
                'total_vectors': total_vectors,
                'documents_with_vectors': documents_with_vectors,
                'categories': categories,
                'model_loaded': self.model is not None,
                'faiss_available': FAISS_AVAILABLE,
                'faiss_index_built': self.index is not None
            }
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏: {e}")
            return {}

if __name__ == "__main__":
    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
    engine = VectorSearchEngine()
    
    # –ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
    stats = engine.get_stats()
    print("üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞:")
    for key, value in stats.items():
        print(f"   {key}: {value}")
