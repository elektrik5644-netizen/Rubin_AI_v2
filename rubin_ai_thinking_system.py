#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–ò–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –º—ã—à–ª–µ–Ω–∏—è Rubin AI
–û–±—ä–µ–¥–∏–Ω—è–µ—Ç —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö, –Ω–µ—Ç—Ä–∏–≤–∏–∞–ª—å–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã –∏ –∞–±–¥—É–∫—Ç–∏–≤–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ
"""

import logging
import time
from typing import Dict, List, Any, Optional
from datetime import datetime
import json

from ai_thinking_datasets import AIThinkingDatasets, KnowledgeItem
from non_trivial_queries import NonTrivialQueryGenerator
from abductive_reasoning import AbductiveReasoningEngine, Evidence, AbductiveReasoning

logging.basicConfig(level=logging.INFO, format='%(levelname)s:%(name)s:%(message)s')
logger = logging.getLogger(__name__)

class RubinAIThinkingSystem:
    """–ò–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –º—ã—à–ª–µ–Ω–∏—è Rubin AI"""
    
    def __init__(self):
        self.datasets = AIThinkingDatasets()
        self.query_generator = NonTrivialQueryGenerator()
        self.reasoning_engine = AbductiveReasoningEngine()
        self.thinking_history = []
        self.learning_progress = {}
        self._initialize_thinking_system()
        logger.info("üß† –ò–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –º—ã—à–ª–µ–Ω–∏—è Rubin AI –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")
    
    def _initialize_thinking_system(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º—ã –º—ã—à–ª–µ–Ω–∏—è"""
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è –ø–æ –¥–æ–º–µ–Ω–∞–º
        domains = ["electrical", "math", "programming", "controllers"]
        for domain in domains:
            self.learning_progress[domain] = {
                "knowledge_items_processed": 0,
                "queries_generated": 0,
                "reasoning_sessions": 0,
                "complexity_level": 1,
                "last_learning_session": None,
                "thinking_patterns": [],
                "improvement_areas": []
            }
        
        logger.info(f"‚úÖ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –ø—Ä–æ–≥—Ä–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è –¥–ª—è {len(domains)} –¥–æ–º–µ–Ω–æ–≤")
    
    def stimulate_thinking(self, domain: str, complexity_level: int = 4) -> Dict[str, Any]:
        """–°—Ç–∏–º—É–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –º—ã—à–ª–µ–Ω–∏—è —á–µ—Ä–µ–∑ –Ω–µ—Ç—Ä–∏–≤–∏–∞–ª—å–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã"""
        
        logger.info(f"üéØ –°—Ç–∏–º—É–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –º—ã—à–ª–µ–Ω–∏—è –≤ –¥–æ–º–µ–Ω–µ {domain} (—É—Ä–æ–≤–µ–Ω—å {complexity_level})")
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –Ω–µ—Ç—Ä–∏–≤–∏–∞–ª—å–Ω—ã–π –∑–∞–ø—Ä–æ—Å
        query_data = self.query_generator.generate_non_trivial_query(domain, complexity_level)
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–ª–æ–∂–Ω–æ—Å—Ç—å –∑–∞–ø—Ä–æ—Å–∞
        complexity_analysis = self.query_generator.analyze_query_complexity(query_data["query"])
        
        # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –∑–Ω–∞–Ω–∏—è
        relevant_knowledge = self.datasets.get_diverse_representative_data(domain, 3)
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∞–±–¥—É–∫—Ç–∏–≤–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è
        thinking_result = self._process_thinking_query(query_data, relevant_knowledge, domain)
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è
        self._update_learning_progress(domain, query_data, thinking_result)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∏—Å—Ç–æ—Ä–∏—é –º—ã—à–ª–µ–Ω–∏—è
        self._save_thinking_session(query_data, thinking_result, domain)
        
        return {
            "query": query_data["query"],
            "query_type": query_data["query_type"],
            "thinking_result": thinking_result,
            "complexity_analysis": complexity_analysis,
            "relevant_knowledge": [item.concept for item in relevant_knowledge],
            "domain": domain,
            "timestamp": datetime.now().isoformat()
        }
    
    def _process_thinking_query(self, query_data: Dict, knowledge_items: List[KnowledgeItem], domain: str) -> Dict[str, Any]:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –º—ã—à–ª–µ–Ω–∏—è"""
        
        # –°–æ–∑–¥–∞–µ–º –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∑–Ω–∞–Ω–∏–π
        evidence_list = []
        for i, item in enumerate(knowledge_items):
            evidence = Evidence(
                id=f"ev_{domain}_{i:03d}",
                description=f"–ó–Ω–∞–Ω–∏–µ: {item.concept} - {item.definition}",
                domain=domain,
                confidence=item.confidence_score,
                timestamp=datetime.now().isoformat(),
                source="knowledge_base"
            )
            evidence_list.append(evidence)
            self.reasoning_engine.add_evidence(evidence)
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –∞–±–¥—É–∫—Ç–∏–≤–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ
        evidence_ids = [ev.id for ev in evidence_list]
        reasoning_result = self.reasoning_engine.perform_abductive_reasoning(evidence_ids, domain)
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç
        extended_response = self._generate_extended_response(query_data, reasoning_result, knowledge_items)
        
        return {
            "reasoning_result": reasoning_result,
            "extended_response": extended_response,
            "evidence_count": len(evidence_list),
            "knowledge_items_used": len(knowledge_items),
            "thinking_depth": self._calculate_thinking_depth(reasoning_result, knowledge_items)
        }
    
    def _generate_extended_response(self, query_data: Dict, reasoning_result: AbductiveReasoning, 
                                   knowledge_items: List[KnowledgeItem]) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞"""
        
        response_parts = []
        
        # –í–≤–µ–¥–µ–Ω–∏–µ
        response_parts.append(f"üß† **–†–ê–°–®–ò–†–ï–ù–ù–û–ï –ú–´–®–õ–ï–ù–ò–ï RUBIN AI**")
        response_parts.append(f"")
        response_parts.append(f"**üéØ –ó–∞–ø—Ä–æ—Å:** {query_data['query']}")
        response_parts.append(f"**üìä –¢–∏–ø –º—ã—à–ª–µ–Ω–∏—è:** {query_data['stimulus_type']}")
        response_parts.append(f"**üß† –£—Ä–æ–≤–µ–Ω—å —Å–ª–æ–∂–Ω–æ—Å—Ç–∏:** {query_data['thinking_level']}/5")
        response_parts.append(f"")
        
        # –ê–±–¥—É–∫—Ç–∏–≤–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ
        response_parts.append(f"**üîç –ê–ë–î–£–ö–¢–ò–í–ù–û–ï –†–ê–°–°–£–ñ–î–ï–ù–ò–ï:**")
        response_parts.append(f"")
        
        explanation = self.reasoning_engine.explain_reasoning(reasoning_result)
        response_parts.append(explanation)
        
        # –ú–µ–∂–¥–æ–º–µ–Ω–Ω—ã–µ —Å–≤—è–∑–∏
        response_parts.append(f"")
        response_parts.append(f"**üîó –ú–ï–ñ–î–û–ú–ï–ù–ù–´–ï –°–í–Ø–ó–ò:**")
        response_parts.append(f"")
        
        cross_domain_connections = self.datasets.find_cross_domain_connections(
            reasoning_result.best_hypothesis.description
        )
        
        if cross_domain_connections:
            response_parts.append(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã —Å–≤—è–∑–∏ —Å –¥—Ä—É–≥–∏–º–∏ –¥–æ–º–µ–Ω–∞–º–∏:")
            for connection in cross_domain_connections[:3]:
                response_parts.append(f"‚Ä¢ {connection['target_concept']} ({connection['target_domain']}) - —Å–∏–ª–∞ —Å–≤—è–∑–∏: {connection['strength']:.2f}")
        else:
            response_parts.append(f"–ú–µ–∂–¥–æ–º–µ–Ω–Ω—ã–µ —Å–≤—è–∑–∏ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã –¥–ª—è –¥–∞–Ω–Ω–æ–π –≥–∏–ø–æ—Ç–µ–∑—ã.")
        
        # –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–º–µ—Ä—ã
        response_parts.append(f"")
        response_parts.append(f"**üìö –ü–†–ê–ö–¢–ò–ß–ï–°–ö–ò–ï –ü–†–ò–ú–ï–†–´:**")
        response_parts.append(f"")
        
        for item in knowledge_items:
            response_parts.append(f"**{item.concept}:**")
            response_parts.append(f"{item.definition}")
            if item.examples:
                response_parts.append(f"–ü—Ä–∏–º–µ—Ä—ã: {'; '.join(item.examples[:2])}")
            response_parts.append(f"")
        
        # –¢–≤–æ—Ä—á–µ—Å–∫–∏–µ –∏–Ω—Å–∞–π—Ç—ã
        response_parts.append(f"**üí° –¢–í–û–†–ß–ï–°–ö–ò–ï –ò–ù–°–ê–ô–¢–´:**")
        response_parts.append(f"")
        
        insights = self._generate_creative_insights(query_data, reasoning_result, knowledge_items)
        for insight in insights:
            response_parts.append(f"‚Ä¢ {insight}")
        
        # –ó–∞–∫–ª—é—á–µ–Ω–∏–µ
        response_parts.append(f"")
        response_parts.append(f"**‚úÖ –ó–ê–ö–õ–Æ–ß–ï–ù–ò–ï:**")
        response_parts.append(f"")
        response_parts.append(f"–ù–∞ –æ—Å–Ω–æ–≤–µ –∞–±–¥—É–∫—Ç–∏–≤–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∏ –∞–Ω–∞–ª–∏–∑–∞ –∑–Ω–∞–Ω–∏–π, –Ω–∞–∏–ª—É—á—à–∏–º –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ–º —è–≤–ª—è–µ—Ç—Å—è:")
        response_parts.append(f"**{reasoning_result.best_hypothesis.description}**")
        response_parts.append(f"")
        response_parts.append(f"–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ: {reasoning_result.confidence_score:.2f}")
        response_parts.append(f"–ì–ª—É–±–∏–Ω–∞ –º—ã—à–ª–µ–Ω–∏—è: {self._calculate_thinking_depth(reasoning_result, knowledge_items):.2f}")
        
        return "\n".join(response_parts)
    
    def _generate_creative_insights(self, query_data: Dict, reasoning_result: AbductiveReasoning, 
                                   knowledge_items: List[KnowledgeItem]) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–≤–æ—Ä—á–µ—Å–∫–∏—Ö –∏–Ω—Å–∞–π—Ç–æ–≤"""
        
        insights = []
        
        # –ò–Ω—Å–∞–π—Ç—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–∏–ø–∞ –∑–∞–ø—Ä–æ—Å–∞
        if query_data["stimulus_type"] == "paradox_resolution":
            insights.append("–ü–∞—Ä–∞–¥–æ–∫—Å—ã —á–∞—Å—Ç–æ —É–∫–∞–∑—ã–≤–∞—é—Ç –Ω–∞ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ–∫ –≤ –Ω–∞—à–µ–º –ø–æ–Ω–∏–º–∞–Ω–∏–∏ —Å–∏—Å—Ç–µ–º—ã")
            insights.append("–†–µ—à–µ–Ω–∏–µ –ø–∞—Ä–∞–¥–æ–∫—Å–∞ –º–æ–∂–µ—Ç –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫ –Ω–æ–≤–æ–º—É –ø–æ–Ω–∏–º–∞–Ω–∏—é –ø—Ä–∏–Ω—Ü–∏–ø–æ–≤")
        
        elif query_data["stimulus_type"] == "cross_domain_connections":
            insights.append("–ê–Ω–∞–ª–æ–≥–∏–∏ –º–µ–∂–¥—É –¥–æ–º–µ–Ω–∞–º–∏ –º–æ–≥—É—Ç —Ä–∞—Å–∫—Ä—ã—Ç—å —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–µ –ø—Ä–∏–Ω—Ü–∏–ø—ã")
            insights.append("–ü–µ—Ä–µ–Ω–æ—Å –∑–Ω–∞–Ω–∏–π –º–µ–∂–¥—É –æ–±–ª–∞—Å—Ç—è–º–∏ —Å—Ç–∏–º—É–ª–∏—Ä—É–µ—Ç –∏–Ω–Ω–æ–≤–∞—Ü–∏–∏")
        
        elif query_data["stimulus_type"] == "abductive_inference":
            insights.append("–õ—É—á—à–µ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ –Ω–µ –≤—Å–µ–≥–¥–∞ —Å–∞–º–æ–µ –ø—Ä–æ—Å—Ç–æ–µ")
            insights.append("–ê–±–¥—É–∫—Ç–∏–≤–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –ø–æ–∑–≤–æ–ª—è–µ—Ç –Ω–∞—Ö–æ–¥–∏—Ç—å —Å–∫—Ä—ã—Ç—ã–µ –ø—Ä–∏—á–∏–Ω—ã")
        
        # –ò–Ω—Å–∞–π—Ç—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ
        if reasoning_result.confidence_score > 0.8:
            insights.append("–í—ã—Å–æ–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç—å –≥–∏–ø–æ—Ç–µ–∑—ã")
        elif reasoning_result.confidence_score < 0.5:
            insights.append("–ù–∏–∑–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å —Ç—Ä–µ–±—É–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤")
        
        # –ò–Ω—Å–∞–π—Ç—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏
        if reasoning_result.best_hypothesis.complexity_score > 0.7:
            insights.append("–°–ª–æ–∂–Ω—ã–µ –≥–∏–ø–æ—Ç–µ–∑—ã —Ç—Ä–µ–±—É—é—Ç –≥–ª—É–±–æ–∫–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è —Å–∏—Å—Ç–µ–º—ã")
        else:
            insights.append("–ü—Ä–æ—Å—Ç—ã–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏—è —á–∞—Å—Ç–æ –æ–∫–∞–∑—ã–≤–∞—é—Ç—Å—è –Ω–∞–∏–±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–º–∏")
        
        return insights
    
    def _calculate_thinking_depth(self, reasoning_result: AbductiveReasoning, knowledge_items: List[KnowledgeItem]) -> float:
        """–†–∞—Å—á–µ—Ç –≥–ª—É–±–∏–Ω—ã –º—ã—à–ª–µ–Ω–∏—è"""
        
        # –§–∞–∫—Ç–æ—Ä—ã –≥–ª—É–±–∏–Ω—ã –º—ã—à–ª–µ–Ω–∏—è
        evidence_factor = len(reasoning_result.evidence_used) / 5.0  # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–æ 1.0
        hypothesis_complexity = reasoning_result.best_hypothesis.complexity_score
        alternative_count = len(reasoning_result.alternative_hypotheses) / 3.0  # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        knowledge_diversity = len(set(item.domain for item in knowledge_items)) / 4.0  # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        
        # –í–∑–≤–µ—à–µ–Ω–Ω–∞—è —Å—É–º–º–∞
        thinking_depth = (
            evidence_factor * 0.3 +
            hypothesis_complexity * 0.3 +
            alternative_count * 0.2 +
            knowledge_diversity * 0.2
        )
        
        return min(1.0, thinking_depth)
    
    def _update_learning_progress(self, domain: str, query_data: Dict, thinking_result: Dict):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è"""
        
        if domain not in self.learning_progress:
            return
        
        progress = self.learning_progress[domain]
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—á–µ—Ç—á–∏–∫–∏
        progress["queries_generated"] += 1
        progress["reasoning_sessions"] += 1
        progress["last_learning_session"] = datetime.now().isoformat()
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —É—Ä–æ–≤–µ–Ω—å —Å–ª–æ–∂–Ω–æ—Å—Ç–∏
        if query_data["thinking_level"] > progress["complexity_level"]:
            progress["complexity_level"] = query_data["thinking_level"]
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã –º—ã—à–ª–µ–Ω–∏—è
        thinking_pattern = {
            "query_type": query_data["query_type"],
            "stimulus_type": query_data["stimulus_type"],
            "thinking_depth": thinking_result["thinking_depth"],
            "timestamp": datetime.now().isoformat()
        }
        progress["thinking_patterns"].append(thinking_pattern)
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –æ–±–ª–∞—Å—Ç–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è
        if thinking_result["thinking_depth"] < 0.6:
            if "–≥–ª—É–±–∏–Ω–∞ –º—ã—à–ª–µ–Ω–∏—è" not in progress["improvement_areas"]:
                progress["improvement_areas"].append("–≥–ª—É–±–∏–Ω–∞ –º—ã—à–ª–µ–Ω–∏—è")
        
        if len(thinking_result["reasoning_result"].alternative_hypotheses) < 2:
            if "–∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –≥–∏–ø–æ—Ç–µ–∑—ã" not in progress["improvement_areas"]:
                progress["improvement_areas"].append("–∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –≥–∏–ø–æ—Ç–µ–∑—ã")
    
    def _save_thinking_session(self, query_data: Dict, thinking_result: Dict, domain: str):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–µ—Å—Å–∏–∏ –º—ã—à–ª–µ–Ω–∏—è"""
        
        session = {
            "domain": domain,
            "query": query_data["query"],
            "query_type": query_data["query_type"],
            "thinking_depth": thinking_result["thinking_depth"],
            "confidence_score": thinking_result["reasoning_result"].confidence_score,
            "timestamp": datetime.now().isoformat(),
            "session_id": f"session_{len(self.thinking_history) + 1:04d}"
        }
        
        self.thinking_history.append(session)
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é –ø–æ—Å–ª–µ–¥–Ω–∏–º–∏ 100 —Å–µ—Å—Å–∏—è–º–∏
        if len(self.thinking_history) > 100:
            self.thinking_history = self.thinking_history[-100:]
    
    def get_thinking_analytics(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∞–Ω–∞–ª–∏—Ç–∏–∫–∏ –º—ã—à–ª–µ–Ω–∏—è"""
        
        analytics = {
            "total_sessions": len(self.thinking_history),
            "domain_breakdown": {},
            "thinking_depth_distribution": {i: 0 for i in range(1, 6)},
            "learning_progress": self.learning_progress,
            "average_thinking_depth": 0.0,
            "improvement_trends": {},
            "most_challenging_domains": []
        }
        
        if not self.thinking_history:
            return analytics
        
        # –ê–Ω–∞–ª–∏–∑ –ø–æ –¥–æ–º–µ–Ω–∞–º
        for session in self.thinking_history:
            domain = session["domain"]
            if domain not in analytics["domain_breakdown"]:
                analytics["domain_breakdown"][domain] = 0
            analytics["domain_breakdown"][domain] += 1
        
        # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≥–ª—É–±–∏–Ω—ã –º—ã—à–ª–µ–Ω–∏—è
        total_depth = 0
        for session in self.thinking_history:
            depth = session["thinking_depth"]
            depth_level = int(depth * 5) + 1
            depth_level = min(5, depth_level)
            analytics["thinking_depth_distribution"][depth_level] += 1
            total_depth += depth
        
        analytics["average_thinking_depth"] = total_depth / len(self.thinking_history)
        
        # –¢—Ä–µ–Ω–¥—ã —É–ª—É—á—à–µ–Ω–∏—è
        for domain, progress in self.learning_progress.items():
            if progress["thinking_patterns"]:
                recent_patterns = progress["thinking_patterns"][-5:]  # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 5 —Å–µ—Å—Å–∏–π
                avg_recent_depth = sum(p["thinking_depth"] for p in recent_patterns) / len(recent_patterns)
                
                if len(progress["thinking_patterns"]) >= 10:
                    older_patterns = progress["thinking_patterns"][-10:-5]  # –ü—Ä–µ–¥—ã–¥—É—â–∏–µ 5 —Å–µ—Å—Å–∏–π
                    avg_older_depth = sum(p["thinking_depth"] for p in older_patterns) / len(older_patterns)
                    
                    improvement = avg_recent_depth - avg_older_depth
                    analytics["improvement_trends"][domain] = improvement
        
        # –°–∞–º—ã–µ —Å–ª–æ–∂–Ω—ã–µ –¥–æ–º–µ–Ω—ã
        domain_difficulties = {}
        for domain, progress in self.learning_progress.items():
            if progress["thinking_patterns"]:
                avg_depth = sum(p["thinking_depth"] for p in progress["thinking_patterns"]) / len(progress["thinking_patterns"])
                domain_difficulties[domain] = avg_depth
        
        analytics["most_challenging_domains"] = sorted(
            domain_difficulties.items(), 
            key=lambda x: x[1], 
            reverse=True
        )[:3]
        
        return analytics
    
    def generate_learning_recommendations(self, domain: str) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –ø–æ –æ–±—É—á–µ–Ω–∏—é"""
        
        recommendations = []
        
        if domain not in self.learning_progress:
            return ["–î–æ–º–µ–Ω –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ —Å–∏—Å—Ç–µ–º–µ –æ–±—É—á–µ–Ω–∏—è"]
        
        progress = self.learning_progress[domain]
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —É—Ä–æ–≤–Ω—è —Å–ª–æ–∂–Ω–æ—Å—Ç–∏
        if progress["complexity_level"] < 3:
            recommendations.append("–£–≤–µ–ª–∏—á–∏—Ç—å —Å–ª–æ–∂–Ω–æ—Å—Ç—å –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–ª—è —Ä–∞–∑–≤–∏—Ç–∏—è –º—ã—à–ª–µ–Ω–∏—è")
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±–ª–∞—Å—Ç–µ–π —É–ª—É—á—à–µ–Ω–∏—è
        for area in progress["improvement_areas"]:
            if area == "–≥–ª—É–±–∏–Ω–∞ –º—ã—à–ª–µ–Ω–∏—è":
                recommendations.append("–§–æ–∫—É—Å–∏—Ä–æ–≤–∞—Ç—å—Å—è –Ω–∞ –±–æ–ª–µ–µ –≥–ª—É–±–æ–∫–æ–º –∞–Ω–∞–ª–∏–∑–µ –ø—Ä–æ–±–ª–µ–º")
            elif area == "–∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –≥–∏–ø–æ—Ç–µ–∑—ã":
                recommendations.append("–†–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—Ç—å –±–æ–ª—å—à–µ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã—Ö –æ–±—ä—è—Å–Ω–µ–Ω–∏–π")
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —á–∞—Å—Ç–æ—Ç—ã —Å–µ—Å—Å–∏–π
        if progress["reasoning_sessions"] < 5:
            recommendations.append("–£–≤–µ–ª–∏—á–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–µ—Å—Å–∏–π –º—ã—à–ª–µ–Ω–∏—è –≤ —ç—Ç–æ–º –¥–æ–º–µ–Ω–µ")
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–æ—Å–ª–µ–¥–Ω–µ–π —Å–µ—Å—Å–∏–∏
        if progress["last_learning_session"]:
            last_session = datetime.fromisoformat(progress["last_learning_session"])
            days_since_last = (datetime.now() - last_session).days
            
            if days_since_last > 7:
                recommendations.append("–í–æ–∑–æ–±–Ω–æ–≤–∏—Ç—å —Ä–µ–≥—É–ª—è—Ä–Ω—ã–µ —Å–µ—Å—Å–∏–∏ –º—ã—à–ª–µ–Ω–∏—è")
        
        return recommendations

if __name__ == "__main__":
    print("üß† –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã –º—ã—à–ª–µ–Ω–∏—è Rubin AI")
    
    thinking_system = RubinAIThinkingSystem()
    
    # –¢–µ—Å—Ç —Å—Ç–∏–º—É–ª–∏—Ä–æ–≤–∞–Ω–∏—è –º—ã—à–ª–µ–Ω–∏—è
    print("\nüéØ –¢–µ—Å—Ç —Å—Ç–∏–º—É–ª–∏—Ä–æ–≤–∞–Ω–∏—è –º—ã—à–ª–µ–Ω–∏—è:")
    domains = ["electrical", "math", "programming", "controllers"]
    
    for domain in domains:
        print(f"\nüìã –î–æ–º–µ–Ω: {domain}")
        result = thinking_system.stimulate_thinking(domain, 4)
        
        print(f"  –ó–∞–ø—Ä–æ—Å: {result['query']}")
        print(f"  –¢–∏–ø: {result['query_type']}")
        print(f"  –ì–ª—É–±–∏–Ω–∞ –º—ã—à–ª–µ–Ω–∏—è: {result['thinking_result']['thinking_depth']:.2f}")
        print(f"  –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {result['thinking_result']['reasoning_result'].confidence_score:.2f}")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–µ–≤—å—é –æ—Ç–≤–µ—Ç–∞
        response_preview = result['thinking_result']['extended_response'][:200]
        print(f"  –ü—Ä–µ–≤—å—é –æ—Ç–≤–µ—Ç–∞: {response_preview}...")
    
    # –ê–Ω–∞–ª–∏—Ç–∏–∫–∞
    print("\nüìä –ê–Ω–∞–ª–∏—Ç–∏–∫–∞ –º—ã—à–ª–µ–Ω–∏—è:")
    analytics = thinking_system.get_thinking_analytics()
    print(f"  –í—Å–µ–≥–æ —Å–µ—Å—Å–∏–π: {analytics['total_sessions']}")
    print(f"  –°—Ä–µ–¥–Ω—è—è –≥–ª—É–±–∏–Ω–∞ –º—ã—à–ª–µ–Ω–∏—è: {analytics['average_thinking_depth']:.2f}")
    print(f"  –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –¥–æ–º–µ–Ω–∞–º: {analytics['domain_breakdown']}")
    
    # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
    print("\nüí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –æ–±—É—á–µ–Ω–∏—é:")
    for domain in domains:
        recommendations = thinking_system.generate_learning_recommendations(domain)
        print(f"  {domain}: {', '.join(recommendations)}")
    
    print("\n‚úÖ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")










